{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN runner\n",
    "## Mark Tenzer & Jimmy Howerton\n",
    "## Adapted from code provided with the Spektral package, cited below:\n",
    "https://github.com/danielegrattarola/spektral/blob/master/examples/node_prediction/citation_gcn.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base python\n",
    "import pickle # for loading sparse matrix from disk\n",
    "import gc     # for garbage collection (RAM management/cleanup)\n",
    "\n",
    "# Common scientific packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import math\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TF imports\n",
    "from tensorflow.keras.callbacks import EarlyStopping,CSVLogger, LearningRateScheduler, Callback, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "# Spektral imports for GNN\n",
    "from spektral.layers import GraphConv, GraphSageConv, GINConv\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of graph convolution\n",
    "subset_2015 = True\n",
    "\n",
    "# is the data/adjacency matrix sorted?\n",
    "is_sorted = False\n",
    "\n",
    "#type of model to be run\n",
    "modelType = 'SAGE' # GIN, GCN, SAGE\n",
    "\n",
    "#location of saved adjacency matrices\n",
    "adjLoc = \"adj/\"\n",
    "\n",
    "#location of saved data\n",
    "dataLoc = \"data/\"\n",
    "\n",
    "#location of saved models\n",
    "modelLoc = \"models/\"\n",
    "\n",
    "# Number of graph convolution layers\n",
    "n_layers = 5\n",
    "\n",
    "# Number of channels at each conv\n",
    "channels = 64\n",
    "\n",
    "# Maximum number of training epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Dropout rate\n",
    "# dropout_rate = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelType == 'GCN':\n",
    "    Conv = GraphConv\n",
    "elif modelType == 'SAGE':\n",
    "    Conv = GraphSageConv\n",
    "elif modelType == 'GIN':\n",
    "    Conv = GINConv\n",
    "else:\n",
    "    raise NameError(\"specify modelType as 'GCN', 'SAGE', or 'GIN'\")\n",
    "\n",
    "if subset_2015:\n",
    "    postfix_time = \"15\"\n",
    "else:\n",
    "    postfix_time = \"ALL\"\n",
    "\n",
    "if is_sorted:\n",
    "    postfix_sort = \"SORTED\"\n",
    "else:\n",
    "    postfix_sort = \"UNSORTED\"\n",
    "\n",
    "modelfile = modelLoc+\"{}_{}_{}.h5\".format(modelType,postfix_time,postfix_sort)\n",
    "# Adjacency matrix and data of choice        \n",
    "\n",
    "data_path = './'+dataLoc+postfix_time+'/'+postfix_sort+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_file = adjLoc+\"A_{}_{}_FINAL.pkl\".format(postfix_time,postfix_sort)\n",
    "X = np.array(pd.read_csv(data_path+'Wildfire_Data_X_{}_{}.csv'.format(postfix_time,postfix_sort),index_col='Unnamed: 0'))\n",
    "Y = np.array(pd.read_csv(data_path+'Wildfire_Data_Y_{}_{}.csv'.format(postfix_time,postfix_sort),index_col='Unnamed: 0'))\n",
    "train_mask = pd.read_csv(data_path+'train_mask_{}_{}.csv'.format(postfix_time,postfix_sort),index_col='Unnamed: 0',squeeze=True).to_numpy()\n",
    "val_mask = pd.read_csv(data_path+'val_mask_{}_{}.csv'.format(postfix_time,postfix_sort),index_col='Unnamed: 0',squeeze=True).to_numpy()\n",
    "test_mask = pd.read_csv(data_path+'test_mask_{}_{}.csv'.format(postfix_time,postfix_sort),index_col='Unnamed: 0',squeeze=True).to_numpy()\n",
    "\n",
    "n_classes = Y.shape[1]\n",
    "# Number of examples\n",
    "N = X.shape[0]\n",
    "# Number of features\n",
    "F = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the adjacency matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load $A$ from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(adj_file, 'rb') as f:\n",
    "    A = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess $A$ as needed for this GNN convolution technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr = Conv.preprocess(A.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input\n",
    "X_in = Input(shape=(F, ))           # features  for this node\n",
    "fltr_in = Input((N, ), sparse=True) # adjacency for this node\n",
    "\n",
    "currentX = X_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n_layers - 1):\n",
    "\n",
    "    currentX = Conv(channels,\n",
    "                      activation='relu',aggregate_op = 'max'\n",
    "                      #kernel_regularizer=l2(0.1),\n",
    "                      #use_bias=False\n",
    "                     )([currentX, fltr_in])\n",
    "output = Conv(n_classes,\n",
    "              activation='softmax',aggregate_op = 'max'\n",
    "              #use_bias=False\n",
    "             )([currentX, fltr_in])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate Decay: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "\n",
    "This is only recommended for GIN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "   initial_lrate = 1e-2\n",
    "   drop = 0.5\n",
    "   epochs_drop = 50.0\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "       self.losses = []\n",
    "       self.lr = []\n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "       self.losses.append(logs.get('loss'))\n",
    "       self.lr.append(step_decay(len(self.losses)))\n",
    "\n",
    "checkpoint = ModelCheckpoint(modelfile, monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "csv_log = CSVLogger(modelLoc+\"/logs/{}_{}_{}.log\".format(modelType,postfix_time,postfix_sort))\n",
    "    \n",
    "callbacks_list = [checkpoint,csv_log]\n",
    "if modelType == 'GIN':\n",
    "    loss_history = LossHistory()\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    callbacks_list.append(loss_history, lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 83)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_sage_conv_1 (GraphSageCon (None, 64)           10688       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_sage_conv_2 (GraphSageCon (None, 64)           8256        graph_sage_conv_1[0][0]          \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_sage_conv_3 (GraphSageCon (None, 64)           8256        graph_sage_conv_2[0][0]          \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_sage_conv_4 (GraphSageCon (None, 64)           8256        graph_sage_conv_3[0][0]          \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_sage_conv_5 (GraphSageCon (None, 11)           1419        graph_sage_conv_4[0][0]          \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 36,875\n",
      "Trainable params: 36,875\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Model(inputs=[X_in, fltr_in], outputs=output)\n",
    "model.compile(optimizer=Adam(1e-2),\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 74491 samples, validate on 74491 samples\n",
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/jh3df/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: loss improved from inf to 1.02095, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 7s 98us/sample - loss: 1.0209 - categorical_accuracy: 0.6141 - val_loss: 0.1474 - val_categorical_accuracy: 0.1353\n",
      "Epoch 2/10000\n",
      "\n",
      "Epoch 00002: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.3188 - categorical_accuracy: 0.1450 - val_loss: 0.1311 - val_categorical_accuracy: 0.3442\n",
      "Epoch 3/10000\n",
      "\n",
      "Epoch 00003: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.1743 - categorical_accuracy: 0.3446 - val_loss: 0.1262 - val_categorical_accuracy: 0.4174\n",
      "Epoch 4/10000\n",
      "\n",
      "Epoch 00004: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.1317 - categorical_accuracy: 0.4242 - val_loss: 0.1243 - val_categorical_accuracy: 0.4331\n",
      "Epoch 5/10000\n",
      "\n",
      "Epoch 00005: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.1151 - categorical_accuracy: 0.4413 - val_loss: 0.1224 - val_categorical_accuracy: 0.4990\n",
      "Epoch 6/10000\n",
      "\n",
      "Epoch 00006: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0976 - categorical_accuracy: 0.5038 - val_loss: 0.1220 - val_categorical_accuracy: 0.4928\n",
      "Epoch 7/10000\n",
      "\n",
      "Epoch 00007: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 1.0955 - categorical_accuracy: 0.4992 - val_loss: 0.1210 - val_categorical_accuracy: 0.5008\n",
      "Epoch 8/10000\n",
      "\n",
      "Epoch 00008: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0858 - categorical_accuracy: 0.5089 - val_loss: 0.1212 - val_categorical_accuracy: 0.4952\n",
      "Epoch 9/10000\n",
      "\n",
      "Epoch 00009: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0868 - categorical_accuracy: 0.5017 - val_loss: 0.1209 - val_categorical_accuracy: 0.4997\n",
      "Epoch 10/10000\n",
      "\n",
      "Epoch 00010: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0846 - categorical_accuracy: 0.5037 - val_loss: 0.1205 - val_categorical_accuracy: 0.5032\n",
      "Epoch 11/10000\n",
      "\n",
      "Epoch 00011: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0815 - categorical_accuracy: 0.5103 - val_loss: 0.1204 - val_categorical_accuracy: 0.5048\n",
      "Epoch 12/10000\n",
      "\n",
      "Epoch 00012: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 1.0808 - categorical_accuracy: 0.5104 - val_loss: 0.1204 - val_categorical_accuracy: 0.5045\n",
      "Epoch 13/10000\n",
      "\n",
      "Epoch 00013: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 54us/sample - loss: 1.0807 - categorical_accuracy: 0.5091 - val_loss: 0.1203 - val_categorical_accuracy: 0.5061\n",
      "Epoch 14/10000\n",
      "\n",
      "Epoch 00014: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 50us/sample - loss: 1.0797 - categorical_accuracy: 0.5108 - val_loss: 0.1203 - val_categorical_accuracy: 0.5057\n",
      "Epoch 15/10000\n",
      "\n",
      "Epoch 00015: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 50us/sample - loss: 1.0791 - categorical_accuracy: 0.5124 - val_loss: 0.1202 - val_categorical_accuracy: 0.5063\n",
      "Epoch 16/10000\n",
      "\n",
      "Epoch 00016: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 54us/sample - loss: 1.0779 - categorical_accuracy: 0.5132 - val_loss: 0.1200 - val_categorical_accuracy: 0.5085\n",
      "Epoch 17/10000\n",
      "\n",
      "Epoch 00017: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 1.0763 - categorical_accuracy: 0.5157 - val_loss: 0.1200 - val_categorical_accuracy: 0.5081\n",
      "Epoch 18/10000\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0759 - categorical_accuracy: 0.5163 - val_loss: 0.1200 - val_categorical_accuracy: 0.5059\n",
      "Epoch 19/10000\n",
      "\n",
      "Epoch 00019: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 54us/sample - loss: 1.0759 - categorical_accuracy: 0.5159 - val_loss: 0.1200 - val_categorical_accuracy: 0.5059\n",
      "Epoch 20/10000\n",
      "\n",
      "Epoch 00020: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 1.0755 - categorical_accuracy: 0.5164 - val_loss: 0.1199 - val_categorical_accuracy: 0.5090\n",
      "Epoch 21/10000\n",
      "\n",
      "Epoch 00021: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 50us/sample - loss: 1.0746 - categorical_accuracy: 0.5177 - val_loss: 0.1198 - val_categorical_accuracy: 0.5088\n",
      "Epoch 22/10000\n",
      "\n",
      "Epoch 00022: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 51us/sample - loss: 1.0736 - categorical_accuracy: 0.5190 - val_loss: 0.1197 - val_categorical_accuracy: 0.5116\n",
      "Epoch 23/10000\n",
      "\n",
      "Epoch 00023: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 53us/sample - loss: 1.0732 - categorical_accuracy: 0.5195 - val_loss: 0.1197 - val_categorical_accuracy: 0.5110\n",
      "Epoch 24/10000\n",
      "\n",
      "Epoch 00024: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 50us/sample - loss: 1.0733 - categorical_accuracy: 0.5185 - val_loss: 0.1196 - val_categorical_accuracy: 0.5116\n",
      "Epoch 25/10000\n",
      "\n",
      "Epoch 00025: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 54us/sample - loss: 1.0727 - categorical_accuracy: 0.5193 - val_loss: 0.1195 - val_categorical_accuracy: 0.5136\n",
      "Epoch 26/10000\n",
      "\n",
      "Epoch 00026: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 53us/sample - loss: 1.0715 - categorical_accuracy: 0.5206 - val_loss: 0.1194 - val_categorical_accuracy: 0.5130\n",
      "Epoch 27/10000\n",
      "\n",
      "Epoch 00027: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0709 - categorical_accuracy: 0.5201 - val_loss: 0.1194 - val_categorical_accuracy: 0.5132\n",
      "Epoch 28/10000\n",
      "\n",
      "Epoch 00028: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0708 - categorical_accuracy: 0.5196 - val_loss: 0.1194 - val_categorical_accuracy: 0.5125\n",
      "Epoch 29/10000\n",
      "\n",
      "Epoch 00029: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0704 - categorical_accuracy: 0.5199 - val_loss: 0.1193 - val_categorical_accuracy: 0.5161\n",
      "Epoch 30/10000\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0696 - categorical_accuracy: 0.5209 - val_loss: 0.1192 - val_categorical_accuracy: 0.5165\n",
      "Epoch 31/10000\n",
      "\n",
      "Epoch 00031: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 1.0691 - categorical_accuracy: 0.5213 - val_loss: 0.1192 - val_categorical_accuracy: 0.5161\n",
      "Epoch 32/10000\n",
      "\n",
      "Epoch 00032: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0687 - categorical_accuracy: 0.5208 - val_loss: 0.1191 - val_categorical_accuracy: 0.5174\n",
      "Epoch 33/10000\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 1.0680 - categorical_accuracy: 0.5213 - val_loss: 0.1190 - val_categorical_accuracy: 0.5203\n",
      "Epoch 34/10000\n",
      "\n",
      "Epoch 00034: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0673 - categorical_accuracy: 0.5230 - val_loss: 0.1190 - val_categorical_accuracy: 0.5201\n",
      "Epoch 35/10000\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0669 - categorical_accuracy: 0.5236 - val_loss: 0.1189 - val_categorical_accuracy: 0.5190\n",
      "Epoch 36/10000\n",
      "\n",
      "Epoch 00036: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0664 - categorical_accuracy: 0.5245 - val_loss: 0.1189 - val_categorical_accuracy: 0.5196\n",
      "Epoch 37/10000\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0657 - categorical_accuracy: 0.5253 - val_loss: 0.1188 - val_categorical_accuracy: 0.5190\n",
      "Epoch 38/10000\n",
      "\n",
      "Epoch 00038: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0652 - categorical_accuracy: 0.5259 - val_loss: 0.1188 - val_categorical_accuracy: 0.5185\n",
      "Epoch 39/10000\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0646 - categorical_accuracy: 0.5256 - val_loss: 0.1187 - val_categorical_accuracy: 0.5190\n",
      "Epoch 40/10000\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0639 - categorical_accuracy: 0.5260 - val_loss: 0.1187 - val_categorical_accuracy: 0.5196\n",
      "Epoch 41/10000\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0633 - categorical_accuracy: 0.5266 - val_loss: 0.1186 - val_categorical_accuracy: 0.5221\n",
      "Epoch 42/10000\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0628 - categorical_accuracy: 0.5280 - val_loss: 0.1186 - val_categorical_accuracy: 0.5232\n",
      "Epoch 43/10000\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0621 - categorical_accuracy: 0.5282 - val_loss: 0.1185 - val_categorical_accuracy: 0.5238\n",
      "Epoch 44/10000\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0616 - categorical_accuracy: 0.5289 - val_loss: 0.1184 - val_categorical_accuracy: 0.5245\n",
      "Epoch 45/10000\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0610 - categorical_accuracy: 0.5306 - val_loss: 0.1184 - val_categorical_accuracy: 0.5247\n",
      "Epoch 46/10000\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0606 - categorical_accuracy: 0.5310 - val_loss: 0.1183 - val_categorical_accuracy: 0.5250\n",
      "Epoch 47/10000\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0601 - categorical_accuracy: 0.5316 - val_loss: 0.1183 - val_categorical_accuracy: 0.5236\n",
      "Epoch 48/10000\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0596 - categorical_accuracy: 0.5317 - val_loss: 0.1182 - val_categorical_accuracy: 0.5263\n",
      "Epoch 49/10000\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 1.0592 - categorical_accuracy: 0.5329 - val_loss: 0.1182 - val_categorical_accuracy: 0.5263\n",
      "Epoch 50/10000\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0587 - categorical_accuracy: 0.5336 - val_loss: 0.1181 - val_categorical_accuracy: 0.5247\n",
      "Epoch 51/10000\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 1.0582 - categorical_accuracy: 0.5331 - val_loss: 0.1181 - val_categorical_accuracy: 0.5278\n",
      "Epoch 52/10000\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0577 - categorical_accuracy: 0.5350 - val_loss: 0.1180 - val_categorical_accuracy: 0.5281\n",
      "Epoch 53/10000\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0572 - categorical_accuracy: 0.5350 - val_loss: 0.1179 - val_categorical_accuracy: 0.5265\n",
      "Epoch 54/10000\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0568 - categorical_accuracy: 0.5351 - val_loss: 0.1179 - val_categorical_accuracy: 0.5274\n",
      "Epoch 55/10000\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0563 - categorical_accuracy: 0.5348 - val_loss: 0.1178 - val_categorical_accuracy: 0.5263\n",
      "Epoch 56/10000\n",
      "\n",
      "Epoch 00056: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0561 - categorical_accuracy: 0.5366 - val_loss: 0.1180 - val_categorical_accuracy: 0.5236\n",
      "Epoch 57/10000\n",
      "\n",
      "Epoch 00057: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0569 - categorical_accuracy: 0.5330 - val_loss: 0.1179 - val_categorical_accuracy: 0.5243\n",
      "Epoch 58/10000\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0571 - categorical_accuracy: 0.5353 - val_loss: 0.1180 - val_categorical_accuracy: 0.5216\n",
      "Epoch 59/10000\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0570 - categorical_accuracy: 0.5316 - val_loss: 0.1177 - val_categorical_accuracy: 0.5256\n",
      "Epoch 60/10000\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0542 - categorical_accuracy: 0.5368 - val_loss: 0.1177 - val_categorical_accuracy: 0.5263\n",
      "Epoch 61/10000\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0553 - categorical_accuracy: 0.5372 - val_loss: 0.1179 - val_categorical_accuracy: 0.5225\n",
      "Epoch 62/10000\n",
      "\n",
      "Epoch 00062: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0564 - categorical_accuracy: 0.5321 - val_loss: 0.1176 - val_categorical_accuracy: 0.5287\n",
      "Epoch 63/10000\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0532 - categorical_accuracy: 0.5411 - val_loss: 0.1177 - val_categorical_accuracy: 0.5296\n",
      "Epoch 64/10000\n",
      "\n",
      "Epoch 00064: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0551 - categorical_accuracy: 0.5422 - val_loss: 0.1178 - val_categorical_accuracy: 0.5227\n",
      "Epoch 65/10000\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0548 - categorical_accuracy: 0.5365 - val_loss: 0.1175 - val_categorical_accuracy: 0.5345\n",
      "Epoch 66/10000\n",
      "\n",
      "Epoch 00066: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0524 - categorical_accuracy: 0.5464 - val_loss: 0.1175 - val_categorical_accuracy: 0.5360\n",
      "Epoch 67/10000\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0538 - categorical_accuracy: 0.5462 - val_loss: 0.1175 - val_categorical_accuracy: 0.5316\n",
      "Epoch 68/10000\n",
      "\n",
      "Epoch 00068: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0522 - categorical_accuracy: 0.5440 - val_loss: 0.1174 - val_categorical_accuracy: 0.5387\n",
      "Epoch 69/10000\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0514 - categorical_accuracy: 0.5507 - val_loss: 0.1173 - val_categorical_accuracy: 0.5420\n",
      "Epoch 70/10000\n",
      "\n",
      "Epoch 00070: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0515 - categorical_accuracy: 0.5516 - val_loss: 0.1173 - val_categorical_accuracy: 0.5391\n",
      "Epoch 71/10000\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0507 - categorical_accuracy: 0.5478 - val_loss: 0.1173 - val_categorical_accuracy: 0.5427\n",
      "Epoch 72/10000\n",
      "\n",
      "Epoch 00072: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0505 - categorical_accuracy: 0.5526 - val_loss: 0.1172 - val_categorical_accuracy: 0.5447\n",
      "Epoch 73/10000\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0495 - categorical_accuracy: 0.5532 - val_loss: 0.1172 - val_categorical_accuracy: 0.5434\n",
      "Epoch 74/10000\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0496 - categorical_accuracy: 0.5515 - val_loss: 0.1171 - val_categorical_accuracy: 0.5436\n",
      "Epoch 75/10000\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0488 - categorical_accuracy: 0.5553 - val_loss: 0.1171 - val_categorical_accuracy: 0.5462\n",
      "Epoch 76/10000\n",
      "\n",
      "Epoch 00076: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0484 - categorical_accuracy: 0.5550 - val_loss: 0.1170 - val_categorical_accuracy: 0.5442\n",
      "Epoch 77/10000\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0483 - categorical_accuracy: 0.5542 - val_loss: 0.1170 - val_categorical_accuracy: 0.5478\n",
      "Epoch 78/10000\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0476 - categorical_accuracy: 0.5581 - val_loss: 0.1170 - val_categorical_accuracy: 0.5454\n",
      "Epoch 79/10000\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0473 - categorical_accuracy: 0.5584 - val_loss: 0.1169 - val_categorical_accuracy: 0.5496\n",
      "Epoch 80/10000\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0468 - categorical_accuracy: 0.5598 - val_loss: 0.1169 - val_categorical_accuracy: 0.5513\n",
      "Epoch 81/10000\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0464 - categorical_accuracy: 0.5592 - val_loss: 0.1170 - val_categorical_accuracy: 0.5520\n",
      "Epoch 82/10000\n",
      "\n",
      "Epoch 00082: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0463 - categorical_accuracy: 0.5609 - val_loss: 0.1168 - val_categorical_accuracy: 0.5498\n",
      "Epoch 83/10000\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0459 - categorical_accuracy: 0.5596 - val_loss: 0.1168 - val_categorical_accuracy: 0.5516\n",
      "Epoch 84/10000\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0458 - categorical_accuracy: 0.5622 - val_loss: 0.1172 - val_categorical_accuracy: 0.5438\n",
      "Epoch 85/10000\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0483 - categorical_accuracy: 0.5545 - val_loss: 0.1175 - val_categorical_accuracy: 0.5369\n",
      "Epoch 86/10000\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0526 - categorical_accuracy: 0.5467 - val_loss: 0.1182 - val_categorical_accuracy: 0.5301\n",
      "Epoch 87/10000\n",
      "\n",
      "Epoch 00087: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0581 - categorical_accuracy: 0.5425 - val_loss: 0.1178 - val_categorical_accuracy: 0.5309\n",
      "Epoch 88/10000\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0542 - categorical_accuracy: 0.5465 - val_loss: 0.1174 - val_categorical_accuracy: 0.5380\n",
      "Epoch 89/10000\n",
      "\n",
      "Epoch 00089: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0515 - categorical_accuracy: 0.5510 - val_loss: 0.1176 - val_categorical_accuracy: 0.5454\n",
      "Epoch 90/10000\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0520 - categorical_accuracy: 0.5535 - val_loss: 0.1174 - val_categorical_accuracy: 0.5389\n",
      "Epoch 91/10000\n",
      "\n",
      "Epoch 00091: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0508 - categorical_accuracy: 0.5490 - val_loss: 0.1172 - val_categorical_accuracy: 0.5409\n",
      "Epoch 92/10000\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0488 - categorical_accuracy: 0.5500 - val_loss: 0.1174 - val_categorical_accuracy: 0.5394\n",
      "Epoch 93/10000\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0514 - categorical_accuracy: 0.5485 - val_loss: 0.1170 - val_categorical_accuracy: 0.5456\n",
      "Epoch 94/10000\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0468 - categorical_accuracy: 0.5603 - val_loss: 0.1173 - val_categorical_accuracy: 0.5436\n",
      "Epoch 95/10000\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0489 - categorical_accuracy: 0.5540 - val_loss: 0.1170 - val_categorical_accuracy: 0.5471\n",
      "Epoch 96/10000\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0469 - categorical_accuracy: 0.5593 - val_loss: 0.1170 - val_categorical_accuracy: 0.5471\n",
      "Epoch 97/10000\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0470 - categorical_accuracy: 0.5605 - val_loss: 0.1169 - val_categorical_accuracy: 0.5471\n",
      "Epoch 98/10000\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0460 - categorical_accuracy: 0.5599 - val_loss: 0.1168 - val_categorical_accuracy: 0.5482\n",
      "Epoch 99/10000\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0453 - categorical_accuracy: 0.5608 - val_loss: 0.1169 - val_categorical_accuracy: 0.5482\n",
      "Epoch 100/10000\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0458 - categorical_accuracy: 0.5610 - val_loss: 0.1168 - val_categorical_accuracy: 0.5520\n",
      "Epoch 101/10000\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0440 - categorical_accuracy: 0.5646 - val_loss: 0.1168 - val_categorical_accuracy: 0.5525\n",
      "Epoch 102/10000\n",
      "\n",
      "Epoch 00102: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0445 - categorical_accuracy: 0.5656 - val_loss: 0.1166 - val_categorical_accuracy: 0.5525\n",
      "Epoch 103/10000\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0434 - categorical_accuracy: 0.5664 - val_loss: 0.1167 - val_categorical_accuracy: 0.5533\n",
      "Epoch 104/10000\n",
      "\n",
      "Epoch 00104: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0437 - categorical_accuracy: 0.5643 - val_loss: 0.1166 - val_categorical_accuracy: 0.5540\n",
      "Epoch 105/10000\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0431 - categorical_accuracy: 0.5655 - val_loss: 0.1166 - val_categorical_accuracy: 0.5531\n",
      "Epoch 106/10000\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0424 - categorical_accuracy: 0.5676 - val_loss: 0.1166 - val_categorical_accuracy: 0.5560\n",
      "Epoch 107/10000\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0425 - categorical_accuracy: 0.5677 - val_loss: 0.1166 - val_categorical_accuracy: 0.5578\n",
      "Epoch 108/10000\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0421 - categorical_accuracy: 0.5691 - val_loss: 0.1165 - val_categorical_accuracy: 0.5573\n",
      "Epoch 109/10000\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0414 - categorical_accuracy: 0.5691 - val_loss: 0.1165 - val_categorical_accuracy: 0.5558\n",
      "Epoch 110/10000\n",
      "\n",
      "Epoch 00110: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0411 - categorical_accuracy: 0.5701 - val_loss: 0.1165 - val_categorical_accuracy: 0.5578\n",
      "Epoch 111/10000\n",
      "\n",
      "Epoch 00111: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0413 - categorical_accuracy: 0.5710 - val_loss: 0.1165 - val_categorical_accuracy: 0.5564\n",
      "Epoch 112/10000\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0408 - categorical_accuracy: 0.5701 - val_loss: 0.1164 - val_categorical_accuracy: 0.5595\n",
      "Epoch 113/10000\n",
      "\n",
      "Epoch 00113: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0402 - categorical_accuracy: 0.5726 - val_loss: 0.1164 - val_categorical_accuracy: 0.5600\n",
      "Epoch 114/10000\n",
      "\n",
      "Epoch 00114: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0405 - categorical_accuracy: 0.5725 - val_loss: 0.1164 - val_categorical_accuracy: 0.5602\n",
      "Epoch 115/10000\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0399 - categorical_accuracy: 0.5716 - val_loss: 0.1164 - val_categorical_accuracy: 0.5587\n",
      "Epoch 116/10000\n",
      "\n",
      "Epoch 00116: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0399 - categorical_accuracy: 0.5720 - val_loss: 0.1164 - val_categorical_accuracy: 0.5560\n",
      "Epoch 117/10000\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 69us/sample - loss: 1.0397 - categorical_accuracy: 0.5730 - val_loss: 0.1163 - val_categorical_accuracy: 0.5578\n",
      "Epoch 118/10000\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0396 - categorical_accuracy: 0.5743 - val_loss: 0.1164 - val_categorical_accuracy: 0.5562\n",
      "Epoch 119/10000\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0392 - categorical_accuracy: 0.5733 - val_loss: 0.1163 - val_categorical_accuracy: 0.5591\n",
      "Epoch 120/10000\n",
      "\n",
      "Epoch 00120: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0386 - categorical_accuracy: 0.5754 - val_loss: 0.1163 - val_categorical_accuracy: 0.5567\n",
      "Epoch 121/10000\n",
      "\n",
      "Epoch 00121: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0386 - categorical_accuracy: 0.5754 - val_loss: 0.1163 - val_categorical_accuracy: 0.5584\n",
      "Epoch 122/10000\n",
      "\n",
      "Epoch 00122: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0382 - categorical_accuracy: 0.5748 - val_loss: 0.1163 - val_categorical_accuracy: 0.5573\n",
      "Epoch 123/10000\n",
      "\n",
      "Epoch 00123: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0380 - categorical_accuracy: 0.5763 - val_loss: 0.1163 - val_categorical_accuracy: 0.5587\n",
      "Epoch 124/10000\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0377 - categorical_accuracy: 0.5775 - val_loss: 0.1163 - val_categorical_accuracy: 0.5584\n",
      "Epoch 125/10000\n",
      "\n",
      "Epoch 00125: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0375 - categorical_accuracy: 0.5772 - val_loss: 0.1162 - val_categorical_accuracy: 0.5613\n",
      "Epoch 126/10000\n",
      "\n",
      "Epoch 00126: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0374 - categorical_accuracy: 0.5790 - val_loss: 0.1163 - val_categorical_accuracy: 0.5562\n",
      "Epoch 127/10000\n",
      "\n",
      "Epoch 00127: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0376 - categorical_accuracy: 0.5773 - val_loss: 0.1164 - val_categorical_accuracy: 0.5595\n",
      "Epoch 128/10000\n",
      "\n",
      "Epoch 00128: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0397 - categorical_accuracy: 0.5749 - val_loss: 0.1169 - val_categorical_accuracy: 0.5425\n",
      "Epoch 129/10000\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0441 - categorical_accuracy: 0.5610 - val_loss: 0.1168 - val_categorical_accuracy: 0.5527\n",
      "Epoch 130/10000\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0434 - categorical_accuracy: 0.5677 - val_loss: 0.1166 - val_categorical_accuracy: 0.5471\n",
      "Epoch 131/10000\n",
      "\n",
      "Epoch 00131: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0402 - categorical_accuracy: 0.5706 - val_loss: 0.1164 - val_categorical_accuracy: 0.5576\n",
      "Epoch 132/10000\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0389 - categorical_accuracy: 0.5722 - val_loss: 0.1168 - val_categorical_accuracy: 0.5558\n",
      "Epoch 133/10000\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0427 - categorical_accuracy: 0.5687 - val_loss: 0.1166 - val_categorical_accuracy: 0.5458\n",
      "Epoch 134/10000\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0406 - categorical_accuracy: 0.5708 - val_loss: 0.1165 - val_categorical_accuracy: 0.5538\n",
      "Epoch 135/10000\n",
      "\n",
      "Epoch 00135: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0396 - categorical_accuracy: 0.5725 - val_loss: 0.1169 - val_categorical_accuracy: 0.5491\n",
      "Epoch 136/10000\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0433 - categorical_accuracy: 0.5680 - val_loss: 0.1165 - val_categorical_accuracy: 0.5516\n",
      "Epoch 137/10000\n",
      "\n",
      "Epoch 00137: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0391 - categorical_accuracy: 0.5738 - val_loss: 0.1169 - val_categorical_accuracy: 0.5425\n",
      "Epoch 138/10000\n",
      "\n",
      "Epoch 00138: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0425 - categorical_accuracy: 0.5638 - val_loss: 0.1164 - val_categorical_accuracy: 0.5593\n",
      "Epoch 139/10000\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0381 - categorical_accuracy: 0.5787 - val_loss: 0.1164 - val_categorical_accuracy: 0.5589\n",
      "Epoch 140/10000\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0387 - categorical_accuracy: 0.5764 - val_loss: 0.1165 - val_categorical_accuracy: 0.5529\n",
      "Epoch 141/10000\n",
      "\n",
      "Epoch 00141: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0388 - categorical_accuracy: 0.5738 - val_loss: 0.1164 - val_categorical_accuracy: 0.5544\n",
      "Epoch 142/10000\n",
      "\n",
      "Epoch 00142: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0376 - categorical_accuracy: 0.5766 - val_loss: 0.1164 - val_categorical_accuracy: 0.5602\n",
      "Epoch 143/10000\n",
      "\n",
      "Epoch 00143: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0380 - categorical_accuracy: 0.5776 - val_loss: 0.1163 - val_categorical_accuracy: 0.5582\n",
      "Epoch 144/10000\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0360 - categorical_accuracy: 0.5817 - val_loss: 0.1164 - val_categorical_accuracy: 0.5560\n",
      "Epoch 145/10000\n",
      "\n",
      "Epoch 00145: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0370 - categorical_accuracy: 0.5778 - val_loss: 0.1162 - val_categorical_accuracy: 0.5600\n",
      "Epoch 146/10000\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 64us/sample - loss: 1.0360 - categorical_accuracy: 0.5820 - val_loss: 0.1163 - val_categorical_accuracy: 0.5622\n",
      "Epoch 147/10000\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0362 - categorical_accuracy: 0.5831 - val_loss: 0.1162 - val_categorical_accuracy: 0.5615\n",
      "Epoch 148/10000\n",
      "\n",
      "Epoch 00148: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0351 - categorical_accuracy: 0.5821 - val_loss: 0.1163 - val_categorical_accuracy: 0.5587\n",
      "Epoch 149/10000\n",
      "\n",
      "Epoch 00149: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0355 - categorical_accuracy: 0.5832 - val_loss: 0.1162 - val_categorical_accuracy: 0.5627\n",
      "Epoch 150/10000\n",
      "\n",
      "Epoch 00150: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0350 - categorical_accuracy: 0.5839 - val_loss: 0.1161 - val_categorical_accuracy: 0.5638\n",
      "Epoch 151/10000\n",
      "\n",
      "Epoch 00151: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0344 - categorical_accuracy: 0.5858 - val_loss: 0.1162 - val_categorical_accuracy: 0.5602\n",
      "Epoch 152/10000\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0348 - categorical_accuracy: 0.5832 - val_loss: 0.1161 - val_categorical_accuracy: 0.5613\n",
      "Epoch 153/10000\n",
      "\n",
      "Epoch 00153: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0340 - categorical_accuracy: 0.5833 - val_loss: 0.1162 - val_categorical_accuracy: 0.5615\n",
      "Epoch 154/10000\n",
      "\n",
      "Epoch 00154: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0345 - categorical_accuracy: 0.5842 - val_loss: 0.1161 - val_categorical_accuracy: 0.5613\n",
      "Epoch 155/10000\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0337 - categorical_accuracy: 0.5859 - val_loss: 0.1161 - val_categorical_accuracy: 0.5622\n",
      "Epoch 156/10000\n",
      "\n",
      "Epoch 00156: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0338 - categorical_accuracy: 0.5867 - val_loss: 0.1161 - val_categorical_accuracy: 0.5607\n",
      "Epoch 157/10000\n",
      "\n",
      "Epoch 00157: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0338 - categorical_accuracy: 0.5860 - val_loss: 0.1161 - val_categorical_accuracy: 0.5633\n",
      "Epoch 158/10000\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0328 - categorical_accuracy: 0.5885 - val_loss: 0.1161 - val_categorical_accuracy: 0.5604\n",
      "Epoch 159/10000\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0333 - categorical_accuracy: 0.5864 - val_loss: 0.1161 - val_categorical_accuracy: 0.5633\n",
      "Epoch 160/10000\n",
      "\n",
      "Epoch 00160: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0327 - categorical_accuracy: 0.5888 - val_loss: 0.1161 - val_categorical_accuracy: 0.5627\n",
      "Epoch 161/10000\n",
      "\n",
      "Epoch 00161: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0331 - categorical_accuracy: 0.5894 - val_loss: 0.1162 - val_categorical_accuracy: 0.5627\n",
      "Epoch 162/10000\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0338 - categorical_accuracy: 0.5853 - val_loss: 0.1163 - val_categorical_accuracy: 0.5622\n",
      "Epoch 163/10000\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0343 - categorical_accuracy: 0.5865 - val_loss: 0.1161 - val_categorical_accuracy: 0.5576\n",
      "Epoch 164/10000\n",
      "\n",
      "Epoch 00164: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0347 - categorical_accuracy: 0.5811 - val_loss: 0.1161 - val_categorical_accuracy: 0.5620\n",
      "Epoch 165/10000\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0331 - categorical_accuracy: 0.5871 - val_loss: 0.1161 - val_categorical_accuracy: 0.5633\n",
      "Epoch 166/10000\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0328 - categorical_accuracy: 0.5885 - val_loss: 0.1161 - val_categorical_accuracy: 0.5631\n",
      "Epoch 167/10000\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0318 - categorical_accuracy: 0.5894 - val_loss: 0.1162 - val_categorical_accuracy: 0.5662\n",
      "Epoch 168/10000\n",
      "\n",
      "Epoch 00168: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0327 - categorical_accuracy: 0.5904 - val_loss: 0.1161 - val_categorical_accuracy: 0.5658\n",
      "Epoch 169/10000\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0321 - categorical_accuracy: 0.5897 - val_loss: 0.1162 - val_categorical_accuracy: 0.5624\n",
      "Epoch 170/10000\n",
      "\n",
      "Epoch 00170: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0324 - categorical_accuracy: 0.5906 - val_loss: 0.1160 - val_categorical_accuracy: 0.5629\n",
      "Epoch 171/10000\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0314 - categorical_accuracy: 0.5903 - val_loss: 0.1161 - val_categorical_accuracy: 0.5640\n",
      "Epoch 172/10000\n",
      "\n",
      "Epoch 00172: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0314 - categorical_accuracy: 0.5905 - val_loss: 0.1161 - val_categorical_accuracy: 0.5624\n",
      "Epoch 173/10000\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0317 - categorical_accuracy: 0.5913 - val_loss: 0.1162 - val_categorical_accuracy: 0.5629\n",
      "Epoch 174/10000\n",
      "\n",
      "Epoch 00174: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0317 - categorical_accuracy: 0.5914 - val_loss: 0.1162 - val_categorical_accuracy: 0.5622\n",
      "Epoch 175/10000\n",
      "\n",
      "Epoch 00175: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0329 - categorical_accuracy: 0.5894 - val_loss: 0.1162 - val_categorical_accuracy: 0.5635\n",
      "Epoch 176/10000\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0322 - categorical_accuracy: 0.5890 - val_loss: 0.1161 - val_categorical_accuracy: 0.5680\n",
      "Epoch 177/10000\n",
      "\n",
      "Epoch 00177: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0318 - categorical_accuracy: 0.5917 - val_loss: 0.1161 - val_categorical_accuracy: 0.5653\n",
      "Epoch 178/10000\n",
      "\n",
      "Epoch 00178: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0306 - categorical_accuracy: 0.5929 - val_loss: 0.1160 - val_categorical_accuracy: 0.5633\n",
      "Epoch 179/10000\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0305 - categorical_accuracy: 0.5917 - val_loss: 0.1161 - val_categorical_accuracy: 0.5673\n",
      "Epoch 180/10000\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0311 - categorical_accuracy: 0.5938 - val_loss: 0.1161 - val_categorical_accuracy: 0.5624\n",
      "Epoch 181/10000\n",
      "\n",
      "Epoch 00181: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0315 - categorical_accuracy: 0.5892 - val_loss: 0.1160 - val_categorical_accuracy: 0.5675\n",
      "Epoch 182/10000\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0305 - categorical_accuracy: 0.5952 - val_loss: 0.1160 - val_categorical_accuracy: 0.5660\n",
      "Epoch 183/10000\n",
      "\n",
      "Epoch 00183: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0295 - categorical_accuracy: 0.5966 - val_loss: 0.1161 - val_categorical_accuracy: 0.5662\n",
      "Epoch 184/10000\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0299 - categorical_accuracy: 0.5947 - val_loss: 0.1160 - val_categorical_accuracy: 0.5680\n",
      "Epoch 185/10000\n",
      "\n",
      "Epoch 00185: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0312 - categorical_accuracy: 0.5945 - val_loss: 0.1162 - val_categorical_accuracy: 0.5638\n",
      "Epoch 186/10000\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0316 - categorical_accuracy: 0.5905 - val_loss: 0.1160 - val_categorical_accuracy: 0.5684\n",
      "Epoch 187/10000\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0308 - categorical_accuracy: 0.5934 - val_loss: 0.1161 - val_categorical_accuracy: 0.5671\n",
      "Epoch 188/10000\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0300 - categorical_accuracy: 0.5966 - val_loss: 0.1162 - val_categorical_accuracy: 0.5587\n",
      "Epoch 189/10000\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0321 - categorical_accuracy: 0.5867 - val_loss: 0.1161 - val_categorical_accuracy: 0.5638\n",
      "Epoch 190/10000\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0307 - categorical_accuracy: 0.5952 - val_loss: 0.1160 - val_categorical_accuracy: 0.5638\n",
      "Epoch 191/10000\n",
      "\n",
      "Epoch 00191: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0287 - categorical_accuracy: 0.5974 - val_loss: 0.1160 - val_categorical_accuracy: 0.5642\n",
      "Epoch 192/10000\n",
      "\n",
      "Epoch 00192: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0290 - categorical_accuracy: 0.5967 - val_loss: 0.1161 - val_categorical_accuracy: 0.5682\n",
      "Epoch 193/10000\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0301 - categorical_accuracy: 0.5959 - val_loss: 0.1161 - val_categorical_accuracy: 0.5651\n",
      "Epoch 194/10000\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0302 - categorical_accuracy: 0.5936 - val_loss: 0.1159 - val_categorical_accuracy: 0.5682\n",
      "Epoch 195/10000\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0281 - categorical_accuracy: 0.6001 - val_loss: 0.1160 - val_categorical_accuracy: 0.5686\n",
      "Epoch 196/10000\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0283 - categorical_accuracy: 0.6000 - val_loss: 0.1161 - val_categorical_accuracy: 0.5646\n",
      "Epoch 197/10000\n",
      "\n",
      "Epoch 00197: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0303 - categorical_accuracy: 0.5917 - val_loss: 0.1161 - val_categorical_accuracy: 0.5671\n",
      "Epoch 198/10000\n",
      "\n",
      "Epoch 00198: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0292 - categorical_accuracy: 0.5974 - val_loss: 0.1159 - val_categorical_accuracy: 0.5658\n",
      "Epoch 199/10000\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 64us/sample - loss: 1.0275 - categorical_accuracy: 0.6002 - val_loss: 0.1160 - val_categorical_accuracy: 0.5649\n",
      "Epoch 200/10000\n",
      "\n",
      "Epoch 00200: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0277 - categorical_accuracy: 0.5992 - val_loss: 0.1161 - val_categorical_accuracy: 0.5693\n",
      "Epoch 201/10000\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0302 - categorical_accuracy: 0.5970 - val_loss: 0.1162 - val_categorical_accuracy: 0.5662\n",
      "Epoch 202/10000\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0308 - categorical_accuracy: 0.5943 - val_loss: 0.1162 - val_categorical_accuracy: 0.5627\n",
      "Epoch 203/10000\n",
      "\n",
      "Epoch 00203: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0326 - categorical_accuracy: 0.5899 - val_loss: 0.1165 - val_categorical_accuracy: 0.5593\n",
      "Epoch 204/10000\n",
      "\n",
      "Epoch 00204: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0334 - categorical_accuracy: 0.5898 - val_loss: 0.1167 - val_categorical_accuracy: 0.5513\n",
      "Epoch 205/10000\n",
      "\n",
      "Epoch 00205: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0367 - categorical_accuracy: 0.5771 - val_loss: 0.1161 - val_categorical_accuracy: 0.5627\n",
      "Epoch 206/10000\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0314 - categorical_accuracy: 0.5899 - val_loss: 0.1166 - val_categorical_accuracy: 0.5567\n",
      "Epoch 207/10000\n",
      "\n",
      "Epoch 00207: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0363 - categorical_accuracy: 0.5837 - val_loss: 0.1160 - val_categorical_accuracy: 0.5591\n",
      "Epoch 208/10000\n",
      "\n",
      "Epoch 00208: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0292 - categorical_accuracy: 0.5948 - val_loss: 0.1163 - val_categorical_accuracy: 0.5580\n",
      "Epoch 209/10000\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0344 - categorical_accuracy: 0.5834 - val_loss: 0.1159 - val_categorical_accuracy: 0.5658\n",
      "Epoch 210/10000\n",
      "\n",
      "Epoch 00210: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0278 - categorical_accuracy: 0.6001 - val_loss: 0.1164 - val_categorical_accuracy: 0.5633\n",
      "Epoch 211/10000\n",
      "\n",
      "Epoch 00211: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0324 - categorical_accuracy: 0.5917 - val_loss: 0.1160 - val_categorical_accuracy: 0.5658\n",
      "Epoch 212/10000\n",
      "\n",
      "Epoch 00212: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0318 - categorical_accuracy: 0.5888 - val_loss: 0.1161 - val_categorical_accuracy: 0.5631\n",
      "Epoch 213/10000\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0300 - categorical_accuracy: 0.5923 - val_loss: 0.1162 - val_categorical_accuracy: 0.5649\n",
      "Epoch 214/10000\n",
      "\n",
      "Epoch 00214: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0307 - categorical_accuracy: 0.5929 - val_loss: 0.1160 - val_categorical_accuracy: 0.5724\n",
      "Epoch 215/10000\n",
      "\n",
      "Epoch 00215: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0308 - categorical_accuracy: 0.5954 - val_loss: 0.1161 - val_categorical_accuracy: 0.5604\n",
      "Epoch 216/10000\n",
      "\n",
      "Epoch 00216: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0302 - categorical_accuracy: 0.5893 - val_loss: 0.1162 - val_categorical_accuracy: 0.5666\n",
      "Epoch 217/10000\n",
      "\n",
      "Epoch 00217: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0297 - categorical_accuracy: 0.5948 - val_loss: 0.1163 - val_categorical_accuracy: 0.5640\n",
      "Epoch 218/10000\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0335 - categorical_accuracy: 0.5876 - val_loss: 0.1159 - val_categorical_accuracy: 0.5680\n",
      "Epoch 219/10000\n",
      "\n",
      "Epoch 00219: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0276 - categorical_accuracy: 0.6000 - val_loss: 0.1163 - val_categorical_accuracy: 0.5598\n",
      "Epoch 220/10000\n",
      "\n",
      "Epoch 00220: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0320 - categorical_accuracy: 0.5878 - val_loss: 0.1160 - val_categorical_accuracy: 0.5675\n",
      "Epoch 221/10000\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0290 - categorical_accuracy: 0.5991 - val_loss: 0.1160 - val_categorical_accuracy: 0.5678\n",
      "Epoch 222/10000\n",
      "\n",
      "Epoch 00222: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0290 - categorical_accuracy: 0.5997 - val_loss: 0.1161 - val_categorical_accuracy: 0.5635\n",
      "Epoch 223/10000\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0295 - categorical_accuracy: 0.5959 - val_loss: 0.1160 - val_categorical_accuracy: 0.5646\n",
      "Epoch 224/10000\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0277 - categorical_accuracy: 0.5989 - val_loss: 0.1160 - val_categorical_accuracy: 0.5678\n",
      "Epoch 225/10000\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0284 - categorical_accuracy: 0.6000 - val_loss: 0.1161 - val_categorical_accuracy: 0.5669\n",
      "Epoch 226/10000\n",
      "\n",
      "Epoch 00226: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0277 - categorical_accuracy: 0.5996 - val_loss: 0.1160 - val_categorical_accuracy: 0.5655\n",
      "Epoch 227/10000\n",
      "\n",
      "Epoch 00227: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 69us/sample - loss: 1.0274 - categorical_accuracy: 0.5987 - val_loss: 0.1160 - val_categorical_accuracy: 0.5662\n",
      "Epoch 228/10000\n",
      "\n",
      "Epoch 00228: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0273 - categorical_accuracy: 0.6005 - val_loss: 0.1160 - val_categorical_accuracy: 0.5678\n",
      "Epoch 229/10000\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0266 - categorical_accuracy: 0.6032 - val_loss: 0.1160 - val_categorical_accuracy: 0.5671\n",
      "Epoch 230/10000\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0263 - categorical_accuracy: 0.6023 - val_loss: 0.1160 - val_categorical_accuracy: 0.5671\n",
      "Epoch 231/10000\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0268 - categorical_accuracy: 0.6013 - val_loss: 0.1160 - val_categorical_accuracy: 0.5706\n",
      "Epoch 232/10000\n",
      "\n",
      "Epoch 00232: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0261 - categorical_accuracy: 0.6054 - val_loss: 0.1161 - val_categorical_accuracy: 0.5669\n",
      "Epoch 233/10000\n",
      "\n",
      "Epoch 00233: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0267 - categorical_accuracy: 0.6022 - val_loss: 0.1161 - val_categorical_accuracy: 0.5646\n",
      "Epoch 234/10000\n",
      "\n",
      "Epoch 00234: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0281 - categorical_accuracy: 0.5984 - val_loss: 0.1160 - val_categorical_accuracy: 0.5682\n",
      "Epoch 235/10000\n",
      "\n",
      "Epoch 00235: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0259 - categorical_accuracy: 0.6039 - val_loss: 0.1161 - val_categorical_accuracy: 0.5664\n",
      "Epoch 236/10000\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0267 - categorical_accuracy: 0.6023 - val_loss: 0.1160 - val_categorical_accuracy: 0.5673\n",
      "Epoch 237/10000\n",
      "\n",
      "Epoch 00237: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0269 - categorical_accuracy: 0.6011 - val_loss: 0.1160 - val_categorical_accuracy: 0.5675\n",
      "Epoch 238/10000\n",
      "\n",
      "Epoch 00238: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0251 - categorical_accuracy: 0.6043 - val_loss: 0.1160 - val_categorical_accuracy: 0.5662\n",
      "Epoch 239/10000\n",
      "\n",
      "Epoch 00239: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0257 - categorical_accuracy: 0.6045 - val_loss: 0.1159 - val_categorical_accuracy: 0.5704\n",
      "Epoch 240/10000\n",
      "\n",
      "Epoch 00240: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0260 - categorical_accuracy: 0.6036 - val_loss: 0.1159 - val_categorical_accuracy: 0.5689\n",
      "Epoch 241/10000\n",
      "\n",
      "Epoch 00241: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0258 - categorical_accuracy: 0.6039 - val_loss: 0.1161 - val_categorical_accuracy: 0.5682\n",
      "Epoch 242/10000\n",
      "\n",
      "Epoch 00242: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0265 - categorical_accuracy: 0.6024 - val_loss: 0.1161 - val_categorical_accuracy: 0.5669\n",
      "Epoch 243/10000\n",
      "\n",
      "Epoch 00243: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0279 - categorical_accuracy: 0.6019 - val_loss: 0.1160 - val_categorical_accuracy: 0.5675\n",
      "Epoch 244/10000\n",
      "\n",
      "Epoch 00244: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0252 - categorical_accuracy: 0.6043 - val_loss: 0.1160 - val_categorical_accuracy: 0.5671\n",
      "Epoch 245/10000\n",
      "\n",
      "Epoch 00245: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0254 - categorical_accuracy: 0.6044 - val_loss: 0.1161 - val_categorical_accuracy: 0.5666\n",
      "Epoch 246/10000\n",
      "\n",
      "Epoch 00246: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0270 - categorical_accuracy: 0.6026 - val_loss: 0.1159 - val_categorical_accuracy: 0.5682\n",
      "Epoch 247/10000\n",
      "\n",
      "Epoch 00247: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0235 - categorical_accuracy: 0.6082 - val_loss: 0.1161 - val_categorical_accuracy: 0.5646\n",
      "Epoch 248/10000\n",
      "\n",
      "Epoch 00248: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0272 - categorical_accuracy: 0.6003 - val_loss: 0.1160 - val_categorical_accuracy: 0.5715\n",
      "Epoch 249/10000\n",
      "\n",
      "Epoch 00249: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0278 - categorical_accuracy: 0.6001 - val_loss: 0.1160 - val_categorical_accuracy: 0.5715\n",
      "Epoch 250/10000\n",
      "\n",
      "Epoch 00250: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0247 - categorical_accuracy: 0.6088 - val_loss: 0.1162 - val_categorical_accuracy: 0.5627\n",
      "Epoch 251/10000\n",
      "\n",
      "Epoch 00251: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0279 - categorical_accuracy: 0.6000 - val_loss: 0.1159 - val_categorical_accuracy: 0.5704\n",
      "Epoch 252/10000\n",
      "\n",
      "Epoch 00252: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0263 - categorical_accuracy: 0.6031 - val_loss: 0.1160 - val_categorical_accuracy: 0.5715\n",
      "Epoch 253/10000\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0247 - categorical_accuracy: 0.6083 - val_loss: 0.1162 - val_categorical_accuracy: 0.5651\n",
      "Epoch 254/10000\n",
      "\n",
      "Epoch 00254: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0265 - categorical_accuracy: 0.6012 - val_loss: 0.1159 - val_categorical_accuracy: 0.5666\n",
      "Epoch 255/10000\n",
      "\n",
      "Epoch 00255: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0254 - categorical_accuracy: 0.6045 - val_loss: 0.1160 - val_categorical_accuracy: 0.5713\n",
      "Epoch 256/10000\n",
      "\n",
      "Epoch 00256: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0253 - categorical_accuracy: 0.6064 - val_loss: 0.1161 - val_categorical_accuracy: 0.5669\n",
      "Epoch 257/10000\n",
      "\n",
      "Epoch 00257: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0248 - categorical_accuracy: 0.6055 - val_loss: 0.1159 - val_categorical_accuracy: 0.5666\n",
      "Epoch 258/10000\n",
      "\n",
      "Epoch 00258: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0240 - categorical_accuracy: 0.6051 - val_loss: 0.1159 - val_categorical_accuracy: 0.5711\n",
      "Epoch 259/10000\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0241 - categorical_accuracy: 0.6092 - val_loss: 0.1160 - val_categorical_accuracy: 0.5697\n",
      "Epoch 260/10000\n",
      "\n",
      "Epoch 00260: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0232 - categorical_accuracy: 0.6102 - val_loss: 0.1159 - val_categorical_accuracy: 0.5693\n",
      "Epoch 261/10000\n",
      "\n",
      "Epoch 00261: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0232 - categorical_accuracy: 0.6083 - val_loss: 0.1158 - val_categorical_accuracy: 0.5693\n",
      "Epoch 262/10000\n",
      "\n",
      "Epoch 00262: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0229 - categorical_accuracy: 0.6114 - val_loss: 0.1159 - val_categorical_accuracy: 0.5695\n",
      "Epoch 263/10000\n",
      "\n",
      "Epoch 00263: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0221 - categorical_accuracy: 0.6130 - val_loss: 0.1160 - val_categorical_accuracy: 0.5691\n",
      "Epoch 264/10000\n",
      "\n",
      "Epoch 00264: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0231 - categorical_accuracy: 0.6096 - val_loss: 0.1158 - val_categorical_accuracy: 0.5737\n",
      "Epoch 265/10000\n",
      "\n",
      "Epoch 00265: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0222 - categorical_accuracy: 0.6119 - val_loss: 0.1159 - val_categorical_accuracy: 0.5729\n",
      "Epoch 266/10000\n",
      "\n",
      "Epoch 00266: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0229 - categorical_accuracy: 0.6117 - val_loss: 0.1162 - val_categorical_accuracy: 0.5638\n",
      "Epoch 267/10000\n",
      "\n",
      "Epoch 00267: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0245 - categorical_accuracy: 0.6061 - val_loss: 0.1162 - val_categorical_accuracy: 0.5604\n",
      "Epoch 268/10000\n",
      "\n",
      "Epoch 00268: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0256 - categorical_accuracy: 0.6050 - val_loss: 0.1162 - val_categorical_accuracy: 0.5644\n",
      "Epoch 269/10000\n",
      "\n",
      "Epoch 00269: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0261 - categorical_accuracy: 0.6047 - val_loss: 0.1160 - val_categorical_accuracy: 0.5646\n",
      "Epoch 270/10000\n",
      "\n",
      "Epoch 00270: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0242 - categorical_accuracy: 0.6056 - val_loss: 0.1159 - val_categorical_accuracy: 0.5689\n",
      "Epoch 271/10000\n",
      "\n",
      "Epoch 00271: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0213 - categorical_accuracy: 0.6132 - val_loss: 0.1159 - val_categorical_accuracy: 0.5709\n",
      "Epoch 272/10000\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0228 - categorical_accuracy: 0.6107 - val_loss: 0.1161 - val_categorical_accuracy: 0.5611\n",
      "Epoch 273/10000\n",
      "\n",
      "Epoch 00273: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0239 - categorical_accuracy: 0.6072 - val_loss: 0.1160 - val_categorical_accuracy: 0.5673\n",
      "Epoch 274/10000\n",
      "\n",
      "Epoch 00274: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0223 - categorical_accuracy: 0.6112 - val_loss: 0.1158 - val_categorical_accuracy: 0.5751\n",
      "Epoch 275/10000\n",
      "\n",
      "Epoch 00275: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0213 - categorical_accuracy: 0.6157 - val_loss: 0.1158 - val_categorical_accuracy: 0.5722\n",
      "Epoch 276/10000\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0210 - categorical_accuracy: 0.6134 - val_loss: 0.1160 - val_categorical_accuracy: 0.5673\n",
      "Epoch 277/10000\n",
      "\n",
      "Epoch 00277: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0215 - categorical_accuracy: 0.6135 - val_loss: 0.1160 - val_categorical_accuracy: 0.5675\n",
      "Epoch 278/10000\n",
      "\n",
      "Epoch 00278: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0227 - categorical_accuracy: 0.6120 - val_loss: 0.1159 - val_categorical_accuracy: 0.5673\n",
      "Epoch 279/10000\n",
      "\n",
      "Epoch 00279: loss did not improve from 1.02095\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0220 - categorical_accuracy: 0.6120 - val_loss: 0.1159 - val_categorical_accuracy: 0.5731\n",
      "Epoch 280/10000\n",
      "\n",
      "Epoch 00280: loss improved from 1.02095 to 1.02054, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0205 - categorical_accuracy: 0.6172 - val_loss: 0.1158 - val_categorical_accuracy: 0.5766\n",
      "Epoch 281/10000\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.02054\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0207 - categorical_accuracy: 0.6162 - val_loss: 0.1160 - val_categorical_accuracy: 0.5713\n",
      "Epoch 282/10000\n",
      "\n",
      "Epoch 00282: loss did not improve from 1.02054\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0213 - categorical_accuracy: 0.6132 - val_loss: 0.1159 - val_categorical_accuracy: 0.5697\n",
      "Epoch 283/10000\n",
      "\n",
      "Epoch 00283: loss improved from 1.02054 to 1.02043, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0204 - categorical_accuracy: 0.6171 - val_loss: 0.1158 - val_categorical_accuracy: 0.5757\n",
      "Epoch 284/10000\n",
      "\n",
      "Epoch 00284: loss improved from 1.02043 to 1.02016, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0202 - categorical_accuracy: 0.6171 - val_loss: 0.1159 - val_categorical_accuracy: 0.5709\n",
      "Epoch 285/10000\n",
      "\n",
      "Epoch 00285: loss did not improve from 1.02016\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0210 - categorical_accuracy: 0.6152 - val_loss: 0.1159 - val_categorical_accuracy: 0.5713\n",
      "Epoch 286/10000\n",
      "\n",
      "Epoch 00286: loss did not improve from 1.02016\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0228 - categorical_accuracy: 0.6109 - val_loss: 0.1160 - val_categorical_accuracy: 0.5733\n",
      "Epoch 287/10000\n",
      "\n",
      "Epoch 00287: loss did not improve from 1.02016\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0203 - categorical_accuracy: 0.6176 - val_loss: 0.1159 - val_categorical_accuracy: 0.5729\n",
      "Epoch 288/10000\n",
      "\n",
      "Epoch 00288: loss did not improve from 1.02016\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0202 - categorical_accuracy: 0.6163 - val_loss: 0.1159 - val_categorical_accuracy: 0.5766\n",
      "Epoch 289/10000\n",
      "\n",
      "Epoch 00289: loss improved from 1.02016 to 1.02007, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0201 - categorical_accuracy: 0.6181 - val_loss: 0.1159 - val_categorical_accuracy: 0.5737\n",
      "Epoch 290/10000\n",
      "\n",
      "Epoch 00290: loss improved from 1.02007 to 1.01961, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0196 - categorical_accuracy: 0.6179 - val_loss: 0.1158 - val_categorical_accuracy: 0.5729\n",
      "Epoch 291/10000\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.01961\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0196 - categorical_accuracy: 0.6181 - val_loss: 0.1160 - val_categorical_accuracy: 0.5711\n",
      "Epoch 292/10000\n",
      "\n",
      "Epoch 00292: loss did not improve from 1.01961\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0206 - categorical_accuracy: 0.6175 - val_loss: 0.1160 - val_categorical_accuracy: 0.5664\n",
      "Epoch 293/10000\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.01961\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0225 - categorical_accuracy: 0.6088 - val_loss: 0.1159 - val_categorical_accuracy: 0.5693\n",
      "Epoch 294/10000\n",
      "\n",
      "Epoch 00294: loss did not improve from 1.01961\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0205 - categorical_accuracy: 0.6173 - val_loss: 0.1159 - val_categorical_accuracy: 0.5715\n",
      "Epoch 295/10000\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.01961\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0204 - categorical_accuracy: 0.6175 - val_loss: 0.1161 - val_categorical_accuracy: 0.5653\n",
      "Epoch 296/10000\n",
      "\n",
      "Epoch 00296: loss did not improve from 1.01961\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0232 - categorical_accuracy: 0.6083 - val_loss: 0.1160 - val_categorical_accuracy: 0.5697\n",
      "Epoch 297/10000\n",
      "\n",
      "Epoch 00297: loss did not improve from 1.01961\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0204 - categorical_accuracy: 0.6167 - val_loss: 0.1158 - val_categorical_accuracy: 0.5706\n",
      "Epoch 298/10000\n",
      "\n",
      "Epoch 00298: loss improved from 1.01961 to 1.01912, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0191 - categorical_accuracy: 0.6196 - val_loss: 0.1159 - val_categorical_accuracy: 0.5720\n",
      "Epoch 299/10000\n",
      "\n",
      "Epoch 00299: loss did not improve from 1.01912\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0200 - categorical_accuracy: 0.6155 - val_loss: 0.1159 - val_categorical_accuracy: 0.5737\n",
      "Epoch 300/10000\n",
      "\n",
      "Epoch 00300: loss improved from 1.01912 to 1.01896, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0190 - categorical_accuracy: 0.6211 - val_loss: 0.1159 - val_categorical_accuracy: 0.5713\n",
      "Epoch 301/10000\n",
      "\n",
      "Epoch 00301: loss improved from 1.01896 to 1.01896, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0190 - categorical_accuracy: 0.6197 - val_loss: 0.1159 - val_categorical_accuracy: 0.5704\n",
      "Epoch 302/10000\n",
      "\n",
      "Epoch 00302: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0195 - categorical_accuracy: 0.6180 - val_loss: 0.1159 - val_categorical_accuracy: 0.5706\n",
      "Epoch 303/10000\n",
      "\n",
      "Epoch 00303: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0190 - categorical_accuracy: 0.6210 - val_loss: 0.1159 - val_categorical_accuracy: 0.5684\n",
      "Epoch 304/10000\n",
      "\n",
      "Epoch 00304: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0204 - categorical_accuracy: 0.6153 - val_loss: 0.1159 - val_categorical_accuracy: 0.5702\n",
      "Epoch 305/10000\n",
      "\n",
      "Epoch 00305: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0203 - categorical_accuracy: 0.6176 - val_loss: 0.1160 - val_categorical_accuracy: 0.5726\n",
      "Epoch 306/10000\n",
      "\n",
      "Epoch 00306: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0212 - categorical_accuracy: 0.6149 - val_loss: 0.1162 - val_categorical_accuracy: 0.5646\n",
      "Epoch 307/10000\n",
      "\n",
      "Epoch 00307: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0226 - categorical_accuracy: 0.6127 - val_loss: 0.1161 - val_categorical_accuracy: 0.5711\n",
      "Epoch 308/10000\n",
      "\n",
      "Epoch 00308: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0252 - categorical_accuracy: 0.6055 - val_loss: 0.1159 - val_categorical_accuracy: 0.5711\n",
      "Epoch 309/10000\n",
      "\n",
      "Epoch 00309: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0199 - categorical_accuracy: 0.6170 - val_loss: 0.1160 - val_categorical_accuracy: 0.5695\n",
      "Epoch 310/10000\n",
      "\n",
      "Epoch 00310: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0211 - categorical_accuracy: 0.6131 - val_loss: 0.1161 - val_categorical_accuracy: 0.5684\n",
      "Epoch 311/10000\n",
      "\n",
      "Epoch 00311: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0237 - categorical_accuracy: 0.6090 - val_loss: 0.1159 - val_categorical_accuracy: 0.5706\n",
      "Epoch 312/10000\n",
      "\n",
      "Epoch 00312: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0201 - categorical_accuracy: 0.6176 - val_loss: 0.1161 - val_categorical_accuracy: 0.5682\n",
      "Epoch 313/10000\n",
      "\n",
      "Epoch 00313: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0213 - categorical_accuracy: 0.6136 - val_loss: 0.1158 - val_categorical_accuracy: 0.5742\n",
      "Epoch 314/10000\n",
      "\n",
      "Epoch 00314: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0214 - categorical_accuracy: 0.6137 - val_loss: 0.1160 - val_categorical_accuracy: 0.5717\n",
      "Epoch 315/10000\n",
      "\n",
      "Epoch 00315: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0210 - categorical_accuracy: 0.6150 - val_loss: 0.1161 - val_categorical_accuracy: 0.5660\n",
      "Epoch 316/10000\n",
      "\n",
      "Epoch 00316: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0224 - categorical_accuracy: 0.6121 - val_loss: 0.1159 - val_categorical_accuracy: 0.5726\n",
      "Epoch 317/10000\n",
      "\n",
      "Epoch 00317: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0200 - categorical_accuracy: 0.6161 - val_loss: 0.1163 - val_categorical_accuracy: 0.5669\n",
      "Epoch 318/10000\n",
      "\n",
      "Epoch 00318: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0227 - categorical_accuracy: 0.6115 - val_loss: 0.1160 - val_categorical_accuracy: 0.5702\n",
      "Epoch 319/10000\n",
      "\n",
      "Epoch 00319: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0222 - categorical_accuracy: 0.6108 - val_loss: 0.1160 - val_categorical_accuracy: 0.5720\n",
      "Epoch 320/10000\n",
      "\n",
      "Epoch 00320: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0206 - categorical_accuracy: 0.6167 - val_loss: 0.1160 - val_categorical_accuracy: 0.5713\n",
      "Epoch 321/10000\n",
      "\n",
      "Epoch 00321: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0206 - categorical_accuracy: 0.6172 - val_loss: 0.1158 - val_categorical_accuracy: 0.5704\n",
      "Epoch 322/10000\n",
      "\n",
      "Epoch 00322: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0202 - categorical_accuracy: 0.6145 - val_loss: 0.1159 - val_categorical_accuracy: 0.5724\n",
      "Epoch 323/10000\n",
      "\n",
      "Epoch 00323: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0191 - categorical_accuracy: 0.6188 - val_loss: 0.1160 - val_categorical_accuracy: 0.5717\n",
      "Epoch 324/10000\n",
      "\n",
      "Epoch 00324: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0205 - categorical_accuracy: 0.6184 - val_loss: 0.1162 - val_categorical_accuracy: 0.5651\n",
      "Epoch 325/10000\n",
      "\n",
      "Epoch 00325: loss did not improve from 1.01896\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0248 - categorical_accuracy: 0.6038 - val_loss: 0.1158 - val_categorical_accuracy: 0.5724\n",
      "Epoch 326/10000\n",
      "\n",
      "Epoch 00326: loss improved from 1.01896 to 1.01881, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0188 - categorical_accuracy: 0.6186 - val_loss: 0.1164 - val_categorical_accuracy: 0.5678\n",
      "Epoch 327/10000\n",
      "\n",
      "Epoch 00327: loss did not improve from 1.01881\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0261 - categorical_accuracy: 0.6076 - val_loss: 0.1161 - val_categorical_accuracy: 0.5642\n",
      "Epoch 328/10000\n",
      "\n",
      "Epoch 00328: loss did not improve from 1.01881\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0242 - categorical_accuracy: 0.6036 - val_loss: 0.1162 - val_categorical_accuracy: 0.5607\n",
      "Epoch 329/10000\n",
      "\n",
      "Epoch 00329: loss did not improve from 1.01881\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0239 - categorical_accuracy: 0.6062 - val_loss: 0.1161 - val_categorical_accuracy: 0.5720\n",
      "Epoch 330/10000\n",
      "\n",
      "Epoch 00330: loss did not improve from 1.01881\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0220 - categorical_accuracy: 0.6153 - val_loss: 0.1160 - val_categorical_accuracy: 0.5700\n",
      "Epoch 331/10000\n",
      "\n",
      "Epoch 00331: loss did not improve from 1.01881\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0219 - categorical_accuracy: 0.6133 - val_loss: 0.1159 - val_categorical_accuracy: 0.5731\n",
      "Epoch 332/10000\n",
      "\n",
      "Epoch 00332: loss did not improve from 1.01881\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0209 - categorical_accuracy: 0.6126 - val_loss: 0.1161 - val_categorical_accuracy: 0.5686\n",
      "Epoch 333/10000\n",
      "\n",
      "Epoch 00333: loss did not improve from 1.01881\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0221 - categorical_accuracy: 0.6135 - val_loss: 0.1158 - val_categorical_accuracy: 0.5782\n",
      "Epoch 334/10000\n",
      "\n",
      "Epoch 00334: loss improved from 1.01881 to 1.01756, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0176 - categorical_accuracy: 0.6229 - val_loss: 0.1161 - val_categorical_accuracy: 0.5686\n",
      "Epoch 335/10000\n",
      "\n",
      "Epoch 00335: loss did not improve from 1.01756\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0216 - categorical_accuracy: 0.6140 - val_loss: 0.1158 - val_categorical_accuracy: 0.5729\n",
      "Epoch 336/10000\n",
      "\n",
      "Epoch 00336: loss did not improve from 1.01756\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0193 - categorical_accuracy: 0.6182 - val_loss: 0.1162 - val_categorical_accuracy: 0.5662\n",
      "Epoch 337/10000\n",
      "\n",
      "Epoch 00337: loss did not improve from 1.01756\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0223 - categorical_accuracy: 0.6116 - val_loss: 0.1158 - val_categorical_accuracy: 0.5749\n",
      "Epoch 338/10000\n",
      "\n",
      "Epoch 00338: loss did not improve from 1.01756\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0183 - categorical_accuracy: 0.6193 - val_loss: 0.1158 - val_categorical_accuracy: 0.5726\n",
      "Epoch 339/10000\n",
      "\n",
      "Epoch 00339: loss did not improve from 1.01756\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0186 - categorical_accuracy: 0.6194 - val_loss: 0.1159 - val_categorical_accuracy: 0.5729\n",
      "Epoch 340/10000\n",
      "\n",
      "Epoch 00340: loss did not improve from 1.01756\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0179 - categorical_accuracy: 0.6230 - val_loss: 0.1159 - val_categorical_accuracy: 0.5704\n",
      "Epoch 341/10000\n",
      "\n",
      "Epoch 00341: loss did not improve from 1.01756\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0187 - categorical_accuracy: 0.6201 - val_loss: 0.1159 - val_categorical_accuracy: 0.5695\n",
      "Epoch 342/10000\n",
      "\n",
      "Epoch 00342: loss improved from 1.01756 to 1.01746, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0175 - categorical_accuracy: 0.6204 - val_loss: 0.1160 - val_categorical_accuracy: 0.5693\n",
      "Epoch 343/10000\n",
      "\n",
      "Epoch 00343: loss did not improve from 1.01746\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0185 - categorical_accuracy: 0.6216 - val_loss: 0.1157 - val_categorical_accuracy: 0.5746\n",
      "Epoch 344/10000\n",
      "\n",
      "Epoch 00344: loss improved from 1.01746 to 1.01736, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0174 - categorical_accuracy: 0.6223 - val_loss: 0.1158 - val_categorical_accuracy: 0.5693\n",
      "Epoch 345/10000\n",
      "\n",
      "Epoch 00345: loss did not improve from 1.01736\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0178 - categorical_accuracy: 0.6221 - val_loss: 0.1158 - val_categorical_accuracy: 0.5753\n",
      "Epoch 346/10000\n",
      "\n",
      "Epoch 00346: loss improved from 1.01736 to 1.01611, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0161 - categorical_accuracy: 0.6259 - val_loss: 0.1159 - val_categorical_accuracy: 0.5713\n",
      "Epoch 347/10000\n",
      "\n",
      "Epoch 00347: loss did not improve from 1.01611\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0163 - categorical_accuracy: 0.6242 - val_loss: 0.1159 - val_categorical_accuracy: 0.5717\n",
      "Epoch 348/10000\n",
      "\n",
      "Epoch 00348: loss did not improve from 1.01611\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0170 - categorical_accuracy: 0.6239 - val_loss: 0.1159 - val_categorical_accuracy: 0.5744\n",
      "Epoch 349/10000\n",
      "\n",
      "Epoch 00349: loss did not improve from 1.01611\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0169 - categorical_accuracy: 0.6231 - val_loss: 0.1160 - val_categorical_accuracy: 0.5693\n",
      "Epoch 350/10000\n",
      "\n",
      "Epoch 00350: loss did not improve from 1.01611\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0176 - categorical_accuracy: 0.6238 - val_loss: 0.1159 - val_categorical_accuracy: 0.5706\n",
      "Epoch 351/10000\n",
      "\n",
      "Epoch 00351: loss did not improve from 1.01611\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0177 - categorical_accuracy: 0.6211 - val_loss: 0.1159 - val_categorical_accuracy: 0.5695\n",
      "Epoch 352/10000\n",
      "\n",
      "Epoch 00352: loss did not improve from 1.01611\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0181 - categorical_accuracy: 0.6215 - val_loss: 0.1159 - val_categorical_accuracy: 0.5766\n",
      "Epoch 353/10000\n",
      "\n",
      "Epoch 00353: loss did not improve from 1.01611\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0165 - categorical_accuracy: 0.6237 - val_loss: 0.1158 - val_categorical_accuracy: 0.5731\n",
      "Epoch 354/10000\n",
      "\n",
      "Epoch 00354: loss improved from 1.01611 to 1.01567, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0157 - categorical_accuracy: 0.6256 - val_loss: 0.1159 - val_categorical_accuracy: 0.5726\n",
      "Epoch 355/10000\n",
      "\n",
      "Epoch 00355: loss improved from 1.01567 to 1.01535, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0154 - categorical_accuracy: 0.6283 - val_loss: 0.1158 - val_categorical_accuracy: 0.5735\n",
      "Epoch 356/10000\n",
      "\n",
      "Epoch 00356: loss did not improve from 1.01535\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0162 - categorical_accuracy: 0.6253 - val_loss: 0.1160 - val_categorical_accuracy: 0.5695\n",
      "Epoch 357/10000\n",
      "\n",
      "Epoch 00357: loss did not improve from 1.01535\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0171 - categorical_accuracy: 0.6231 - val_loss: 0.1158 - val_categorical_accuracy: 0.5726\n",
      "Epoch 358/10000\n",
      "\n",
      "Epoch 00358: loss did not improve from 1.01535\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0165 - categorical_accuracy: 0.6248 - val_loss: 0.1159 - val_categorical_accuracy: 0.5726\n",
      "Epoch 359/10000\n",
      "\n",
      "Epoch 00359: loss improved from 1.01535 to 1.01528, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0153 - categorical_accuracy: 0.6273 - val_loss: 0.1159 - val_categorical_accuracy: 0.5697\n",
      "Epoch 360/10000\n",
      "\n",
      "Epoch 00360: loss did not improve from 1.01528\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0153 - categorical_accuracy: 0.6267 - val_loss: 0.1160 - val_categorical_accuracy: 0.5700\n",
      "Epoch 361/10000\n",
      "\n",
      "Epoch 00361: loss did not improve from 1.01528\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0188 - categorical_accuracy: 0.6191 - val_loss: 0.1162 - val_categorical_accuracy: 0.5646\n",
      "Epoch 362/10000\n",
      "\n",
      "Epoch 00362: loss did not improve from 1.01528\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0195 - categorical_accuracy: 0.6188 - val_loss: 0.1160 - val_categorical_accuracy: 0.5635\n",
      "Epoch 363/10000\n",
      "\n",
      "Epoch 00363: loss did not improve from 1.01528\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0215 - categorical_accuracy: 0.6118 - val_loss: 0.1159 - val_categorical_accuracy: 0.5746\n",
      "Epoch 364/10000\n",
      "\n",
      "Epoch 00364: loss did not improve from 1.01528\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0156 - categorical_accuracy: 0.6265 - val_loss: 0.1160 - val_categorical_accuracy: 0.5702\n",
      "Epoch 365/10000\n",
      "\n",
      "Epoch 00365: loss did not improve from 1.01528\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0166 - categorical_accuracy: 0.6244 - val_loss: 0.1160 - val_categorical_accuracy: 0.5731\n",
      "Epoch 366/10000\n",
      "\n",
      "Epoch 00366: loss did not improve from 1.01528\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 1.0216 - categorical_accuracy: 0.6126 - val_loss: 0.1159 - val_categorical_accuracy: 0.5766\n",
      "Epoch 367/10000\n",
      "\n",
      "Epoch 00367: loss improved from 1.01528 to 1.01519, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0152 - categorical_accuracy: 0.6276 - val_loss: 0.1161 - val_categorical_accuracy: 0.5675\n",
      "Epoch 368/10000\n",
      "\n",
      "Epoch 00368: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0185 - categorical_accuracy: 0.6203 - val_loss: 0.1162 - val_categorical_accuracy: 0.5717\n",
      "Epoch 369/10000\n",
      "\n",
      "Epoch 00369: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0263 - categorical_accuracy: 0.6034 - val_loss: 0.1158 - val_categorical_accuracy: 0.5755\n",
      "Epoch 370/10000\n",
      "\n",
      "Epoch 00370: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0164 - categorical_accuracy: 0.6244 - val_loss: 0.1166 - val_categorical_accuracy: 0.5576\n",
      "Epoch 371/10000\n",
      "\n",
      "Epoch 00371: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0271 - categorical_accuracy: 0.6019 - val_loss: 0.1158 - val_categorical_accuracy: 0.5709\n",
      "Epoch 372/10000\n",
      "\n",
      "Epoch 00372: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0166 - categorical_accuracy: 0.6224 - val_loss: 0.1160 - val_categorical_accuracy: 0.5749\n",
      "Epoch 373/10000\n",
      "\n",
      "Epoch 00373: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0220 - categorical_accuracy: 0.6130 - val_loss: 0.1159 - val_categorical_accuracy: 0.5724\n",
      "Epoch 374/10000\n",
      "\n",
      "Epoch 00374: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0172 - categorical_accuracy: 0.6223 - val_loss: 0.1160 - val_categorical_accuracy: 0.5684\n",
      "Epoch 375/10000\n",
      "\n",
      "Epoch 00375: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0176 - categorical_accuracy: 0.6223 - val_loss: 0.1159 - val_categorical_accuracy: 0.5749\n",
      "Epoch 376/10000\n",
      "\n",
      "Epoch 00376: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0182 - categorical_accuracy: 0.6215 - val_loss: 0.1158 - val_categorical_accuracy: 0.5733\n",
      "Epoch 377/10000\n",
      "\n",
      "Epoch 00377: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0160 - categorical_accuracy: 0.6251 - val_loss: 0.1161 - val_categorical_accuracy: 0.5662\n",
      "Epoch 378/10000\n",
      "\n",
      "Epoch 00378: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0189 - categorical_accuracy: 0.6194 - val_loss: 0.1159 - val_categorical_accuracy: 0.5709\n",
      "Epoch 379/10000\n",
      "\n",
      "Epoch 00379: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0166 - categorical_accuracy: 0.6237 - val_loss: 0.1158 - val_categorical_accuracy: 0.5751\n",
      "Epoch 380/10000\n",
      "\n",
      "Epoch 00380: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0156 - categorical_accuracy: 0.6259 - val_loss: 0.1161 - val_categorical_accuracy: 0.5662\n",
      "Epoch 381/10000\n",
      "\n",
      "Epoch 00381: loss did not improve from 1.01519\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0185 - categorical_accuracy: 0.6192 - val_loss: 0.1158 - val_categorical_accuracy: 0.5757\n",
      "Epoch 382/10000\n",
      "\n",
      "Epoch 00382: loss improved from 1.01519 to 1.01489, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0149 - categorical_accuracy: 0.6274 - val_loss: 0.1158 - val_categorical_accuracy: 0.5715\n",
      "Epoch 383/10000\n",
      "\n",
      "Epoch 00383: loss did not improve from 1.01489\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0151 - categorical_accuracy: 0.6275 - val_loss: 0.1159 - val_categorical_accuracy: 0.5724\n",
      "Epoch 384/10000\n",
      "\n",
      "Epoch 00384: loss did not improve from 1.01489\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0152 - categorical_accuracy: 0.6280 - val_loss: 0.1158 - val_categorical_accuracy: 0.5735\n",
      "Epoch 385/10000\n",
      "\n",
      "Epoch 00385: loss improved from 1.01489 to 1.01355, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0136 - categorical_accuracy: 0.6316 - val_loss: 0.1159 - val_categorical_accuracy: 0.5720\n",
      "Epoch 386/10000\n",
      "\n",
      "Epoch 00386: loss did not improve from 1.01355\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0150 - categorical_accuracy: 0.6273 - val_loss: 0.1158 - val_categorical_accuracy: 0.5731\n",
      "Epoch 387/10000\n",
      "\n",
      "Epoch 00387: loss improved from 1.01355 to 1.01349, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0135 - categorical_accuracy: 0.6318 - val_loss: 0.1159 - val_categorical_accuracy: 0.5702\n",
      "Epoch 388/10000\n",
      "\n",
      "Epoch 00388: loss improved from 1.01349 to 1.01342, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0134 - categorical_accuracy: 0.6310 - val_loss: 0.1158 - val_categorical_accuracy: 0.5744\n",
      "Epoch 389/10000\n",
      "\n",
      "Epoch 00389: loss did not improve from 1.01342\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0137 - categorical_accuracy: 0.6313 - val_loss: 0.1158 - val_categorical_accuracy: 0.5746\n",
      "Epoch 390/10000\n",
      "\n",
      "Epoch 00390: loss improved from 1.01342 to 1.01279, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0128 - categorical_accuracy: 0.6326 - val_loss: 0.1158 - val_categorical_accuracy: 0.5724\n",
      "Epoch 391/10000\n",
      "\n",
      "Epoch 00391: loss did not improve from 1.01279\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0133 - categorical_accuracy: 0.6315 - val_loss: 0.1158 - val_categorical_accuracy: 0.5753\n",
      "Epoch 392/10000\n",
      "\n",
      "Epoch 00392: loss did not improve from 1.01279\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0132 - categorical_accuracy: 0.6317 - val_loss: 0.1158 - val_categorical_accuracy: 0.5755\n",
      "Epoch 393/10000\n",
      "\n",
      "Epoch 00393: loss improved from 1.01279 to 1.01266, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0127 - categorical_accuracy: 0.6341 - val_loss: 0.1160 - val_categorical_accuracy: 0.5700\n",
      "Epoch 394/10000\n",
      "\n",
      "Epoch 00394: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0136 - categorical_accuracy: 0.6301 - val_loss: 0.1159 - val_categorical_accuracy: 0.5717\n",
      "Epoch 395/10000\n",
      "\n",
      "Epoch 00395: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0156 - categorical_accuracy: 0.6264 - val_loss: 0.1162 - val_categorical_accuracy: 0.5660\n",
      "Epoch 396/10000\n",
      "\n",
      "Epoch 00396: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0172 - categorical_accuracy: 0.6229 - val_loss: 0.1165 - val_categorical_accuracy: 0.5562\n",
      "Epoch 397/10000\n",
      "\n",
      "Epoch 00397: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0228 - categorical_accuracy: 0.6096 - val_loss: 0.1160 - val_categorical_accuracy: 0.5717\n",
      "Epoch 398/10000\n",
      "\n",
      "Epoch 00398: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0171 - categorical_accuracy: 0.6220 - val_loss: 0.1159 - val_categorical_accuracy: 0.5740\n",
      "Epoch 399/10000\n",
      "\n",
      "Epoch 00399: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0138 - categorical_accuracy: 0.6302 - val_loss: 0.1162 - val_categorical_accuracy: 0.5640\n",
      "Epoch 400/10000\n",
      "\n",
      "Epoch 00400: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0190 - categorical_accuracy: 0.6175 - val_loss: 0.1161 - val_categorical_accuracy: 0.5717\n",
      "Epoch 401/10000\n",
      "\n",
      "Epoch 00401: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0196 - categorical_accuracy: 0.6177 - val_loss: 0.1158 - val_categorical_accuracy: 0.5737\n",
      "Epoch 402/10000\n",
      "\n",
      "Epoch 00402: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0137 - categorical_accuracy: 0.6308 - val_loss: 0.1160 - val_categorical_accuracy: 0.5673\n",
      "Epoch 403/10000\n",
      "\n",
      "Epoch 00403: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0176 - categorical_accuracy: 0.6211 - val_loss: 0.1159 - val_categorical_accuracy: 0.5722\n",
      "Epoch 404/10000\n",
      "\n",
      "Epoch 00404: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0157 - categorical_accuracy: 0.6257 - val_loss: 0.1161 - val_categorical_accuracy: 0.5702\n",
      "Epoch 405/10000\n",
      "\n",
      "Epoch 00405: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0159 - categorical_accuracy: 0.6256 - val_loss: 0.1163 - val_categorical_accuracy: 0.5689\n",
      "Epoch 406/10000\n",
      "\n",
      "Epoch 00406: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0201 - categorical_accuracy: 0.6167 - val_loss: 0.1158 - val_categorical_accuracy: 0.5724\n",
      "Epoch 407/10000\n",
      "\n",
      "Epoch 00407: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0138 - categorical_accuracy: 0.6299 - val_loss: 0.1160 - val_categorical_accuracy: 0.5700\n",
      "Epoch 408/10000\n",
      "\n",
      "Epoch 00408: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0166 - categorical_accuracy: 0.6245 - val_loss: 0.1160 - val_categorical_accuracy: 0.5704\n",
      "Epoch 409/10000\n",
      "\n",
      "Epoch 00409: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0173 - categorical_accuracy: 0.6212 - val_loss: 0.1159 - val_categorical_accuracy: 0.5695\n",
      "Epoch 410/10000\n",
      "\n",
      "Epoch 00410: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0142 - categorical_accuracy: 0.6310 - val_loss: 0.1161 - val_categorical_accuracy: 0.5702\n",
      "Epoch 411/10000\n",
      "\n",
      "Epoch 00411: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0180 - categorical_accuracy: 0.6218 - val_loss: 0.1159 - val_categorical_accuracy: 0.5693\n",
      "Epoch 412/10000\n",
      "\n",
      "Epoch 00412: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0150 - categorical_accuracy: 0.6257 - val_loss: 0.1162 - val_categorical_accuracy: 0.5646\n",
      "Epoch 413/10000\n",
      "\n",
      "Epoch 00413: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0177 - categorical_accuracy: 0.6235 - val_loss: 0.1159 - val_categorical_accuracy: 0.5746\n",
      "Epoch 414/10000\n",
      "\n",
      "Epoch 00414: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0180 - categorical_accuracy: 0.6207 - val_loss: 0.1157 - val_categorical_accuracy: 0.5749\n",
      "Epoch 415/10000\n",
      "\n",
      "Epoch 00415: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0144 - categorical_accuracy: 0.6276 - val_loss: 0.1161 - val_categorical_accuracy: 0.5666\n",
      "Epoch 416/10000\n",
      "\n",
      "Epoch 00416: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0194 - categorical_accuracy: 0.6175 - val_loss: 0.1157 - val_categorical_accuracy: 0.5768\n",
      "Epoch 417/10000\n",
      "\n",
      "Epoch 00417: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0135 - categorical_accuracy: 0.6306 - val_loss: 0.1158 - val_categorical_accuracy: 0.5742\n",
      "Epoch 418/10000\n",
      "\n",
      "Epoch 00418: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0162 - categorical_accuracy: 0.6246 - val_loss: 0.1159 - val_categorical_accuracy: 0.5749\n",
      "Epoch 419/10000\n",
      "\n",
      "Epoch 00419: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0138 - categorical_accuracy: 0.6323 - val_loss: 0.1160 - val_categorical_accuracy: 0.5702\n",
      "Epoch 420/10000\n",
      "\n",
      "Epoch 00420: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0140 - categorical_accuracy: 0.6305 - val_loss: 0.1159 - val_categorical_accuracy: 0.5744\n",
      "Epoch 421/10000\n",
      "\n",
      "Epoch 00421: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0136 - categorical_accuracy: 0.6303 - val_loss: 0.1160 - val_categorical_accuracy: 0.5722\n",
      "Epoch 422/10000\n",
      "\n",
      "Epoch 00422: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 1.0134 - categorical_accuracy: 0.6308 - val_loss: 0.1160 - val_categorical_accuracy: 0.5709\n",
      "Epoch 423/10000\n",
      "\n",
      "Epoch 00423: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0144 - categorical_accuracy: 0.6290 - val_loss: 0.1159 - val_categorical_accuracy: 0.5722\n",
      "Epoch 424/10000\n",
      "\n",
      "Epoch 00424: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0128 - categorical_accuracy: 0.6330 - val_loss: 0.1159 - val_categorical_accuracy: 0.5737\n",
      "Epoch 425/10000\n",
      "\n",
      "Epoch 00425: loss did not improve from 1.01266\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0147 - categorical_accuracy: 0.6282 - val_loss: 0.1158 - val_categorical_accuracy: 0.5717\n",
      "Epoch 426/10000\n",
      "\n",
      "Epoch 00426: loss improved from 1.01266 to 1.01217, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0122 - categorical_accuracy: 0.6330 - val_loss: 0.1159 - val_categorical_accuracy: 0.5729\n",
      "Epoch 427/10000\n",
      "\n",
      "Epoch 00427: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0136 - categorical_accuracy: 0.6324 - val_loss: 0.1158 - val_categorical_accuracy: 0.5757\n",
      "Epoch 428/10000\n",
      "\n",
      "Epoch 00428: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0123 - categorical_accuracy: 0.6325 - val_loss: 0.1160 - val_categorical_accuracy: 0.5729\n",
      "Epoch 429/10000\n",
      "\n",
      "Epoch 00429: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0127 - categorical_accuracy: 0.6334 - val_loss: 0.1159 - val_categorical_accuracy: 0.5740\n",
      "Epoch 430/10000\n",
      "\n",
      "Epoch 00430: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0130 - categorical_accuracy: 0.6326 - val_loss: 0.1159 - val_categorical_accuracy: 0.5742\n",
      "Epoch 431/10000\n",
      "\n",
      "Epoch 00431: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0123 - categorical_accuracy: 0.6330 - val_loss: 0.1161 - val_categorical_accuracy: 0.5693\n",
      "Epoch 432/10000\n",
      "\n",
      "Epoch 00432: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0154 - categorical_accuracy: 0.6282 - val_loss: 0.1160 - val_categorical_accuracy: 0.5717\n",
      "Epoch 433/10000\n",
      "\n",
      "Epoch 00433: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0131 - categorical_accuracy: 0.6313 - val_loss: 0.1160 - val_categorical_accuracy: 0.5733\n",
      "Epoch 434/10000\n",
      "\n",
      "Epoch 00434: loss did not improve from 1.01217\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0145 - categorical_accuracy: 0.6302 - val_loss: 0.1159 - val_categorical_accuracy: 0.5746\n",
      "Epoch 435/10000\n",
      "\n",
      "Epoch 00435: loss improved from 1.01217 to 1.01179, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0118 - categorical_accuracy: 0.6358 - val_loss: 0.1158 - val_categorical_accuracy: 0.5729\n",
      "Epoch 436/10000\n",
      "\n",
      "Epoch 00436: loss did not improve from 1.01179\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0132 - categorical_accuracy: 0.6301 - val_loss: 0.1158 - val_categorical_accuracy: 0.5733\n",
      "Epoch 437/10000\n",
      "\n",
      "Epoch 00437: loss improved from 1.01179 to 1.01063, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0106 - categorical_accuracy: 0.6387 - val_loss: 0.1159 - val_categorical_accuracy: 0.5711\n",
      "Epoch 438/10000\n",
      "\n",
      "Epoch 00438: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0109 - categorical_accuracy: 0.6379 - val_loss: 0.1158 - val_categorical_accuracy: 0.5731\n",
      "Epoch 439/10000\n",
      "\n",
      "Epoch 00439: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0123 - categorical_accuracy: 0.6328 - val_loss: 0.1162 - val_categorical_accuracy: 0.5724\n",
      "Epoch 440/10000\n",
      "\n",
      "Epoch 00440: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0137 - categorical_accuracy: 0.6307 - val_loss: 0.1161 - val_categorical_accuracy: 0.5700\n",
      "Epoch 441/10000\n",
      "\n",
      "Epoch 00441: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0157 - categorical_accuracy: 0.6271 - val_loss: 0.1160 - val_categorical_accuracy: 0.5695\n",
      "Epoch 442/10000\n",
      "\n",
      "Epoch 00442: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0129 - categorical_accuracy: 0.6312 - val_loss: 0.1161 - val_categorical_accuracy: 0.5700\n",
      "Epoch 443/10000\n",
      "\n",
      "Epoch 00443: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0148 - categorical_accuracy: 0.6291 - val_loss: 0.1158 - val_categorical_accuracy: 0.5729\n",
      "Epoch 444/10000\n",
      "\n",
      "Epoch 00444: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0128 - categorical_accuracy: 0.6312 - val_loss: 0.1160 - val_categorical_accuracy: 0.5731\n",
      "Epoch 445/10000\n",
      "\n",
      "Epoch 00445: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0121 - categorical_accuracy: 0.6356 - val_loss: 0.1159 - val_categorical_accuracy: 0.5713\n",
      "Epoch 446/10000\n",
      "\n",
      "Epoch 00446: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0126 - categorical_accuracy: 0.6342 - val_loss: 0.1159 - val_categorical_accuracy: 0.5735\n",
      "Epoch 447/10000\n",
      "\n",
      "Epoch 00447: loss did not improve from 1.01063\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0122 - categorical_accuracy: 0.6321 - val_loss: 0.1158 - val_categorical_accuracy: 0.5780\n",
      "Epoch 448/10000\n",
      "\n",
      "Epoch 00448: loss improved from 1.01063 to 1.00983, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0098 - categorical_accuracy: 0.6402 - val_loss: 0.1158 - val_categorical_accuracy: 0.5764\n",
      "Epoch 449/10000\n",
      "\n",
      "Epoch 00449: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0102 - categorical_accuracy: 0.6390 - val_loss: 0.1160 - val_categorical_accuracy: 0.5726\n",
      "Epoch 450/10000\n",
      "\n",
      "Epoch 00450: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0116 - categorical_accuracy: 0.6344 - val_loss: 0.1160 - val_categorical_accuracy: 0.5675\n",
      "Epoch 451/10000\n",
      "\n",
      "Epoch 00451: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0122 - categorical_accuracy: 0.6337 - val_loss: 0.1160 - val_categorical_accuracy: 0.5724\n",
      "Epoch 452/10000\n",
      "\n",
      "Epoch 00452: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0121 - categorical_accuracy: 0.6334 - val_loss: 0.1158 - val_categorical_accuracy: 0.5722\n",
      "Epoch 453/10000\n",
      "\n",
      "Epoch 00453: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0105 - categorical_accuracy: 0.6367 - val_loss: 0.1160 - val_categorical_accuracy: 0.5702\n",
      "Epoch 454/10000\n",
      "\n",
      "Epoch 00454: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0112 - categorical_accuracy: 0.6368 - val_loss: 0.1159 - val_categorical_accuracy: 0.5760\n",
      "Epoch 455/10000\n",
      "\n",
      "Epoch 00455: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0104 - categorical_accuracy: 0.6369 - val_loss: 0.1158 - val_categorical_accuracy: 0.5771\n",
      "Epoch 456/10000\n",
      "\n",
      "Epoch 00456: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0104 - categorical_accuracy: 0.6387 - val_loss: 0.1160 - val_categorical_accuracy: 0.5731\n",
      "Epoch 457/10000\n",
      "\n",
      "Epoch 00457: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0108 - categorical_accuracy: 0.6372 - val_loss: 0.1158 - val_categorical_accuracy: 0.5746\n",
      "Epoch 458/10000\n",
      "\n",
      "Epoch 00458: loss did not improve from 1.00983\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0101 - categorical_accuracy: 0.6378 - val_loss: 0.1159 - val_categorical_accuracy: 0.5713\n",
      "Epoch 459/10000\n",
      "\n",
      "Epoch 00459: loss improved from 1.00983 to 1.00966, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0097 - categorical_accuracy: 0.6411 - val_loss: 0.1159 - val_categorical_accuracy: 0.5753\n",
      "Epoch 460/10000\n",
      "\n",
      "Epoch 00460: loss did not improve from 1.00966\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0098 - categorical_accuracy: 0.6389 - val_loss: 0.1159 - val_categorical_accuracy: 0.5753\n",
      "Epoch 461/10000\n",
      "\n",
      "Epoch 00461: loss improved from 1.00966 to 1.00937, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0094 - categorical_accuracy: 0.6406 - val_loss: 0.1159 - val_categorical_accuracy: 0.5762\n",
      "Epoch 462/10000\n",
      "\n",
      "Epoch 00462: loss did not improve from 1.00937\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0098 - categorical_accuracy: 0.6399 - val_loss: 0.1158 - val_categorical_accuracy: 0.5773\n",
      "Epoch 463/10000\n",
      "\n",
      "Epoch 00463: loss improved from 1.00937 to 1.00871, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0087 - categorical_accuracy: 0.6415 - val_loss: 0.1159 - val_categorical_accuracy: 0.5751\n",
      "Epoch 464/10000\n",
      "\n",
      "Epoch 00464: loss did not improve from 1.00871\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0087 - categorical_accuracy: 0.6419 - val_loss: 0.1159 - val_categorical_accuracy: 0.5746\n",
      "Epoch 465/10000\n",
      "\n",
      "Epoch 00465: loss did not improve from 1.00871\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0098 - categorical_accuracy: 0.6377 - val_loss: 0.1161 - val_categorical_accuracy: 0.5682\n",
      "Epoch 466/10000\n",
      "\n",
      "Epoch 00466: loss did not improve from 1.00871\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0096 - categorical_accuracy: 0.6410 - val_loss: 0.1159 - val_categorical_accuracy: 0.5737\n",
      "Epoch 467/10000\n",
      "\n",
      "Epoch 00467: loss did not improve from 1.00871\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0114 - categorical_accuracy: 0.6343 - val_loss: 0.1159 - val_categorical_accuracy: 0.5753\n",
      "Epoch 468/10000\n",
      "\n",
      "Epoch 00468: loss did not improve from 1.00871\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0093 - categorical_accuracy: 0.6410 - val_loss: 0.1159 - val_categorical_accuracy: 0.5768\n",
      "Epoch 469/10000\n",
      "\n",
      "Epoch 00469: loss did not improve from 1.00871\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0088 - categorical_accuracy: 0.6418 - val_loss: 0.1159 - val_categorical_accuracy: 0.5771\n",
      "Epoch 470/10000\n",
      "\n",
      "Epoch 00470: loss improved from 1.00871 to 1.00824, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0082 - categorical_accuracy: 0.6428 - val_loss: 0.1160 - val_categorical_accuracy: 0.5742\n",
      "Epoch 471/10000\n",
      "\n",
      "Epoch 00471: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0088 - categorical_accuracy: 0.6437 - val_loss: 0.1160 - val_categorical_accuracy: 0.5733\n",
      "Epoch 472/10000\n",
      "\n",
      "Epoch 00472: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0113 - categorical_accuracy: 0.6343 - val_loss: 0.1161 - val_categorical_accuracy: 0.5691\n",
      "Epoch 473/10000\n",
      "\n",
      "Epoch 00473: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0118 - categorical_accuracy: 0.6369 - val_loss: 0.1160 - val_categorical_accuracy: 0.5697\n",
      "Epoch 474/10000\n",
      "\n",
      "Epoch 00474: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0134 - categorical_accuracy: 0.6293 - val_loss: 0.1159 - val_categorical_accuracy: 0.5717\n",
      "Epoch 475/10000\n",
      "\n",
      "Epoch 00475: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0099 - categorical_accuracy: 0.6398 - val_loss: 0.1161 - val_categorical_accuracy: 0.5711\n",
      "Epoch 476/10000\n",
      "\n",
      "Epoch 00476: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0103 - categorical_accuracy: 0.6396 - val_loss: 0.1161 - val_categorical_accuracy: 0.5666\n",
      "Epoch 477/10000\n",
      "\n",
      "Epoch 00477: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 1.0152 - categorical_accuracy: 0.6263 - val_loss: 0.1160 - val_categorical_accuracy: 0.5673\n",
      "Epoch 478/10000\n",
      "\n",
      "Epoch 00478: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0098 - categorical_accuracy: 0.6411 - val_loss: 0.1159 - val_categorical_accuracy: 0.5726\n",
      "Epoch 479/10000\n",
      "\n",
      "Epoch 00479: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0084 - categorical_accuracy: 0.6423 - val_loss: 0.1160 - val_categorical_accuracy: 0.5773\n",
      "Epoch 480/10000\n",
      "\n",
      "Epoch 00480: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0098 - categorical_accuracy: 0.6394 - val_loss: 0.1161 - val_categorical_accuracy: 0.5686\n",
      "Epoch 481/10000\n",
      "\n",
      "Epoch 00481: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0105 - categorical_accuracy: 0.6397 - val_loss: 0.1159 - val_categorical_accuracy: 0.5717\n",
      "Epoch 482/10000\n",
      "\n",
      "Epoch 00482: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0103 - categorical_accuracy: 0.6384 - val_loss: 0.1159 - val_categorical_accuracy: 0.5751\n",
      "Epoch 483/10000\n",
      "\n",
      "Epoch 00483: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0087 - categorical_accuracy: 0.6414 - val_loss: 0.1161 - val_categorical_accuracy: 0.5722\n",
      "Epoch 484/10000\n",
      "\n",
      "Epoch 00484: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0117 - categorical_accuracy: 0.6354 - val_loss: 0.1161 - val_categorical_accuracy: 0.5693\n",
      "Epoch 485/10000\n",
      "\n",
      "Epoch 00485: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0134 - categorical_accuracy: 0.6314 - val_loss: 0.1159 - val_categorical_accuracy: 0.5755\n",
      "Epoch 486/10000\n",
      "\n",
      "Epoch 00486: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0098 - categorical_accuracy: 0.6397 - val_loss: 0.1159 - val_categorical_accuracy: 0.5735\n",
      "Epoch 487/10000\n",
      "\n",
      "Epoch 00487: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0085 - categorical_accuracy: 0.6428 - val_loss: 0.1159 - val_categorical_accuracy: 0.5764\n",
      "Epoch 488/10000\n",
      "\n",
      "Epoch 00488: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0097 - categorical_accuracy: 0.6392 - val_loss: 0.1160 - val_categorical_accuracy: 0.5720\n",
      "Epoch 489/10000\n",
      "\n",
      "Epoch 00489: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0093 - categorical_accuracy: 0.6415 - val_loss: 0.1160 - val_categorical_accuracy: 0.5742\n",
      "Epoch 490/10000\n",
      "\n",
      "Epoch 00490: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0084 - categorical_accuracy: 0.6423 - val_loss: 0.1160 - val_categorical_accuracy: 0.5711\n",
      "Epoch 491/10000\n",
      "\n",
      "Epoch 00491: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0097 - categorical_accuracy: 0.6398 - val_loss: 0.1162 - val_categorical_accuracy: 0.5680\n",
      "Epoch 492/10000\n",
      "\n",
      "Epoch 00492: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 5s 64us/sample - loss: 1.0103 - categorical_accuracy: 0.6395 - val_loss: 0.1160 - val_categorical_accuracy: 0.5737\n",
      "Epoch 493/10000\n",
      "\n",
      "Epoch 00493: loss did not improve from 1.00824\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0111 - categorical_accuracy: 0.6359 - val_loss: 0.1159 - val_categorical_accuracy: 0.5755\n",
      "Epoch 494/10000\n",
      "\n",
      "Epoch 00494: loss improved from 1.00824 to 1.00794, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0079 - categorical_accuracy: 0.6432 - val_loss: 0.1159 - val_categorical_accuracy: 0.5733\n",
      "Epoch 495/10000\n",
      "\n",
      "Epoch 00495: loss did not improve from 1.00794\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0082 - categorical_accuracy: 0.6443 - val_loss: 0.1159 - val_categorical_accuracy: 0.5764\n",
      "Epoch 496/10000\n",
      "\n",
      "Epoch 00496: loss did not improve from 1.00794\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0103 - categorical_accuracy: 0.6373 - val_loss: 0.1160 - val_categorical_accuracy: 0.5726\n",
      "Epoch 497/10000\n",
      "\n",
      "Epoch 00497: loss improved from 1.00794 to 1.00741, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0074 - categorical_accuracy: 0.6462 - val_loss: 0.1159 - val_categorical_accuracy: 0.5753\n",
      "Epoch 498/10000\n",
      "\n",
      "Epoch 00498: loss did not improve from 1.00741\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0080 - categorical_accuracy: 0.6442 - val_loss: 0.1161 - val_categorical_accuracy: 0.5737\n",
      "Epoch 499/10000\n",
      "\n",
      "Epoch 00499: loss did not improve from 1.00741\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0103 - categorical_accuracy: 0.6386 - val_loss: 0.1161 - val_categorical_accuracy: 0.5706\n",
      "Epoch 500/10000\n",
      "\n",
      "Epoch 00500: loss did not improve from 1.00741\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0091 - categorical_accuracy: 0.6418 - val_loss: 0.1161 - val_categorical_accuracy: 0.5729\n",
      "Epoch 501/10000\n",
      "\n",
      "Epoch 00501: loss did not improve from 1.00741\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0081 - categorical_accuracy: 0.6440 - val_loss: 0.1160 - val_categorical_accuracy: 0.5717\n",
      "Epoch 502/10000\n",
      "\n",
      "Epoch 00502: loss did not improve from 1.00741\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0098 - categorical_accuracy: 0.6392 - val_loss: 0.1161 - val_categorical_accuracy: 0.5711\n",
      "Epoch 503/10000\n",
      "\n",
      "Epoch 00503: loss did not improve from 1.00741\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0086 - categorical_accuracy: 0.6421 - val_loss: 0.1159 - val_categorical_accuracy: 0.5753\n",
      "Epoch 504/10000\n",
      "\n",
      "Epoch 00504: loss improved from 1.00741 to 1.00694, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0069 - categorical_accuracy: 0.6458 - val_loss: 0.1159 - val_categorical_accuracy: 0.5773\n",
      "Epoch 505/10000\n",
      "\n",
      "Epoch 00505: loss improved from 1.00694 to 1.00674, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0067 - categorical_accuracy: 0.6477 - val_loss: 0.1160 - val_categorical_accuracy: 0.5722\n",
      "Epoch 506/10000\n",
      "\n",
      "Epoch 00506: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0072 - categorical_accuracy: 0.6462 - val_loss: 0.1159 - val_categorical_accuracy: 0.5749\n",
      "Epoch 507/10000\n",
      "\n",
      "Epoch 00507: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0076 - categorical_accuracy: 0.6452 - val_loss: 0.1161 - val_categorical_accuracy: 0.5724\n",
      "Epoch 508/10000\n",
      "\n",
      "Epoch 00508: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0074 - categorical_accuracy: 0.6459 - val_loss: 0.1161 - val_categorical_accuracy: 0.5722\n",
      "Epoch 509/10000\n",
      "\n",
      "Epoch 00509: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0085 - categorical_accuracy: 0.6431 - val_loss: 0.1162 - val_categorical_accuracy: 0.5709\n",
      "Epoch 510/10000\n",
      "\n",
      "Epoch 00510: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0097 - categorical_accuracy: 0.6399 - val_loss: 0.1162 - val_categorical_accuracy: 0.5644\n",
      "Epoch 511/10000\n",
      "\n",
      "Epoch 00511: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0106 - categorical_accuracy: 0.6386 - val_loss: 0.1162 - val_categorical_accuracy: 0.5662\n",
      "Epoch 512/10000\n",
      "\n",
      "Epoch 00512: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0136 - categorical_accuracy: 0.6289 - val_loss: 0.1159 - val_categorical_accuracy: 0.5735\n",
      "Epoch 513/10000\n",
      "\n",
      "Epoch 00513: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0071 - categorical_accuracy: 0.6449 - val_loss: 0.1163 - val_categorical_accuracy: 0.5658\n",
      "Epoch 514/10000\n",
      "\n",
      "Epoch 00514: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0127 - categorical_accuracy: 0.6359 - val_loss: 0.1164 - val_categorical_accuracy: 0.5622\n",
      "Epoch 515/10000\n",
      "\n",
      "Epoch 00515: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0175 - categorical_accuracy: 0.6217 - val_loss: 0.1160 - val_categorical_accuracy: 0.5704\n",
      "Epoch 516/10000\n",
      "\n",
      "Epoch 00516: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0105 - categorical_accuracy: 0.6364 - val_loss: 0.1163 - val_categorical_accuracy: 0.5693\n",
      "Epoch 517/10000\n",
      "\n",
      "Epoch 00517: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0152 - categorical_accuracy: 0.6329 - val_loss: 0.1166 - val_categorical_accuracy: 0.5582\n",
      "Epoch 518/10000\n",
      "\n",
      "Epoch 00518: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0215 - categorical_accuracy: 0.6107 - val_loss: 0.1162 - val_categorical_accuracy: 0.5655\n",
      "Epoch 519/10000\n",
      "\n",
      "Epoch 00519: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0143 - categorical_accuracy: 0.6263 - val_loss: 0.1170 - val_categorical_accuracy: 0.5558\n",
      "Epoch 520/10000\n",
      "\n",
      "Epoch 00520: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0234 - categorical_accuracy: 0.6107 - val_loss: 0.1159 - val_categorical_accuracy: 0.5713\n",
      "Epoch 521/10000\n",
      "\n",
      "Epoch 00521: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0113 - categorical_accuracy: 0.6330 - val_loss: 0.1163 - val_categorical_accuracy: 0.5662\n",
      "Epoch 522/10000\n",
      "\n",
      "Epoch 00522: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0168 - categorical_accuracy: 0.6233 - val_loss: 0.1159 - val_categorical_accuracy: 0.5751\n",
      "Epoch 523/10000\n",
      "\n",
      "Epoch 00523: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0089 - categorical_accuracy: 0.6415 - val_loss: 0.1162 - val_categorical_accuracy: 0.5664\n",
      "Epoch 524/10000\n",
      "\n",
      "Epoch 00524: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0134 - categorical_accuracy: 0.6322 - val_loss: 0.1160 - val_categorical_accuracy: 0.5691\n",
      "Epoch 525/10000\n",
      "\n",
      "Epoch 00525: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0107 - categorical_accuracy: 0.6363 - val_loss: 0.1160 - val_categorical_accuracy: 0.5702\n",
      "Epoch 526/10000\n",
      "\n",
      "Epoch 00526: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0107 - categorical_accuracy: 0.6357 - val_loss: 0.1160 - val_categorical_accuracy: 0.5737\n",
      "Epoch 527/10000\n",
      "\n",
      "Epoch 00527: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0100 - categorical_accuracy: 0.6405 - val_loss: 0.1160 - val_categorical_accuracy: 0.5726\n",
      "Epoch 528/10000\n",
      "\n",
      "Epoch 00528: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0094 - categorical_accuracy: 0.6398 - val_loss: 0.1160 - val_categorical_accuracy: 0.5720\n",
      "Epoch 529/10000\n",
      "\n",
      "Epoch 00529: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0089 - categorical_accuracy: 0.6415 - val_loss: 0.1160 - val_categorical_accuracy: 0.5744\n",
      "Epoch 530/10000\n",
      "\n",
      "Epoch 00530: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0094 - categorical_accuracy: 0.6426 - val_loss: 0.1158 - val_categorical_accuracy: 0.5760\n",
      "Epoch 531/10000\n",
      "\n",
      "Epoch 00531: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0070 - categorical_accuracy: 0.6457 - val_loss: 0.1160 - val_categorical_accuracy: 0.5729\n",
      "Epoch 532/10000\n",
      "\n",
      "Epoch 00532: loss did not improve from 1.00674\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0089 - categorical_accuracy: 0.6400 - val_loss: 0.1159 - val_categorical_accuracy: 0.5744\n",
      "Epoch 533/10000\n",
      "\n",
      "Epoch 00533: loss improved from 1.00674 to 1.00671, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0067 - categorical_accuracy: 0.6473 - val_loss: 0.1161 - val_categorical_accuracy: 0.5757\n",
      "Epoch 534/10000\n",
      "\n",
      "Epoch 00534: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0084 - categorical_accuracy: 0.6455 - val_loss: 0.1160 - val_categorical_accuracy: 0.5724\n",
      "Epoch 535/10000\n",
      "\n",
      "Epoch 00535: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0090 - categorical_accuracy: 0.6398 - val_loss: 0.1161 - val_categorical_accuracy: 0.5735\n",
      "Epoch 536/10000\n",
      "\n",
      "Epoch 00536: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0076 - categorical_accuracy: 0.6443 - val_loss: 0.1161 - val_categorical_accuracy: 0.5713\n",
      "Epoch 537/10000\n",
      "\n",
      "Epoch 00537: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0097 - categorical_accuracy: 0.6415 - val_loss: 0.1161 - val_categorical_accuracy: 0.5729\n",
      "Epoch 538/10000\n",
      "\n",
      "Epoch 00538: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0082 - categorical_accuracy: 0.6423 - val_loss: 0.1159 - val_categorical_accuracy: 0.5737\n",
      "Epoch 539/10000\n",
      "\n",
      "Epoch 00539: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0068 - categorical_accuracy: 0.6456 - val_loss: 0.1162 - val_categorical_accuracy: 0.5720\n",
      "Epoch 540/10000\n",
      "\n",
      "Epoch 00540: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0085 - categorical_accuracy: 0.6447 - val_loss: 0.1160 - val_categorical_accuracy: 0.5722\n",
      "Epoch 541/10000\n",
      "\n",
      "Epoch 00541: loss did not improve from 1.00671\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0070 - categorical_accuracy: 0.6458 - val_loss: 0.1159 - val_categorical_accuracy: 0.5773\n",
      "Epoch 542/10000\n",
      "\n",
      "Epoch 00542: loss improved from 1.00671 to 1.00557, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0056 - categorical_accuracy: 0.6486 - val_loss: 0.1161 - val_categorical_accuracy: 0.5731\n",
      "Epoch 543/10000\n",
      "\n",
      "Epoch 00543: loss did not improve from 1.00557\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0072 - categorical_accuracy: 0.6474 - val_loss: 0.1161 - val_categorical_accuracy: 0.5715\n",
      "Epoch 544/10000\n",
      "\n",
      "Epoch 00544: loss did not improve from 1.00557\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0064 - categorical_accuracy: 0.6470 - val_loss: 0.1160 - val_categorical_accuracy: 0.5746\n",
      "Epoch 545/10000\n",
      "\n",
      "Epoch 00545: loss improved from 1.00557 to 1.00541, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0054 - categorical_accuracy: 0.6503 - val_loss: 0.1160 - val_categorical_accuracy: 0.5720\n",
      "Epoch 546/10000\n",
      "\n",
      "Epoch 00546: loss did not improve from 1.00541\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0063 - categorical_accuracy: 0.6490 - val_loss: 0.1159 - val_categorical_accuracy: 0.5768\n",
      "Epoch 547/10000\n",
      "\n",
      "Epoch 00547: loss did not improve from 1.00541\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0066 - categorical_accuracy: 0.6455 - val_loss: 0.1160 - val_categorical_accuracy: 0.5733\n",
      "Epoch 548/10000\n",
      "\n",
      "Epoch 00548: loss improved from 1.00541 to 1.00497, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0050 - categorical_accuracy: 0.6516 - val_loss: 0.1161 - val_categorical_accuracy: 0.5691\n",
      "Epoch 549/10000\n",
      "\n",
      "Epoch 00549: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0076 - categorical_accuracy: 0.6455 - val_loss: 0.1161 - val_categorical_accuracy: 0.5715\n",
      "Epoch 550/10000\n",
      "\n",
      "Epoch 00550: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0074 - categorical_accuracy: 0.6439 - val_loss: 0.1159 - val_categorical_accuracy: 0.5753\n",
      "Epoch 551/10000\n",
      "\n",
      "Epoch 00551: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0051 - categorical_accuracy: 0.6498 - val_loss: 0.1163 - val_categorical_accuracy: 0.5702\n",
      "Epoch 552/10000\n",
      "\n",
      "Epoch 00552: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0085 - categorical_accuracy: 0.6463 - val_loss: 0.1162 - val_categorical_accuracy: 0.5653\n",
      "Epoch 553/10000\n",
      "\n",
      "Epoch 00553: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0129 - categorical_accuracy: 0.6300 - val_loss: 0.1162 - val_categorical_accuracy: 0.5664\n",
      "Epoch 554/10000\n",
      "\n",
      "Epoch 00554: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0081 - categorical_accuracy: 0.6411 - val_loss: 0.1165 - val_categorical_accuracy: 0.5604\n",
      "Epoch 555/10000\n",
      "\n",
      "Epoch 00555: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0151 - categorical_accuracy: 0.6297 - val_loss: 0.1160 - val_categorical_accuracy: 0.5740\n",
      "Epoch 556/10000\n",
      "\n",
      "Epoch 00556: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0088 - categorical_accuracy: 0.6408 - val_loss: 0.1166 - val_categorical_accuracy: 0.5653\n",
      "Epoch 557/10000\n",
      "\n",
      "Epoch 00557: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0128 - categorical_accuracy: 0.6331 - val_loss: 0.1166 - val_categorical_accuracy: 0.5602\n",
      "Epoch 558/10000\n",
      "\n",
      "Epoch 00558: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0162 - categorical_accuracy: 0.6257 - val_loss: 0.1161 - val_categorical_accuracy: 0.5697\n",
      "Epoch 559/10000\n",
      "\n",
      "Epoch 00559: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0098 - categorical_accuracy: 0.6408 - val_loss: 0.1167 - val_categorical_accuracy: 0.5613\n",
      "Epoch 560/10000\n",
      "\n",
      "Epoch 00560: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0175 - categorical_accuracy: 0.6237 - val_loss: 0.1163 - val_categorical_accuracy: 0.5660\n",
      "Epoch 561/10000\n",
      "\n",
      "Epoch 00561: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0160 - categorical_accuracy: 0.6233 - val_loss: 0.1164 - val_categorical_accuracy: 0.5642\n",
      "Epoch 562/10000\n",
      "\n",
      "Epoch 00562: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0128 - categorical_accuracy: 0.6334 - val_loss: 0.1162 - val_categorical_accuracy: 0.5689\n",
      "Epoch 563/10000\n",
      "\n",
      "Epoch 00563: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0097 - categorical_accuracy: 0.6417 - val_loss: 0.1163 - val_categorical_accuracy: 0.5680\n",
      "Epoch 564/10000\n",
      "\n",
      "Epoch 00564: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0178 - categorical_accuracy: 0.6186 - val_loss: 0.1159 - val_categorical_accuracy: 0.5733\n",
      "Epoch 565/10000\n",
      "\n",
      "Epoch 00565: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0086 - categorical_accuracy: 0.6410 - val_loss: 0.1168 - val_categorical_accuracy: 0.5635\n",
      "Epoch 566/10000\n",
      "\n",
      "Epoch 00566: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0187 - categorical_accuracy: 0.6237 - val_loss: 0.1163 - val_categorical_accuracy: 0.5662\n",
      "Epoch 567/10000\n",
      "\n",
      "Epoch 00567: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0188 - categorical_accuracy: 0.6162 - val_loss: 0.1161 - val_categorical_accuracy: 0.5691\n",
      "Epoch 568/10000\n",
      "\n",
      "Epoch 00568: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0154 - categorical_accuracy: 0.6244 - val_loss: 0.1165 - val_categorical_accuracy: 0.5611\n",
      "Epoch 569/10000\n",
      "\n",
      "Epoch 00569: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0172 - categorical_accuracy: 0.6235 - val_loss: 0.1162 - val_categorical_accuracy: 0.5684\n",
      "Epoch 570/10000\n",
      "\n",
      "Epoch 00570: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0129 - categorical_accuracy: 0.6318 - val_loss: 0.1162 - val_categorical_accuracy: 0.5700\n",
      "Epoch 571/10000\n",
      "\n",
      "Epoch 00571: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0130 - categorical_accuracy: 0.6314 - val_loss: 0.1163 - val_categorical_accuracy: 0.5702\n",
      "Epoch 572/10000\n",
      "\n",
      "Epoch 00572: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0119 - categorical_accuracy: 0.6354 - val_loss: 0.1161 - val_categorical_accuracy: 0.5715\n",
      "Epoch 573/10000\n",
      "\n",
      "Epoch 00573: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0087 - categorical_accuracy: 0.6419 - val_loss: 0.1162 - val_categorical_accuracy: 0.5678\n",
      "Epoch 574/10000\n",
      "\n",
      "Epoch 00574: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0116 - categorical_accuracy: 0.6360 - val_loss: 0.1160 - val_categorical_accuracy: 0.5729\n",
      "Epoch 575/10000\n",
      "\n",
      "Epoch 00575: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 1.0074 - categorical_accuracy: 0.6443 - val_loss: 0.1165 - val_categorical_accuracy: 0.5664\n",
      "Epoch 576/10000\n",
      "\n",
      "Epoch 00576: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0125 - categorical_accuracy: 0.6339 - val_loss: 0.1161 - val_categorical_accuracy: 0.5720\n",
      "Epoch 577/10000\n",
      "\n",
      "Epoch 00577: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0099 - categorical_accuracy: 0.6392 - val_loss: 0.1162 - val_categorical_accuracy: 0.5689\n",
      "Epoch 578/10000\n",
      "\n",
      "Epoch 00578: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0097 - categorical_accuracy: 0.6402 - val_loss: 0.1162 - val_categorical_accuracy: 0.5695\n",
      "Epoch 579/10000\n",
      "\n",
      "Epoch 00579: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0079 - categorical_accuracy: 0.6459 - val_loss: 0.1160 - val_categorical_accuracy: 0.5744\n",
      "Epoch 580/10000\n",
      "\n",
      "Epoch 00580: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0085 - categorical_accuracy: 0.6422 - val_loss: 0.1159 - val_categorical_accuracy: 0.5791\n",
      "Epoch 581/10000\n",
      "\n",
      "Epoch 00581: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0069 - categorical_accuracy: 0.6463 - val_loss: 0.1161 - val_categorical_accuracy: 0.5737\n",
      "Epoch 582/10000\n",
      "\n",
      "Epoch 00582: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0085 - categorical_accuracy: 0.6458 - val_loss: 0.1159 - val_categorical_accuracy: 0.5764\n",
      "Epoch 583/10000\n",
      "\n",
      "Epoch 00583: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0054 - categorical_accuracy: 0.6498 - val_loss: 0.1160 - val_categorical_accuracy: 0.5726\n",
      "Epoch 584/10000\n",
      "\n",
      "Epoch 00584: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0069 - categorical_accuracy: 0.6462 - val_loss: 0.1160 - val_categorical_accuracy: 0.5744\n",
      "Epoch 585/10000\n",
      "\n",
      "Epoch 00585: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0051 - categorical_accuracy: 0.6518 - val_loss: 0.1160 - val_categorical_accuracy: 0.5740\n",
      "Epoch 586/10000\n",
      "\n",
      "Epoch 00586: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0057 - categorical_accuracy: 0.6502 - val_loss: 0.1159 - val_categorical_accuracy: 0.5757\n",
      "Epoch 587/10000\n",
      "\n",
      "Epoch 00587: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0050 - categorical_accuracy: 0.6505 - val_loss: 0.1160 - val_categorical_accuracy: 0.5755\n",
      "Epoch 588/10000\n",
      "\n",
      "Epoch 00588: loss did not improve from 1.00497\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0052 - categorical_accuracy: 0.6505 - val_loss: 0.1161 - val_categorical_accuracy: 0.5755\n",
      "Epoch 589/10000\n",
      "\n",
      "Epoch 00589: loss improved from 1.00497 to 1.00491, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0049 - categorical_accuracy: 0.6520 - val_loss: 0.1160 - val_categorical_accuracy: 0.5731\n",
      "Epoch 590/10000\n",
      "\n",
      "Epoch 00590: loss did not improve from 1.00491\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0051 - categorical_accuracy: 0.6504 - val_loss: 0.1160 - val_categorical_accuracy: 0.5717\n",
      "Epoch 591/10000\n",
      "\n",
      "Epoch 00591: loss did not improve from 1.00491\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0058 - categorical_accuracy: 0.6490 - val_loss: 0.1160 - val_categorical_accuracy: 0.5753\n",
      "Epoch 592/10000\n",
      "\n",
      "Epoch 00592: loss did not improve from 1.00491\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0054 - categorical_accuracy: 0.6496 - val_loss: 0.1160 - val_categorical_accuracy: 0.5720\n",
      "Epoch 593/10000\n",
      "\n",
      "Epoch 00593: loss did not improve from 1.00491\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0062 - categorical_accuracy: 0.6485 - val_loss: 0.1162 - val_categorical_accuracy: 0.5742\n",
      "Epoch 594/10000\n",
      "\n",
      "Epoch 00594: loss did not improve from 1.00491\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0064 - categorical_accuracy: 0.6471 - val_loss: 0.1160 - val_categorical_accuracy: 0.5753\n",
      "Epoch 595/10000\n",
      "\n",
      "Epoch 00595: loss did not improve from 1.00491\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0063 - categorical_accuracy: 0.6469 - val_loss: 0.1160 - val_categorical_accuracy: 0.5724\n",
      "Epoch 596/10000\n",
      "\n",
      "Epoch 00596: loss did not improve from 1.00491\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0052 - categorical_accuracy: 0.6505 - val_loss: 0.1160 - val_categorical_accuracy: 0.5720\n",
      "Epoch 597/10000\n",
      "\n",
      "Epoch 00597: loss improved from 1.00491 to 1.00462, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0046 - categorical_accuracy: 0.6526 - val_loss: 0.1160 - val_categorical_accuracy: 0.5771\n",
      "Epoch 598/10000\n",
      "\n",
      "Epoch 00598: loss improved from 1.00462 to 1.00436, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0044 - categorical_accuracy: 0.6517 - val_loss: 0.1159 - val_categorical_accuracy: 0.5760\n",
      "Epoch 599/10000\n",
      "\n",
      "Epoch 00599: loss improved from 1.00436 to 1.00283, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0028 - categorical_accuracy: 0.6558 - val_loss: 0.1161 - val_categorical_accuracy: 0.5717\n",
      "Epoch 600/10000\n",
      "\n",
      "Epoch 00600: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0034 - categorical_accuracy: 0.6561 - val_loss: 0.1160 - val_categorical_accuracy: 0.5755\n",
      "Epoch 601/10000\n",
      "\n",
      "Epoch 00601: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0036 - categorical_accuracy: 0.6543 - val_loss: 0.1160 - val_categorical_accuracy: 0.5753\n",
      "Epoch 602/10000\n",
      "\n",
      "Epoch 00602: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0035 - categorical_accuracy: 0.6557 - val_loss: 0.1161 - val_categorical_accuracy: 0.5737\n",
      "Epoch 603/10000\n",
      "\n",
      "Epoch 00603: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0041 - categorical_accuracy: 0.6536 - val_loss: 0.1161 - val_categorical_accuracy: 0.5751\n",
      "Epoch 604/10000\n",
      "\n",
      "Epoch 00604: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0055 - categorical_accuracy: 0.6492 - val_loss: 0.1162 - val_categorical_accuracy: 0.5715\n",
      "Epoch 605/10000\n",
      "\n",
      "Epoch 00605: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0058 - categorical_accuracy: 0.6490 - val_loss: 0.1161 - val_categorical_accuracy: 0.5733\n",
      "Epoch 606/10000\n",
      "\n",
      "Epoch 00606: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0060 - categorical_accuracy: 0.6484 - val_loss: 0.1161 - val_categorical_accuracy: 0.5744\n",
      "Epoch 607/10000\n",
      "\n",
      "Epoch 00607: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0056 - categorical_accuracy: 0.6483 - val_loss: 0.1162 - val_categorical_accuracy: 0.5709\n",
      "Epoch 608/10000\n",
      "\n",
      "Epoch 00608: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0066 - categorical_accuracy: 0.6483 - val_loss: 0.1160 - val_categorical_accuracy: 0.5744\n",
      "Epoch 609/10000\n",
      "\n",
      "Epoch 00609: loss did not improve from 1.00283\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0047 - categorical_accuracy: 0.6516 - val_loss: 0.1160 - val_categorical_accuracy: 0.5749\n",
      "Epoch 610/10000\n",
      "\n",
      "Epoch 00610: loss improved from 1.00283 to 1.00218, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0022 - categorical_accuracy: 0.6575 - val_loss: 0.1160 - val_categorical_accuracy: 0.5740\n",
      "Epoch 611/10000\n",
      "\n",
      "Epoch 00611: loss did not improve from 1.00218\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0030 - categorical_accuracy: 0.6564 - val_loss: 0.1161 - val_categorical_accuracy: 0.5771\n",
      "Epoch 612/10000\n",
      "\n",
      "Epoch 00612: loss did not improve from 1.00218\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0044 - categorical_accuracy: 0.6521 - val_loss: 0.1160 - val_categorical_accuracy: 0.5746\n",
      "Epoch 613/10000\n",
      "\n",
      "Epoch 00613: loss did not improve from 1.00218\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0040 - categorical_accuracy: 0.6543 - val_loss: 0.1161 - val_categorical_accuracy: 0.5731\n",
      "Epoch 614/10000\n",
      "\n",
      "Epoch 00614: loss did not improve from 1.00218\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0040 - categorical_accuracy: 0.6528 - val_loss: 0.1160 - val_categorical_accuracy: 0.5735\n",
      "Epoch 615/10000\n",
      "\n",
      "Epoch 00615: loss did not improve from 1.00218\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0036 - categorical_accuracy: 0.6551 - val_loss: 0.1162 - val_categorical_accuracy: 0.5720\n",
      "Epoch 616/10000\n",
      "\n",
      "Epoch 00616: loss did not improve from 1.00218\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0038 - categorical_accuracy: 0.6557 - val_loss: 0.1160 - val_categorical_accuracy: 0.5782\n",
      "Epoch 617/10000\n",
      "\n",
      "Epoch 00617: loss did not improve from 1.00218\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0036 - categorical_accuracy: 0.6538 - val_loss: 0.1160 - val_categorical_accuracy: 0.5766\n",
      "Epoch 618/10000\n",
      "\n",
      "Epoch 00618: loss improved from 1.00218 to 1.00180, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0018 - categorical_accuracy: 0.6584 - val_loss: 0.1161 - val_categorical_accuracy: 0.5717\n",
      "Epoch 619/10000\n",
      "\n",
      "Epoch 00619: loss did not improve from 1.00180\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0024 - categorical_accuracy: 0.6584 - val_loss: 0.1161 - val_categorical_accuracy: 0.5757\n",
      "Epoch 620/10000\n",
      "\n",
      "Epoch 00620: loss did not improve from 1.00180\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0040 - categorical_accuracy: 0.6530 - val_loss: 0.1161 - val_categorical_accuracy: 0.5744\n",
      "Epoch 621/10000\n",
      "\n",
      "Epoch 00621: loss did not improve from 1.00180\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0021 - categorical_accuracy: 0.6584 - val_loss: 0.1160 - val_categorical_accuracy: 0.5766\n",
      "Epoch 622/10000\n",
      "\n",
      "Epoch 00622: loss improved from 1.00180 to 1.00167, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0017 - categorical_accuracy: 0.6585 - val_loss: 0.1161 - val_categorical_accuracy: 0.5749\n",
      "Epoch 623/10000\n",
      "\n",
      "Epoch 00623: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0023 - categorical_accuracy: 0.6562 - val_loss: 0.1161 - val_categorical_accuracy: 0.5742\n",
      "Epoch 624/10000\n",
      "\n",
      "Epoch 00624: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0024 - categorical_accuracy: 0.6575 - val_loss: 0.1161 - val_categorical_accuracy: 0.5729\n",
      "Epoch 625/10000\n",
      "\n",
      "Epoch 00625: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0031 - categorical_accuracy: 0.6558 - val_loss: 0.1161 - val_categorical_accuracy: 0.5771\n",
      "Epoch 626/10000\n",
      "\n",
      "Epoch 00626: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0040 - categorical_accuracy: 0.6534 - val_loss: 0.1164 - val_categorical_accuracy: 0.5706\n",
      "Epoch 627/10000\n",
      "\n",
      "Epoch 00627: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0048 - categorical_accuracy: 0.6531 - val_loss: 0.1163 - val_categorical_accuracy: 0.5706\n",
      "Epoch 628/10000\n",
      "\n",
      "Epoch 00628: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0089 - categorical_accuracy: 0.6413 - val_loss: 0.1162 - val_categorical_accuracy: 0.5691\n",
      "Epoch 629/10000\n",
      "\n",
      "Epoch 00629: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0064 - categorical_accuracy: 0.6468 - val_loss: 0.1162 - val_categorical_accuracy: 0.5666\n",
      "Epoch 630/10000\n",
      "\n",
      "Epoch 00630: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0079 - categorical_accuracy: 0.6459 - val_loss: 0.1161 - val_categorical_accuracy: 0.5737\n",
      "Epoch 631/10000\n",
      "\n",
      "Epoch 00631: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0086 - categorical_accuracy: 0.6415 - val_loss: 0.1160 - val_categorical_accuracy: 0.5735\n",
      "Epoch 632/10000\n",
      "\n",
      "Epoch 00632: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0021 - categorical_accuracy: 0.6587 - val_loss: 0.1162 - val_categorical_accuracy: 0.5686\n",
      "Epoch 633/10000\n",
      "\n",
      "Epoch 00633: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0059 - categorical_accuracy: 0.6507 - val_loss: 0.1164 - val_categorical_accuracy: 0.5669\n",
      "Epoch 634/10000\n",
      "\n",
      "Epoch 00634: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0138 - categorical_accuracy: 0.6282 - val_loss: 0.1160 - val_categorical_accuracy: 0.5735\n",
      "Epoch 635/10000\n",
      "\n",
      "Epoch 00635: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0039 - categorical_accuracy: 0.6524 - val_loss: 0.1164 - val_categorical_accuracy: 0.5693\n",
      "Epoch 636/10000\n",
      "\n",
      "Epoch 00636: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0088 - categorical_accuracy: 0.6445 - val_loss: 0.1163 - val_categorical_accuracy: 0.5711\n",
      "Epoch 637/10000\n",
      "\n",
      "Epoch 00637: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0130 - categorical_accuracy: 0.6298 - val_loss: 0.1160 - val_categorical_accuracy: 0.5733\n",
      "Epoch 638/10000\n",
      "\n",
      "Epoch 00638: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0047 - categorical_accuracy: 0.6495 - val_loss: 0.1169 - val_categorical_accuracy: 0.5573\n",
      "Epoch 639/10000\n",
      "\n",
      "Epoch 00639: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0154 - categorical_accuracy: 0.6320 - val_loss: 0.1162 - val_categorical_accuracy: 0.5675\n",
      "Epoch 640/10000\n",
      "\n",
      "Epoch 00640: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0152 - categorical_accuracy: 0.6240 - val_loss: 0.1161 - val_categorical_accuracy: 0.5713\n",
      "Epoch 641/10000\n",
      "\n",
      "Epoch 00641: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0116 - categorical_accuracy: 0.6327 - val_loss: 0.1166 - val_categorical_accuracy: 0.5653\n",
      "Epoch 642/10000\n",
      "\n",
      "Epoch 00642: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0145 - categorical_accuracy: 0.6309 - val_loss: 0.1161 - val_categorical_accuracy: 0.5742\n",
      "Epoch 643/10000\n",
      "\n",
      "Epoch 00643: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0068 - categorical_accuracy: 0.6484 - val_loss: 0.1162 - val_categorical_accuracy: 0.5717\n",
      "Epoch 644/10000\n",
      "\n",
      "Epoch 00644: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0123 - categorical_accuracy: 0.6315 - val_loss: 0.1160 - val_categorical_accuracy: 0.5773\n",
      "Epoch 645/10000\n",
      "\n",
      "Epoch 00645: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0050 - categorical_accuracy: 0.6494 - val_loss: 0.1167 - val_categorical_accuracy: 0.5675\n",
      "Epoch 646/10000\n",
      "\n",
      "Epoch 00646: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0129 - categorical_accuracy: 0.6364 - val_loss: 0.1160 - val_categorical_accuracy: 0.5731\n",
      "Epoch 647/10000\n",
      "\n",
      "Epoch 00647: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0048 - categorical_accuracy: 0.6501 - val_loss: 0.1163 - val_categorical_accuracy: 0.5706\n",
      "Epoch 648/10000\n",
      "\n",
      "Epoch 00648: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0089 - categorical_accuracy: 0.6412 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 649/10000\n",
      "\n",
      "Epoch 00649: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0032 - categorical_accuracy: 0.6560 - val_loss: 0.1163 - val_categorical_accuracy: 0.5675\n",
      "Epoch 650/10000\n",
      "\n",
      "Epoch 00650: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0075 - categorical_accuracy: 0.6461 - val_loss: 0.1160 - val_categorical_accuracy: 0.5735\n",
      "Epoch 651/10000\n",
      "\n",
      "Epoch 00651: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0046 - categorical_accuracy: 0.6496 - val_loss: 0.1161 - val_categorical_accuracy: 0.5742\n",
      "Epoch 652/10000\n",
      "\n",
      "Epoch 00652: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0047 - categorical_accuracy: 0.6497 - val_loss: 0.1163 - val_categorical_accuracy: 0.5717\n",
      "Epoch 653/10000\n",
      "\n",
      "Epoch 00653: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0049 - categorical_accuracy: 0.6534 - val_loss: 0.1160 - val_categorical_accuracy: 0.5733\n",
      "Epoch 654/10000\n",
      "\n",
      "Epoch 00654: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0024 - categorical_accuracy: 0.6554 - val_loss: 0.1161 - val_categorical_accuracy: 0.5755\n",
      "Epoch 655/10000\n",
      "\n",
      "Epoch 00655: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0048 - categorical_accuracy: 0.6508 - val_loss: 0.1160 - val_categorical_accuracy: 0.5746\n",
      "Epoch 656/10000\n",
      "\n",
      "Epoch 00656: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0018 - categorical_accuracy: 0.6585 - val_loss: 0.1161 - val_categorical_accuracy: 0.5726\n",
      "Epoch 657/10000\n",
      "\n",
      "Epoch 00657: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0031 - categorical_accuracy: 0.6567 - val_loss: 0.1161 - val_categorical_accuracy: 0.5717\n",
      "Epoch 658/10000\n",
      "\n",
      "Epoch 00658: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0026 - categorical_accuracy: 0.6565 - val_loss: 0.1161 - val_categorical_accuracy: 0.5775\n",
      "Epoch 659/10000\n",
      "\n",
      "Epoch 00659: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0023 - categorical_accuracy: 0.6579 - val_loss: 0.1161 - val_categorical_accuracy: 0.5731\n",
      "Epoch 660/10000\n",
      "\n",
      "Epoch 00660: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0027 - categorical_accuracy: 0.6582 - val_loss: 0.1161 - val_categorical_accuracy: 0.5742\n",
      "Epoch 661/10000\n",
      "\n",
      "Epoch 00661: loss did not improve from 1.00167\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0019 - categorical_accuracy: 0.6585 - val_loss: 0.1161 - val_categorical_accuracy: 0.5737\n",
      "Epoch 662/10000\n",
      "\n",
      "Epoch 00662: loss improved from 1.00167 to 1.00145, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0015 - categorical_accuracy: 0.6588 - val_loss: 0.1160 - val_categorical_accuracy: 0.5753\n",
      "Epoch 663/10000\n",
      "\n",
      "Epoch 00663: loss did not improve from 1.00145\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0015 - categorical_accuracy: 0.6598 - val_loss: 0.1160 - val_categorical_accuracy: 0.5777\n",
      "Epoch 664/10000\n",
      "\n",
      "Epoch 00664: loss improved from 1.00145 to 1.00061, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0006 - categorical_accuracy: 0.6605 - val_loss: 0.1161 - val_categorical_accuracy: 0.5742\n",
      "Epoch 665/10000\n",
      "\n",
      "Epoch 00665: loss did not improve from 1.00061\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0007 - categorical_accuracy: 0.6604 - val_loss: 0.1162 - val_categorical_accuracy: 0.5737\n",
      "Epoch 666/10000\n",
      "\n",
      "Epoch 00666: loss did not improve from 1.00061\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0015 - categorical_accuracy: 0.6604 - val_loss: 0.1160 - val_categorical_accuracy: 0.5731\n",
      "Epoch 667/10000\n",
      "\n",
      "Epoch 00667: loss did not improve from 1.00061\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0021 - categorical_accuracy: 0.6580 - val_loss: 0.1162 - val_categorical_accuracy: 0.5693\n",
      "Epoch 668/10000\n",
      "\n",
      "Epoch 00668: loss did not improve from 1.00061\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0043 - categorical_accuracy: 0.6522 - val_loss: 0.1162 - val_categorical_accuracy: 0.5731\n",
      "Epoch 669/10000\n",
      "\n",
      "Epoch 00669: loss did not improve from 1.00061\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0058 - categorical_accuracy: 0.6489 - val_loss: 0.1163 - val_categorical_accuracy: 0.5686\n",
      "Epoch 670/10000\n",
      "\n",
      "Epoch 00670: loss did not improve from 1.00061\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0046 - categorical_accuracy: 0.6524 - val_loss: 0.1160 - val_categorical_accuracy: 0.5742\n",
      "Epoch 671/10000\n",
      "\n",
      "Epoch 00671: loss did not improve from 1.00061\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0030 - categorical_accuracy: 0.6547 - val_loss: 0.1160 - val_categorical_accuracy: 0.5746\n",
      "Epoch 672/10000\n",
      "\n",
      "Epoch 00672: loss improved from 1.00061 to 1.00027, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 1.0003 - categorical_accuracy: 0.6633 - val_loss: 0.1161 - val_categorical_accuracy: 0.5733\n",
      "Epoch 673/10000\n",
      "\n",
      "Epoch 00673: loss improved from 1.00027 to 1.00023, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0002 - categorical_accuracy: 0.6633 - val_loss: 0.1162 - val_categorical_accuracy: 0.5740\n",
      "Epoch 674/10000\n",
      "\n",
      "Epoch 00674: loss did not improve from 1.00023\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0026 - categorical_accuracy: 0.6568 - val_loss: 0.1163 - val_categorical_accuracy: 0.5715\n",
      "Epoch 675/10000\n",
      "\n",
      "Epoch 00675: loss did not improve from 1.00023\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0035 - categorical_accuracy: 0.6544 - val_loss: 0.1161 - val_categorical_accuracy: 0.5735\n",
      "Epoch 676/10000\n",
      "\n",
      "Epoch 00676: loss did not improve from 1.00023\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0041 - categorical_accuracy: 0.6529 - val_loss: 0.1161 - val_categorical_accuracy: 0.5731\n",
      "Epoch 677/10000\n",
      "\n",
      "Epoch 00677: loss did not improve from 1.00023\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0015 - categorical_accuracy: 0.6595 - val_loss: 0.1160 - val_categorical_accuracy: 0.5777\n",
      "Epoch 678/10000\n",
      "\n",
      "Epoch 00678: loss improved from 1.00023 to 1.00002, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0000 - categorical_accuracy: 0.6617 - val_loss: 0.1161 - val_categorical_accuracy: 0.5731\n",
      "Epoch 679/10000\n",
      "\n",
      "Epoch 00679: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0020 - categorical_accuracy: 0.6592 - val_loss: 0.1164 - val_categorical_accuracy: 0.5675\n",
      "Epoch 680/10000\n",
      "\n",
      "Epoch 00680: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0047 - categorical_accuracy: 0.6511 - val_loss: 0.1161 - val_categorical_accuracy: 0.5742\n",
      "Epoch 681/10000\n",
      "\n",
      "Epoch 00681: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0030 - categorical_accuracy: 0.6555 - val_loss: 0.1162 - val_categorical_accuracy: 0.5737\n",
      "Epoch 682/10000\n",
      "\n",
      "Epoch 00682: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0003 - categorical_accuracy: 0.6630 - val_loss: 0.1160 - val_categorical_accuracy: 0.5773\n",
      "Epoch 683/10000\n",
      "\n",
      "Epoch 00683: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0003 - categorical_accuracy: 0.6624 - val_loss: 0.1162 - val_categorical_accuracy: 0.5709\n",
      "Epoch 684/10000\n",
      "\n",
      "Epoch 00684: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0016 - categorical_accuracy: 0.6602 - val_loss: 0.1162 - val_categorical_accuracy: 0.5720\n",
      "Epoch 685/10000\n",
      "\n",
      "Epoch 00685: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0020 - categorical_accuracy: 0.6589 - val_loss: 0.1160 - val_categorical_accuracy: 0.5751\n",
      "Epoch 686/10000\n",
      "\n",
      "Epoch 00686: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 5s 71us/sample - loss: 1.0003 - categorical_accuracy: 0.6619 - val_loss: 0.1161 - val_categorical_accuracy: 0.5760\n",
      "Epoch 687/10000\n",
      "\n",
      "Epoch 00687: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0007 - categorical_accuracy: 0.6613 - val_loss: 0.1162 - val_categorical_accuracy: 0.5711\n",
      "Epoch 688/10000\n",
      "\n",
      "Epoch 00688: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0024 - categorical_accuracy: 0.6581 - val_loss: 0.1162 - val_categorical_accuracy: 0.5720\n",
      "Epoch 689/10000\n",
      "\n",
      "Epoch 00689: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0037 - categorical_accuracy: 0.6523 - val_loss: 0.1163 - val_categorical_accuracy: 0.5709\n",
      "Epoch 690/10000\n",
      "\n",
      "Epoch 00690: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0029 - categorical_accuracy: 0.6576 - val_loss: 0.1162 - val_categorical_accuracy: 0.5717\n",
      "Epoch 691/10000\n",
      "\n",
      "Epoch 00691: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0042 - categorical_accuracy: 0.6535 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 692/10000\n",
      "\n",
      "Epoch 00692: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0031 - categorical_accuracy: 0.6546 - val_loss: 0.1160 - val_categorical_accuracy: 0.5749\n",
      "Epoch 693/10000\n",
      "\n",
      "Epoch 00693: loss did not improve from 1.00002\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0002 - categorical_accuracy: 0.6620 - val_loss: 0.1161 - val_categorical_accuracy: 0.5746\n",
      "Epoch 694/10000\n",
      "\n",
      "Epoch 00694: loss improved from 1.00002 to 0.99927, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9993 - categorical_accuracy: 0.6660 - val_loss: 0.1163 - val_categorical_accuracy: 0.5733\n",
      "Epoch 695/10000\n",
      "\n",
      "Epoch 00695: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0011 - categorical_accuracy: 0.6604 - val_loss: 0.1161 - val_categorical_accuracy: 0.5755\n",
      "Epoch 696/10000\n",
      "\n",
      "Epoch 00696: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0007 - categorical_accuracy: 0.6617 - val_loss: 0.1162 - val_categorical_accuracy: 0.5717\n",
      "Epoch 697/10000\n",
      "\n",
      "Epoch 00697: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0005 - categorical_accuracy: 0.6621 - val_loss: 0.1162 - val_categorical_accuracy: 0.5749\n",
      "Epoch 698/10000\n",
      "\n",
      "Epoch 00698: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0009 - categorical_accuracy: 0.6598 - val_loss: 0.1163 - val_categorical_accuracy: 0.5715\n",
      "Epoch 699/10000\n",
      "\n",
      "Epoch 00699: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0016 - categorical_accuracy: 0.6610 - val_loss: 0.1161 - val_categorical_accuracy: 0.5749\n",
      "Epoch 700/10000\n",
      "\n",
      "Epoch 00700: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0023 - categorical_accuracy: 0.6557 - val_loss: 0.1161 - val_categorical_accuracy: 0.5731\n",
      "Epoch 701/10000\n",
      "\n",
      "Epoch 00701: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9996 - categorical_accuracy: 0.6638 - val_loss: 0.1162 - val_categorical_accuracy: 0.5753\n",
      "Epoch 702/10000\n",
      "\n",
      "Epoch 00702: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0002 - categorical_accuracy: 0.6639 - val_loss: 0.1162 - val_categorical_accuracy: 0.5724\n",
      "Epoch 703/10000\n",
      "\n",
      "Epoch 00703: loss did not improve from 0.99927\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0015 - categorical_accuracy: 0.6582 - val_loss: 0.1160 - val_categorical_accuracy: 0.5760\n",
      "Epoch 704/10000\n",
      "\n",
      "Epoch 00704: loss improved from 0.99927 to 0.99918, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9992 - categorical_accuracy: 0.6644 - val_loss: 0.1161 - val_categorical_accuracy: 0.5766\n",
      "Epoch 705/10000\n",
      "\n",
      "Epoch 00705: loss improved from 0.99918 to 0.99884, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9988 - categorical_accuracy: 0.6670 - val_loss: 0.1162 - val_categorical_accuracy: 0.5755\n",
      "Epoch 706/10000\n",
      "\n",
      "Epoch 00706: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0007 - categorical_accuracy: 0.6618 - val_loss: 0.1161 - val_categorical_accuracy: 0.5751\n",
      "Epoch 707/10000\n",
      "\n",
      "Epoch 00707: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9998 - categorical_accuracy: 0.6642 - val_loss: 0.1162 - val_categorical_accuracy: 0.5740\n",
      "Epoch 708/10000\n",
      "\n",
      "Epoch 00708: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0004 - categorical_accuracy: 0.6624 - val_loss: 0.1163 - val_categorical_accuracy: 0.5695\n",
      "Epoch 709/10000\n",
      "\n",
      "Epoch 00709: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0016 - categorical_accuracy: 0.6583 - val_loss: 0.1164 - val_categorical_accuracy: 0.5709\n",
      "Epoch 710/10000\n",
      "\n",
      "Epoch 00710: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0024 - categorical_accuracy: 0.6595 - val_loss: 0.1163 - val_categorical_accuracy: 0.5684\n",
      "Epoch 711/10000\n",
      "\n",
      "Epoch 00711: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0061 - categorical_accuracy: 0.6465 - val_loss: 0.1162 - val_categorical_accuracy: 0.5709\n",
      "Epoch 712/10000\n",
      "\n",
      "Epoch 00712: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0024 - categorical_accuracy: 0.6569 - val_loss: 0.1162 - val_categorical_accuracy: 0.5715\n",
      "Epoch 713/10000\n",
      "\n",
      "Epoch 00713: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0025 - categorical_accuracy: 0.6568 - val_loss: 0.1163 - val_categorical_accuracy: 0.5702\n",
      "Epoch 714/10000\n",
      "\n",
      "Epoch 00714: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 1.0037 - categorical_accuracy: 0.6526 - val_loss: 0.1162 - val_categorical_accuracy: 0.5746\n",
      "Epoch 715/10000\n",
      "\n",
      "Epoch 00715: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0023 - categorical_accuracy: 0.6576 - val_loss: 0.1163 - val_categorical_accuracy: 0.5711\n",
      "Epoch 716/10000\n",
      "\n",
      "Epoch 00716: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0035 - categorical_accuracy: 0.6557 - val_loss: 0.1162 - val_categorical_accuracy: 0.5775\n",
      "Epoch 717/10000\n",
      "\n",
      "Epoch 00717: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0031 - categorical_accuracy: 0.6535 - val_loss: 0.1162 - val_categorical_accuracy: 0.5735\n",
      "Epoch 718/10000\n",
      "\n",
      "Epoch 00718: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9996 - categorical_accuracy: 0.6644 - val_loss: 0.1163 - val_categorical_accuracy: 0.5678\n",
      "Epoch 719/10000\n",
      "\n",
      "Epoch 00719: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0035 - categorical_accuracy: 0.6566 - val_loss: 0.1161 - val_categorical_accuracy: 0.5753\n",
      "Epoch 720/10000\n",
      "\n",
      "Epoch 00720: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0054 - categorical_accuracy: 0.6487 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 721/10000\n",
      "\n",
      "Epoch 00721: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0003 - categorical_accuracy: 0.6624 - val_loss: 0.1165 - val_categorical_accuracy: 0.5720\n",
      "Epoch 722/10000\n",
      "\n",
      "Epoch 00722: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0041 - categorical_accuracy: 0.6557 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 723/10000\n",
      "\n",
      "Epoch 00723: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0045 - categorical_accuracy: 0.6508 - val_loss: 0.1161 - val_categorical_accuracy: 0.5717\n",
      "Epoch 724/10000\n",
      "\n",
      "Epoch 00724: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0009 - categorical_accuracy: 0.6606 - val_loss: 0.1165 - val_categorical_accuracy: 0.5724\n",
      "Epoch 725/10000\n",
      "\n",
      "Epoch 00725: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0060 - categorical_accuracy: 0.6530 - val_loss: 0.1162 - val_categorical_accuracy: 0.5762\n",
      "Epoch 726/10000\n",
      "\n",
      "Epoch 00726: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0041 - categorical_accuracy: 0.6518 - val_loss: 0.1166 - val_categorical_accuracy: 0.5655\n",
      "Epoch 727/10000\n",
      "\n",
      "Epoch 00727: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0065 - categorical_accuracy: 0.6460 - val_loss: 0.1161 - val_categorical_accuracy: 0.5757\n",
      "Epoch 728/10000\n",
      "\n",
      "Epoch 00728: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0039 - categorical_accuracy: 0.6552 - val_loss: 0.1163 - val_categorical_accuracy: 0.5740\n",
      "Epoch 729/10000\n",
      "\n",
      "Epoch 00729: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0047 - categorical_accuracy: 0.6520 - val_loss: 0.1162 - val_categorical_accuracy: 0.5753\n",
      "Epoch 730/10000\n",
      "\n",
      "Epoch 00730: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0018 - categorical_accuracy: 0.6578 - val_loss: 0.1163 - val_categorical_accuracy: 0.5700\n",
      "Epoch 731/10000\n",
      "\n",
      "Epoch 00731: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0023 - categorical_accuracy: 0.6566 - val_loss: 0.1162 - val_categorical_accuracy: 0.5729\n",
      "Epoch 732/10000\n",
      "\n",
      "Epoch 00732: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0024 - categorical_accuracy: 0.6573 - val_loss: 0.1162 - val_categorical_accuracy: 0.5731\n",
      "Epoch 733/10000\n",
      "\n",
      "Epoch 00733: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0019 - categorical_accuracy: 0.6573 - val_loss: 0.1163 - val_categorical_accuracy: 0.5711\n",
      "Epoch 734/10000\n",
      "\n",
      "Epoch 00734: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0015 - categorical_accuracy: 0.6592 - val_loss: 0.1161 - val_categorical_accuracy: 0.5768\n",
      "Epoch 735/10000\n",
      "\n",
      "Epoch 00735: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0014 - categorical_accuracy: 0.6587 - val_loss: 0.1163 - val_categorical_accuracy: 0.5704\n",
      "Epoch 736/10000\n",
      "\n",
      "Epoch 00736: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0009 - categorical_accuracy: 0.6614 - val_loss: 0.1161 - val_categorical_accuracy: 0.5726\n",
      "Epoch 737/10000\n",
      "\n",
      "Epoch 00737: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0005 - categorical_accuracy: 0.6611 - val_loss: 0.1161 - val_categorical_accuracy: 0.5722\n",
      "Epoch 738/10000\n",
      "\n",
      "Epoch 00738: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0016 - categorical_accuracy: 0.6577 - val_loss: 0.1162 - val_categorical_accuracy: 0.5749\n",
      "Epoch 739/10000\n",
      "\n",
      "Epoch 00739: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9995 - categorical_accuracy: 0.6634 - val_loss: 0.1162 - val_categorical_accuracy: 0.5729\n",
      "Epoch 740/10000\n",
      "\n",
      "Epoch 00740: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9991 - categorical_accuracy: 0.6644 - val_loss: 0.1160 - val_categorical_accuracy: 0.5753\n",
      "Epoch 741/10000\n",
      "\n",
      "Epoch 00741: loss did not improve from 0.99884\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0000 - categorical_accuracy: 0.6617 - val_loss: 0.1161 - val_categorical_accuracy: 0.5724\n",
      "Epoch 742/10000\n",
      "\n",
      "Epoch 00742: loss improved from 0.99884 to 0.99872, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9987 - categorical_accuracy: 0.6660 - val_loss: 0.1163 - val_categorical_accuracy: 0.5724\n",
      "Epoch 743/10000\n",
      "\n",
      "Epoch 00743: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9999 - categorical_accuracy: 0.6629 - val_loss: 0.1160 - val_categorical_accuracy: 0.5755\n",
      "Epoch 744/10000\n",
      "\n",
      "Epoch 00744: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9998 - categorical_accuracy: 0.6628 - val_loss: 0.1163 - val_categorical_accuracy: 0.5706\n",
      "Epoch 745/10000\n",
      "\n",
      "Epoch 00745: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0000 - categorical_accuracy: 0.6619 - val_loss: 0.1162 - val_categorical_accuracy: 0.5740\n",
      "Epoch 746/10000\n",
      "\n",
      "Epoch 00746: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9994 - categorical_accuracy: 0.6641 - val_loss: 0.1164 - val_categorical_accuracy: 0.5740\n",
      "Epoch 747/10000\n",
      "\n",
      "Epoch 00747: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9996 - categorical_accuracy: 0.6652 - val_loss: 0.1161 - val_categorical_accuracy: 0.5735\n",
      "Epoch 748/10000\n",
      "\n",
      "Epoch 00748: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9994 - categorical_accuracy: 0.6632 - val_loss: 0.1164 - val_categorical_accuracy: 0.5697\n",
      "Epoch 749/10000\n",
      "\n",
      "Epoch 00749: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0027 - categorical_accuracy: 0.6567 - val_loss: 0.1166 - val_categorical_accuracy: 0.5633\n",
      "Epoch 750/10000\n",
      "\n",
      "Epoch 00750: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0088 - categorical_accuracy: 0.6423 - val_loss: 0.1166 - val_categorical_accuracy: 0.5664\n",
      "Epoch 751/10000\n",
      "\n",
      "Epoch 00751: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0038 - categorical_accuracy: 0.6555 - val_loss: 0.1160 - val_categorical_accuracy: 0.5762\n",
      "Epoch 752/10000\n",
      "\n",
      "Epoch 00752: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0008 - categorical_accuracy: 0.6602 - val_loss: 0.1161 - val_categorical_accuracy: 0.5757\n",
      "Epoch 753/10000\n",
      "\n",
      "Epoch 00753: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9989 - categorical_accuracy: 0.6663 - val_loss: 0.1163 - val_categorical_accuracy: 0.5731\n",
      "Epoch 754/10000\n",
      "\n",
      "Epoch 00754: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 1.0017 - categorical_accuracy: 0.6586 - val_loss: 0.1162 - val_categorical_accuracy: 0.5749\n",
      "Epoch 755/10000\n",
      "\n",
      "Epoch 00755: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0026 - categorical_accuracy: 0.6559 - val_loss: 0.1167 - val_categorical_accuracy: 0.5678\n",
      "Epoch 756/10000\n",
      "\n",
      "Epoch 00756: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 5s 64us/sample - loss: 1.0043 - categorical_accuracy: 0.6536 - val_loss: 0.1162 - val_categorical_accuracy: 0.5731\n",
      "Epoch 757/10000\n",
      "\n",
      "Epoch 00757: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0025 - categorical_accuracy: 0.6551 - val_loss: 0.1161 - val_categorical_accuracy: 0.5711\n",
      "Epoch 758/10000\n",
      "\n",
      "Epoch 00758: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9989 - categorical_accuracy: 0.6653 - val_loss: 0.1162 - val_categorical_accuracy: 0.5735\n",
      "Epoch 759/10000\n",
      "\n",
      "Epoch 00759: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9994 - categorical_accuracy: 0.6647 - val_loss: 0.1163 - val_categorical_accuracy: 0.5695\n",
      "Epoch 760/10000\n",
      "\n",
      "Epoch 00760: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0022 - categorical_accuracy: 0.6562 - val_loss: 0.1164 - val_categorical_accuracy: 0.5711\n",
      "Epoch 761/10000\n",
      "\n",
      "Epoch 00761: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0007 - categorical_accuracy: 0.6629 - val_loss: 0.1161 - val_categorical_accuracy: 0.5735\n",
      "Epoch 762/10000\n",
      "\n",
      "Epoch 00762: loss did not improve from 0.99872\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9989 - categorical_accuracy: 0.6655 - val_loss: 0.1162 - val_categorical_accuracy: 0.5717\n",
      "Epoch 763/10000\n",
      "\n",
      "Epoch 00763: loss improved from 0.99872 to 0.99868, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9987 - categorical_accuracy: 0.6644 - val_loss: 0.1163 - val_categorical_accuracy: 0.5717\n",
      "Epoch 764/10000\n",
      "\n",
      "Epoch 00764: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0009 - categorical_accuracy: 0.6613 - val_loss: 0.1163 - val_categorical_accuracy: 0.5684\n",
      "Epoch 765/10000\n",
      "\n",
      "Epoch 00765: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0017 - categorical_accuracy: 0.6585 - val_loss: 0.1161 - val_categorical_accuracy: 0.5720\n",
      "Epoch 766/10000\n",
      "\n",
      "Epoch 00766: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0001 - categorical_accuracy: 0.6622 - val_loss: 0.1162 - val_categorical_accuracy: 0.5711\n",
      "Epoch 767/10000\n",
      "\n",
      "Epoch 00767: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9993 - categorical_accuracy: 0.6639 - val_loss: 0.1163 - val_categorical_accuracy: 0.5726\n",
      "Epoch 768/10000\n",
      "\n",
      "Epoch 00768: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0004 - categorical_accuracy: 0.6611 - val_loss: 0.1163 - val_categorical_accuracy: 0.5709\n",
      "Epoch 769/10000\n",
      "\n",
      "Epoch 00769: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0007 - categorical_accuracy: 0.6611 - val_loss: 0.1163 - val_categorical_accuracy: 0.5686\n",
      "Epoch 770/10000\n",
      "\n",
      "Epoch 00770: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 1.0022 - categorical_accuracy: 0.6566 - val_loss: 0.1161 - val_categorical_accuracy: 0.5742\n",
      "Epoch 771/10000\n",
      "\n",
      "Epoch 00771: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0001 - categorical_accuracy: 0.6606 - val_loss: 0.1163 - val_categorical_accuracy: 0.5744\n",
      "Epoch 772/10000\n",
      "\n",
      "Epoch 00772: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9988 - categorical_accuracy: 0.6661 - val_loss: 0.1164 - val_categorical_accuracy: 0.5666\n",
      "Epoch 773/10000\n",
      "\n",
      "Epoch 00773: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0003 - categorical_accuracy: 0.6601 - val_loss: 0.1161 - val_categorical_accuracy: 0.5757\n",
      "Epoch 774/10000\n",
      "\n",
      "Epoch 00774: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0002 - categorical_accuracy: 0.6610 - val_loss: 0.1163 - val_categorical_accuracy: 0.5751\n",
      "Epoch 775/10000\n",
      "\n",
      "Epoch 00775: loss did not improve from 0.99868\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9990 - categorical_accuracy: 0.6660 - val_loss: 0.1161 - val_categorical_accuracy: 0.5764\n",
      "Epoch 776/10000\n",
      "\n",
      "Epoch 00776: loss improved from 0.99868 to 0.99730, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9973 - categorical_accuracy: 0.6691 - val_loss: 0.1162 - val_categorical_accuracy: 0.5746\n",
      "Epoch 777/10000\n",
      "\n",
      "Epoch 00777: loss did not improve from 0.99730\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9982 - categorical_accuracy: 0.6665 - val_loss: 0.1161 - val_categorical_accuracy: 0.5753\n",
      "Epoch 778/10000\n",
      "\n",
      "Epoch 00778: loss did not improve from 0.99730\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9983 - categorical_accuracy: 0.6660 - val_loss: 0.1161 - val_categorical_accuracy: 0.5731\n",
      "Epoch 779/10000\n",
      "\n",
      "Epoch 00779: loss improved from 0.99730 to 0.99601, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9960 - categorical_accuracy: 0.6722 - val_loss: 0.1163 - val_categorical_accuracy: 0.5764\n",
      "Epoch 780/10000\n",
      "\n",
      "Epoch 00780: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9972 - categorical_accuracy: 0.6701 - val_loss: 0.1162 - val_categorical_accuracy: 0.5753\n",
      "Epoch 781/10000\n",
      "\n",
      "Epoch 00781: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9979 - categorical_accuracy: 0.6675 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 782/10000\n",
      "\n",
      "Epoch 00782: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9975 - categorical_accuracy: 0.6690 - val_loss: 0.1162 - val_categorical_accuracy: 0.5773\n",
      "Epoch 783/10000\n",
      "\n",
      "Epoch 00783: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9973 - categorical_accuracy: 0.6682 - val_loss: 0.1165 - val_categorical_accuracy: 0.5691\n",
      "Epoch 784/10000\n",
      "\n",
      "Epoch 00784: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9987 - categorical_accuracy: 0.6669 - val_loss: 0.1162 - val_categorical_accuracy: 0.5742\n",
      "Epoch 785/10000\n",
      "\n",
      "Epoch 00785: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0021 - categorical_accuracy: 0.6554 - val_loss: 0.1164 - val_categorical_accuracy: 0.5680\n",
      "Epoch 786/10000\n",
      "\n",
      "Epoch 00786: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0004 - categorical_accuracy: 0.6610 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 787/10000\n",
      "\n",
      "Epoch 00787: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0000 - categorical_accuracy: 0.6629 - val_loss: 0.1163 - val_categorical_accuracy: 0.5720\n",
      "Epoch 788/10000\n",
      "\n",
      "Epoch 00788: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9990 - categorical_accuracy: 0.6631 - val_loss: 0.1163 - val_categorical_accuracy: 0.5742\n",
      "Epoch 789/10000\n",
      "\n",
      "Epoch 00789: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9978 - categorical_accuracy: 0.6690 - val_loss: 0.1161 - val_categorical_accuracy: 0.5764\n",
      "Epoch 790/10000\n",
      "\n",
      "Epoch 00790: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9985 - categorical_accuracy: 0.6652 - val_loss: 0.1165 - val_categorical_accuracy: 0.5709\n",
      "Epoch 791/10000\n",
      "\n",
      "Epoch 00791: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9995 - categorical_accuracy: 0.6643 - val_loss: 0.1163 - val_categorical_accuracy: 0.5726\n",
      "Epoch 792/10000\n",
      "\n",
      "Epoch 00792: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0016 - categorical_accuracy: 0.6582 - val_loss: 0.1165 - val_categorical_accuracy: 0.5651\n",
      "Epoch 793/10000\n",
      "\n",
      "Epoch 00793: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 1.0039 - categorical_accuracy: 0.6497 - val_loss: 0.1162 - val_categorical_accuracy: 0.5729\n",
      "Epoch 794/10000\n",
      "\n",
      "Epoch 00794: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9995 - categorical_accuracy: 0.6634 - val_loss: 0.1161 - val_categorical_accuracy: 0.5746\n",
      "Epoch 795/10000\n",
      "\n",
      "Epoch 00795: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9982 - categorical_accuracy: 0.6675 - val_loss: 0.1165 - val_categorical_accuracy: 0.5664\n",
      "Epoch 796/10000\n",
      "\n",
      "Epoch 00796: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0027 - categorical_accuracy: 0.6550 - val_loss: 0.1164 - val_categorical_accuracy: 0.5706\n",
      "Epoch 797/10000\n",
      "\n",
      "Epoch 00797: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 1.0035 - categorical_accuracy: 0.6558 - val_loss: 0.1163 - val_categorical_accuracy: 0.5704\n",
      "Epoch 798/10000\n",
      "\n",
      "Epoch 00798: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9984 - categorical_accuracy: 0.6666 - val_loss: 0.1163 - val_categorical_accuracy: 0.5731\n",
      "Epoch 799/10000\n",
      "\n",
      "Epoch 00799: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9986 - categorical_accuracy: 0.6642 - val_loss: 0.1163 - val_categorical_accuracy: 0.5726\n",
      "Epoch 800/10000\n",
      "\n",
      "Epoch 00800: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0001 - categorical_accuracy: 0.6621 - val_loss: 0.1162 - val_categorical_accuracy: 0.5742\n",
      "Epoch 801/10000\n",
      "\n",
      "Epoch 00801: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9987 - categorical_accuracy: 0.6649 - val_loss: 0.1161 - val_categorical_accuracy: 0.5777\n",
      "Epoch 802/10000\n",
      "\n",
      "Epoch 00802: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9976 - categorical_accuracy: 0.6678 - val_loss: 0.1163 - val_categorical_accuracy: 0.5724\n",
      "Epoch 803/10000\n",
      "\n",
      "Epoch 00803: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9983 - categorical_accuracy: 0.6676 - val_loss: 0.1163 - val_categorical_accuracy: 0.5731\n",
      "Epoch 804/10000\n",
      "\n",
      "Epoch 00804: loss did not improve from 0.99601\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9993 - categorical_accuracy: 0.6618 - val_loss: 0.1162 - val_categorical_accuracy: 0.5742\n",
      "Epoch 805/10000\n",
      "\n",
      "Epoch 00805: loss improved from 0.99601 to 0.99588, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9959 - categorical_accuracy: 0.6720 - val_loss: 0.1163 - val_categorical_accuracy: 0.5729\n",
      "Epoch 806/10000\n",
      "\n",
      "Epoch 00806: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9979 - categorical_accuracy: 0.6687 - val_loss: 0.1163 - val_categorical_accuracy: 0.5711\n",
      "Epoch 807/10000\n",
      "\n",
      "Epoch 00807: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0012 - categorical_accuracy: 0.6582 - val_loss: 0.1162 - val_categorical_accuracy: 0.5729\n",
      "Epoch 808/10000\n",
      "\n",
      "Epoch 00808: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9960 - categorical_accuracy: 0.6732 - val_loss: 0.1163 - val_categorical_accuracy: 0.5746\n",
      "Epoch 809/10000\n",
      "\n",
      "Epoch 00809: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9993 - categorical_accuracy: 0.6662 - val_loss: 0.1165 - val_categorical_accuracy: 0.5669\n",
      "Epoch 810/10000\n",
      "\n",
      "Epoch 00810: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0036 - categorical_accuracy: 0.6510 - val_loss: 0.1161 - val_categorical_accuracy: 0.5751\n",
      "Epoch 811/10000\n",
      "\n",
      "Epoch 00811: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9964 - categorical_accuracy: 0.6714 - val_loss: 0.1166 - val_categorical_accuracy: 0.5724\n",
      "Epoch 812/10000\n",
      "\n",
      "Epoch 00812: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 1.0054 - categorical_accuracy: 0.6520 - val_loss: 0.1165 - val_categorical_accuracy: 0.5697\n",
      "Epoch 813/10000\n",
      "\n",
      "Epoch 00813: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0058 - categorical_accuracy: 0.6457 - val_loss: 0.1165 - val_categorical_accuracy: 0.5682\n",
      "Epoch 814/10000\n",
      "\n",
      "Epoch 00814: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0035 - categorical_accuracy: 0.6519 - val_loss: 0.1164 - val_categorical_accuracy: 0.5711\n",
      "Epoch 815/10000\n",
      "\n",
      "Epoch 00815: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0060 - categorical_accuracy: 0.6504 - val_loss: 0.1163 - val_categorical_accuracy: 0.5715\n",
      "Epoch 816/10000\n",
      "\n",
      "Epoch 00816: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0009 - categorical_accuracy: 0.6595 - val_loss: 0.1164 - val_categorical_accuracy: 0.5735\n",
      "Epoch 817/10000\n",
      "\n",
      "Epoch 00817: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0016 - categorical_accuracy: 0.6581 - val_loss: 0.1164 - val_categorical_accuracy: 0.5700\n",
      "Epoch 818/10000\n",
      "\n",
      "Epoch 00818: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9999 - categorical_accuracy: 0.6628 - val_loss: 0.1163 - val_categorical_accuracy: 0.5717\n",
      "Epoch 819/10000\n",
      "\n",
      "Epoch 00819: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0026 - categorical_accuracy: 0.6585 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 820/10000\n",
      "\n",
      "Epoch 00820: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0004 - categorical_accuracy: 0.6602 - val_loss: 0.1166 - val_categorical_accuracy: 0.5702\n",
      "Epoch 821/10000\n",
      "\n",
      "Epoch 00821: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0012 - categorical_accuracy: 0.6600 - val_loss: 0.1161 - val_categorical_accuracy: 0.5760\n",
      "Epoch 822/10000\n",
      "\n",
      "Epoch 00822: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9994 - categorical_accuracy: 0.6620 - val_loss: 0.1163 - val_categorical_accuracy: 0.5717\n",
      "Epoch 823/10000\n",
      "\n",
      "Epoch 00823: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9998 - categorical_accuracy: 0.6654 - val_loss: 0.1163 - val_categorical_accuracy: 0.5729\n",
      "Epoch 824/10000\n",
      "\n",
      "Epoch 00824: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9992 - categorical_accuracy: 0.6633 - val_loss: 0.1162 - val_categorical_accuracy: 0.5780\n",
      "Epoch 825/10000\n",
      "\n",
      "Epoch 00825: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0007 - categorical_accuracy: 0.6587 - val_loss: 0.1163 - val_categorical_accuracy: 0.5704\n",
      "Epoch 826/10000\n",
      "\n",
      "Epoch 00826: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 0.9989 - categorical_accuracy: 0.6662 - val_loss: 0.1164 - val_categorical_accuracy: 0.5726\n",
      "Epoch 827/10000\n",
      "\n",
      "Epoch 00827: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9996 - categorical_accuracy: 0.6633 - val_loss: 0.1163 - val_categorical_accuracy: 0.5720\n",
      "Epoch 828/10000\n",
      "\n",
      "Epoch 00828: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9978 - categorical_accuracy: 0.6662 - val_loss: 0.1162 - val_categorical_accuracy: 0.5742\n",
      "Epoch 829/10000\n",
      "\n",
      "Epoch 00829: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9979 - categorical_accuracy: 0.6671 - val_loss: 0.1163 - val_categorical_accuracy: 0.5740\n",
      "Epoch 830/10000\n",
      "\n",
      "Epoch 00830: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9982 - categorical_accuracy: 0.6671 - val_loss: 0.1163 - val_categorical_accuracy: 0.5722\n",
      "Epoch 831/10000\n",
      "\n",
      "Epoch 00831: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9968 - categorical_accuracy: 0.6695 - val_loss: 0.1163 - val_categorical_accuracy: 0.5742\n",
      "Epoch 832/10000\n",
      "\n",
      "Epoch 00832: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0006 - categorical_accuracy: 0.6595 - val_loss: 0.1163 - val_categorical_accuracy: 0.5709\n",
      "Epoch 833/10000\n",
      "\n",
      "Epoch 00833: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9992 - categorical_accuracy: 0.6634 - val_loss: 0.1162 - val_categorical_accuracy: 0.5757\n",
      "Epoch 834/10000\n",
      "\n",
      "Epoch 00834: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9992 - categorical_accuracy: 0.6637 - val_loss: 0.1162 - val_categorical_accuracy: 0.5737\n",
      "Epoch 835/10000\n",
      "\n",
      "Epoch 00835: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9966 - categorical_accuracy: 0.6704 - val_loss: 0.1162 - val_categorical_accuracy: 0.5749\n",
      "Epoch 836/10000\n",
      "\n",
      "Epoch 00836: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9973 - categorical_accuracy: 0.6668 - val_loss: 0.1162 - val_categorical_accuracy: 0.5742\n",
      "Epoch 837/10000\n",
      "\n",
      "Epoch 00837: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9969 - categorical_accuracy: 0.6706 - val_loss: 0.1163 - val_categorical_accuracy: 0.5760\n",
      "Epoch 838/10000\n",
      "\n",
      "Epoch 00838: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9978 - categorical_accuracy: 0.6670 - val_loss: 0.1162 - val_categorical_accuracy: 0.5753\n",
      "Epoch 839/10000\n",
      "\n",
      "Epoch 00839: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9970 - categorical_accuracy: 0.6693 - val_loss: 0.1163 - val_categorical_accuracy: 0.5726\n",
      "Epoch 840/10000\n",
      "\n",
      "Epoch 00840: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 0.9972 - categorical_accuracy: 0.6691 - val_loss: 0.1162 - val_categorical_accuracy: 0.5764\n",
      "Epoch 841/10000\n",
      "\n",
      "Epoch 00841: loss did not improve from 0.99588\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9973 - categorical_accuracy: 0.6682 - val_loss: 0.1163 - val_categorical_accuracy: 0.5726\n",
      "Epoch 842/10000\n",
      "\n",
      "Epoch 00842: loss improved from 0.99588 to 0.99583, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9958 - categorical_accuracy: 0.6730 - val_loss: 0.1161 - val_categorical_accuracy: 0.5766\n",
      "Epoch 843/10000\n",
      "\n",
      "Epoch 00843: loss improved from 0.99583 to 0.99582, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9958 - categorical_accuracy: 0.6709 - val_loss: 0.1162 - val_categorical_accuracy: 0.5751\n",
      "Epoch 844/10000\n",
      "\n",
      "Epoch 00844: loss improved from 0.99582 to 0.99449, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9945 - categorical_accuracy: 0.6751 - val_loss: 0.1163 - val_categorical_accuracy: 0.5755\n",
      "Epoch 845/10000\n",
      "\n",
      "Epoch 00845: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9948 - categorical_accuracy: 0.6745 - val_loss: 0.1162 - val_categorical_accuracy: 0.5753\n",
      "Epoch 846/10000\n",
      "\n",
      "Epoch 00846: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9960 - categorical_accuracy: 0.6719 - val_loss: 0.1164 - val_categorical_accuracy: 0.5713\n",
      "Epoch 847/10000\n",
      "\n",
      "Epoch 00847: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9958 - categorical_accuracy: 0.6717 - val_loss: 0.1163 - val_categorical_accuracy: 0.5729\n",
      "Epoch 848/10000\n",
      "\n",
      "Epoch 00848: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9974 - categorical_accuracy: 0.6689 - val_loss: 0.1164 - val_categorical_accuracy: 0.5706\n",
      "Epoch 849/10000\n",
      "\n",
      "Epoch 00849: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9989 - categorical_accuracy: 0.6643 - val_loss: 0.1165 - val_categorical_accuracy: 0.5726\n",
      "Epoch 850/10000\n",
      "\n",
      "Epoch 00850: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9982 - categorical_accuracy: 0.6678 - val_loss: 0.1164 - val_categorical_accuracy: 0.5737\n",
      "Epoch 851/10000\n",
      "\n",
      "Epoch 00851: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0022 - categorical_accuracy: 0.6543 - val_loss: 0.1167 - val_categorical_accuracy: 0.5655\n",
      "Epoch 852/10000\n",
      "\n",
      "Epoch 00852: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0024 - categorical_accuracy: 0.6558 - val_loss: 0.1163 - val_categorical_accuracy: 0.5709\n",
      "Epoch 853/10000\n",
      "\n",
      "Epoch 00853: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0031 - categorical_accuracy: 0.6528 - val_loss: 0.1162 - val_categorical_accuracy: 0.5724\n",
      "Epoch 854/10000\n",
      "\n",
      "Epoch 00854: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 0.9963 - categorical_accuracy: 0.6709 - val_loss: 0.1164 - val_categorical_accuracy: 0.5711\n",
      "Epoch 855/10000\n",
      "\n",
      "Epoch 00855: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9969 - categorical_accuracy: 0.6702 - val_loss: 0.1164 - val_categorical_accuracy: 0.5702\n",
      "Epoch 856/10000\n",
      "\n",
      "Epoch 00856: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0028 - categorical_accuracy: 0.6549 - val_loss: 0.1166 - val_categorical_accuracy: 0.5706\n",
      "Epoch 857/10000\n",
      "\n",
      "Epoch 00857: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9995 - categorical_accuracy: 0.6642 - val_loss: 0.1162 - val_categorical_accuracy: 0.5771\n",
      "Epoch 858/10000\n",
      "\n",
      "Epoch 00858: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9959 - categorical_accuracy: 0.6709 - val_loss: 0.1162 - val_categorical_accuracy: 0.5757\n",
      "Epoch 859/10000\n",
      "\n",
      "Epoch 00859: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9960 - categorical_accuracy: 0.6708 - val_loss: 0.1164 - val_categorical_accuracy: 0.5733\n",
      "Epoch 860/10000\n",
      "\n",
      "Epoch 00860: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9974 - categorical_accuracy: 0.6687 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 861/10000\n",
      "\n",
      "Epoch 00861: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9983 - categorical_accuracy: 0.6659 - val_loss: 0.1164 - val_categorical_accuracy: 0.5720\n",
      "Epoch 862/10000\n",
      "\n",
      "Epoch 00862: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 0.9958 - categorical_accuracy: 0.6733 - val_loss: 0.1163 - val_categorical_accuracy: 0.5744\n",
      "Epoch 863/10000\n",
      "\n",
      "Epoch 00863: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9953 - categorical_accuracy: 0.6719 - val_loss: 0.1162 - val_categorical_accuracy: 0.5744\n",
      "Epoch 864/10000\n",
      "\n",
      "Epoch 00864: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9951 - categorical_accuracy: 0.6734 - val_loss: 0.1163 - val_categorical_accuracy: 0.5753\n",
      "Epoch 865/10000\n",
      "\n",
      "Epoch 00865: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9959 - categorical_accuracy: 0.6716 - val_loss: 0.1163 - val_categorical_accuracy: 0.5737\n",
      "Epoch 866/10000\n",
      "\n",
      "Epoch 00866: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9954 - categorical_accuracy: 0.6735 - val_loss: 0.1164 - val_categorical_accuracy: 0.5740\n",
      "Epoch 867/10000\n",
      "\n",
      "Epoch 00867: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9950 - categorical_accuracy: 0.6747 - val_loss: 0.1161 - val_categorical_accuracy: 0.5771\n",
      "Epoch 868/10000\n",
      "\n",
      "Epoch 00868: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 0.9945 - categorical_accuracy: 0.6744 - val_loss: 0.1163 - val_categorical_accuracy: 0.5737\n",
      "Epoch 869/10000\n",
      "\n",
      "Epoch 00869: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9952 - categorical_accuracy: 0.6744 - val_loss: 0.1163 - val_categorical_accuracy: 0.5735\n",
      "Epoch 870/10000\n",
      "\n",
      "Epoch 00870: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9973 - categorical_accuracy: 0.6673 - val_loss: 0.1164 - val_categorical_accuracy: 0.5726\n",
      "Epoch 871/10000\n",
      "\n",
      "Epoch 00871: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9972 - categorical_accuracy: 0.6673 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 872/10000\n",
      "\n",
      "Epoch 00872: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9978 - categorical_accuracy: 0.6675 - val_loss: 0.1163 - val_categorical_accuracy: 0.5700\n",
      "Epoch 873/10000\n",
      "\n",
      "Epoch 00873: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9985 - categorical_accuracy: 0.6646 - val_loss: 0.1165 - val_categorical_accuracy: 0.5702\n",
      "Epoch 874/10000\n",
      "\n",
      "Epoch 00874: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9962 - categorical_accuracy: 0.6721 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 875/10000\n",
      "\n",
      "Epoch 00875: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9957 - categorical_accuracy: 0.6700 - val_loss: 0.1164 - val_categorical_accuracy: 0.5742\n",
      "Epoch 876/10000\n",
      "\n",
      "Epoch 00876: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9950 - categorical_accuracy: 0.6748 - val_loss: 0.1163 - val_categorical_accuracy: 0.5755\n",
      "Epoch 877/10000\n",
      "\n",
      "Epoch 00877: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9979 - categorical_accuracy: 0.6667 - val_loss: 0.1164 - val_categorical_accuracy: 0.5720\n",
      "Epoch 878/10000\n",
      "\n",
      "Epoch 00878: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9975 - categorical_accuracy: 0.6675 - val_loss: 0.1163 - val_categorical_accuracy: 0.5766\n",
      "Epoch 879/10000\n",
      "\n",
      "Epoch 00879: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9956 - categorical_accuracy: 0.6739 - val_loss: 0.1164 - val_categorical_accuracy: 0.5744\n",
      "Epoch 880/10000\n",
      "\n",
      "Epoch 00880: loss did not improve from 0.99449\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9966 - categorical_accuracy: 0.6693 - val_loss: 0.1162 - val_categorical_accuracy: 0.5749\n",
      "Epoch 881/10000\n",
      "\n",
      "Epoch 00881: loss improved from 0.99449 to 0.99414, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 69us/sample - loss: 0.9941 - categorical_accuracy: 0.6749 - val_loss: 0.1163 - val_categorical_accuracy: 0.5768\n",
      "Epoch 882/10000\n",
      "\n",
      "Epoch 00882: loss did not improve from 0.99414\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9945 - categorical_accuracy: 0.6758 - val_loss: 0.1165 - val_categorical_accuracy: 0.5726\n",
      "Epoch 883/10000\n",
      "\n",
      "Epoch 00883: loss did not improve from 0.99414\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9963 - categorical_accuracy: 0.6686 - val_loss: 0.1163 - val_categorical_accuracy: 0.5711\n",
      "Epoch 884/10000\n",
      "\n",
      "Epoch 00884: loss did not improve from 0.99414\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9959 - categorical_accuracy: 0.6721 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 885/10000\n",
      "\n",
      "Epoch 00885: loss did not improve from 0.99414\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9946 - categorical_accuracy: 0.6730 - val_loss: 0.1163 - val_categorical_accuracy: 0.5749\n",
      "Epoch 886/10000\n",
      "\n",
      "Epoch 00886: loss improved from 0.99414 to 0.99357, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9936 - categorical_accuracy: 0.6776 - val_loss: 0.1163 - val_categorical_accuracy: 0.5751\n",
      "Epoch 887/10000\n",
      "\n",
      "Epoch 00887: loss did not improve from 0.99357\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9949 - categorical_accuracy: 0.6743 - val_loss: 0.1164 - val_categorical_accuracy: 0.5731\n",
      "Epoch 888/10000\n",
      "\n",
      "Epoch 00888: loss did not improve from 0.99357\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9940 - categorical_accuracy: 0.6756 - val_loss: 0.1162 - val_categorical_accuracy: 0.5768\n",
      "Epoch 889/10000\n",
      "\n",
      "Epoch 00889: loss improved from 0.99357 to 0.99322, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9932 - categorical_accuracy: 0.6780 - val_loss: 0.1164 - val_categorical_accuracy: 0.5724\n",
      "Epoch 890/10000\n",
      "\n",
      "Epoch 00890: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9948 - categorical_accuracy: 0.6742 - val_loss: 0.1164 - val_categorical_accuracy: 0.5760\n",
      "Epoch 891/10000\n",
      "\n",
      "Epoch 00891: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9976 - categorical_accuracy: 0.6666 - val_loss: 0.1165 - val_categorical_accuracy: 0.5724\n",
      "Epoch 892/10000\n",
      "\n",
      "Epoch 00892: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9964 - categorical_accuracy: 0.6714 - val_loss: 0.1163 - val_categorical_accuracy: 0.5742\n",
      "Epoch 893/10000\n",
      "\n",
      "Epoch 00893: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9992 - categorical_accuracy: 0.6618 - val_loss: 0.1166 - val_categorical_accuracy: 0.5729\n",
      "Epoch 894/10000\n",
      "\n",
      "Epoch 00894: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9962 - categorical_accuracy: 0.6713 - val_loss: 0.1162 - val_categorical_accuracy: 0.5744\n",
      "Epoch 895/10000\n",
      "\n",
      "Epoch 00895: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 0.9956 - categorical_accuracy: 0.6709 - val_loss: 0.1163 - val_categorical_accuracy: 0.5729\n",
      "Epoch 896/10000\n",
      "\n",
      "Epoch 00896: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9949 - categorical_accuracy: 0.6737 - val_loss: 0.1169 - val_categorical_accuracy: 0.5600\n",
      "Epoch 897/10000\n",
      "\n",
      "Epoch 00897: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0009 - categorical_accuracy: 0.6594 - val_loss: 0.1164 - val_categorical_accuracy: 0.5704\n",
      "Epoch 898/10000\n",
      "\n",
      "Epoch 00898: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0018 - categorical_accuracy: 0.6566 - val_loss: 0.1163 - val_categorical_accuracy: 0.5715\n",
      "Epoch 899/10000\n",
      "\n",
      "Epoch 00899: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9963 - categorical_accuracy: 0.6709 - val_loss: 0.1165 - val_categorical_accuracy: 0.5704\n",
      "Epoch 900/10000\n",
      "\n",
      "Epoch 00900: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9977 - categorical_accuracy: 0.6665 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 901/10000\n",
      "\n",
      "Epoch 00901: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9961 - categorical_accuracy: 0.6720 - val_loss: 0.1165 - val_categorical_accuracy: 0.5680\n",
      "Epoch 902/10000\n",
      "\n",
      "Epoch 00902: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9977 - categorical_accuracy: 0.6683 - val_loss: 0.1164 - val_categorical_accuracy: 0.5760\n",
      "Epoch 903/10000\n",
      "\n",
      "Epoch 00903: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9988 - categorical_accuracy: 0.6639 - val_loss: 0.1165 - val_categorical_accuracy: 0.5715\n",
      "Epoch 904/10000\n",
      "\n",
      "Epoch 00904: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9950 - categorical_accuracy: 0.6730 - val_loss: 0.1165 - val_categorical_accuracy: 0.5693\n",
      "Epoch 905/10000\n",
      "\n",
      "Epoch 00905: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9965 - categorical_accuracy: 0.6713 - val_loss: 0.1162 - val_categorical_accuracy: 0.5791\n",
      "Epoch 906/10000\n",
      "\n",
      "Epoch 00906: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9979 - categorical_accuracy: 0.6643 - val_loss: 0.1167 - val_categorical_accuracy: 0.5682\n",
      "Epoch 907/10000\n",
      "\n",
      "Epoch 00907: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9975 - categorical_accuracy: 0.6691 - val_loss: 0.1164 - val_categorical_accuracy: 0.5742\n",
      "Epoch 908/10000\n",
      "\n",
      "Epoch 00908: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9990 - categorical_accuracy: 0.6647 - val_loss: 0.1163 - val_categorical_accuracy: 0.5744\n",
      "Epoch 909/10000\n",
      "\n",
      "Epoch 00909: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9960 - categorical_accuracy: 0.6695 - val_loss: 0.1168 - val_categorical_accuracy: 0.5635\n",
      "Epoch 910/10000\n",
      "\n",
      "Epoch 00910: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0002 - categorical_accuracy: 0.6615 - val_loss: 0.1164 - val_categorical_accuracy: 0.5733\n",
      "Epoch 911/10000\n",
      "\n",
      "Epoch 00911: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0006 - categorical_accuracy: 0.6598 - val_loss: 0.1166 - val_categorical_accuracy: 0.5711\n",
      "Epoch 912/10000\n",
      "\n",
      "Epoch 00912: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9995 - categorical_accuracy: 0.6649 - val_loss: 0.1165 - val_categorical_accuracy: 0.5737\n",
      "Epoch 913/10000\n",
      "\n",
      "Epoch 00913: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9991 - categorical_accuracy: 0.6629 - val_loss: 0.1162 - val_categorical_accuracy: 0.5755\n",
      "Epoch 914/10000\n",
      "\n",
      "Epoch 00914: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9990 - categorical_accuracy: 0.6618 - val_loss: 0.1164 - val_categorical_accuracy: 0.5722\n",
      "Epoch 915/10000\n",
      "\n",
      "Epoch 00915: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9964 - categorical_accuracy: 0.6726 - val_loss: 0.1165 - val_categorical_accuracy: 0.5713\n",
      "Epoch 916/10000\n",
      "\n",
      "Epoch 00916: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9977 - categorical_accuracy: 0.6666 - val_loss: 0.1163 - val_categorical_accuracy: 0.5751\n",
      "Epoch 917/10000\n",
      "\n",
      "Epoch 00917: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9980 - categorical_accuracy: 0.6661 - val_loss: 0.1163 - val_categorical_accuracy: 0.5720\n",
      "Epoch 918/10000\n",
      "\n",
      "Epoch 00918: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9979 - categorical_accuracy: 0.6675 - val_loss: 0.1165 - val_categorical_accuracy: 0.5704\n",
      "Epoch 919/10000\n",
      "\n",
      "Epoch 00919: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9977 - categorical_accuracy: 0.6661 - val_loss: 0.1162 - val_categorical_accuracy: 0.5749\n",
      "Epoch 920/10000\n",
      "\n",
      "Epoch 00920: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9956 - categorical_accuracy: 0.6719 - val_loss: 0.1165 - val_categorical_accuracy: 0.5709\n",
      "Epoch 921/10000\n",
      "\n",
      "Epoch 00921: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9978 - categorical_accuracy: 0.6680 - val_loss: 0.1164 - val_categorical_accuracy: 0.5729\n",
      "Epoch 922/10000\n",
      "\n",
      "Epoch 00922: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0000 - categorical_accuracy: 0.6593 - val_loss: 0.1164 - val_categorical_accuracy: 0.5695\n",
      "Epoch 923/10000\n",
      "\n",
      "Epoch 00923: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 69us/sample - loss: 0.9955 - categorical_accuracy: 0.6733 - val_loss: 0.1163 - val_categorical_accuracy: 0.5757\n",
      "Epoch 924/10000\n",
      "\n",
      "Epoch 00924: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9991 - categorical_accuracy: 0.6647 - val_loss: 0.1163 - val_categorical_accuracy: 0.5753\n",
      "Epoch 925/10000\n",
      "\n",
      "Epoch 00925: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9971 - categorical_accuracy: 0.6666 - val_loss: 0.1169 - val_categorical_accuracy: 0.5680\n",
      "Epoch 926/10000\n",
      "\n",
      "Epoch 00926: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0002 - categorical_accuracy: 0.6624 - val_loss: 0.1163 - val_categorical_accuracy: 0.5733\n",
      "Epoch 927/10000\n",
      "\n",
      "Epoch 00927: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0012 - categorical_accuracy: 0.6598 - val_loss: 0.1162 - val_categorical_accuracy: 0.5757\n",
      "Epoch 928/10000\n",
      "\n",
      "Epoch 00928: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 1.0005 - categorical_accuracy: 0.6585 - val_loss: 0.1166 - val_categorical_accuracy: 0.5669\n",
      "Epoch 929/10000\n",
      "\n",
      "Epoch 00929: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0006 - categorical_accuracy: 0.6590 - val_loss: 0.1163 - val_categorical_accuracy: 0.5740\n",
      "Epoch 930/10000\n",
      "\n",
      "Epoch 00930: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9975 - categorical_accuracy: 0.6670 - val_loss: 0.1165 - val_categorical_accuracy: 0.5735\n",
      "Epoch 931/10000\n",
      "\n",
      "Epoch 00931: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0010 - categorical_accuracy: 0.6603 - val_loss: 0.1164 - val_categorical_accuracy: 0.5709\n",
      "Epoch 932/10000\n",
      "\n",
      "Epoch 00932: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0004 - categorical_accuracy: 0.6599 - val_loss: 0.1163 - val_categorical_accuracy: 0.5740\n",
      "Epoch 933/10000\n",
      "\n",
      "Epoch 00933: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9961 - categorical_accuracy: 0.6706 - val_loss: 0.1164 - val_categorical_accuracy: 0.5784\n",
      "Epoch 934/10000\n",
      "\n",
      "Epoch 00934: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9989 - categorical_accuracy: 0.6673 - val_loss: 0.1162 - val_categorical_accuracy: 0.5726\n",
      "Epoch 935/10000\n",
      "\n",
      "Epoch 00935: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9972 - categorical_accuracy: 0.6675 - val_loss: 0.1165 - val_categorical_accuracy: 0.5704\n",
      "Epoch 936/10000\n",
      "\n",
      "Epoch 00936: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 1.0012 - categorical_accuracy: 0.6598 - val_loss: 0.1162 - val_categorical_accuracy: 0.5755\n",
      "Epoch 937/10000\n",
      "\n",
      "Epoch 00937: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 0.9954 - categorical_accuracy: 0.6710 - val_loss: 0.1165 - val_categorical_accuracy: 0.5740\n",
      "Epoch 938/10000\n",
      "\n",
      "Epoch 00938: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9960 - categorical_accuracy: 0.6715 - val_loss: 0.1164 - val_categorical_accuracy: 0.5735\n",
      "Epoch 939/10000\n",
      "\n",
      "Epoch 00939: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9974 - categorical_accuracy: 0.6681 - val_loss: 0.1163 - val_categorical_accuracy: 0.5733\n",
      "Epoch 940/10000\n",
      "\n",
      "Epoch 00940: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9962 - categorical_accuracy: 0.6687 - val_loss: 0.1162 - val_categorical_accuracy: 0.5784\n",
      "Epoch 941/10000\n",
      "\n",
      "Epoch 00941: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9951 - categorical_accuracy: 0.6734 - val_loss: 0.1164 - val_categorical_accuracy: 0.5751\n",
      "Epoch 942/10000\n",
      "\n",
      "Epoch 00942: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9948 - categorical_accuracy: 0.6734 - val_loss: 0.1164 - val_categorical_accuracy: 0.5726\n",
      "Epoch 943/10000\n",
      "\n",
      "Epoch 00943: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9951 - categorical_accuracy: 0.6722 - val_loss: 0.1162 - val_categorical_accuracy: 0.5744\n",
      "Epoch 944/10000\n",
      "\n",
      "Epoch 00944: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9967 - categorical_accuracy: 0.6704 - val_loss: 0.1165 - val_categorical_accuracy: 0.5702\n",
      "Epoch 945/10000\n",
      "\n",
      "Epoch 00945: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9959 - categorical_accuracy: 0.6710 - val_loss: 0.1165 - val_categorical_accuracy: 0.5737\n",
      "Epoch 946/10000\n",
      "\n",
      "Epoch 00946: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9941 - categorical_accuracy: 0.6748 - val_loss: 0.1163 - val_categorical_accuracy: 0.5749\n",
      "Epoch 947/10000\n",
      "\n",
      "Epoch 00947: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9943 - categorical_accuracy: 0.6750 - val_loss: 0.1162 - val_categorical_accuracy: 0.5771\n",
      "Epoch 948/10000\n",
      "\n",
      "Epoch 00948: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9941 - categorical_accuracy: 0.6738 - val_loss: 0.1165 - val_categorical_accuracy: 0.5702\n",
      "Epoch 949/10000\n",
      "\n",
      "Epoch 00949: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9955 - categorical_accuracy: 0.6733 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 950/10000\n",
      "\n",
      "Epoch 00950: loss did not improve from 0.99322\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9941 - categorical_accuracy: 0.6739 - val_loss: 0.1162 - val_categorical_accuracy: 0.5768\n",
      "Epoch 951/10000\n",
      "\n",
      "Epoch 00951: loss improved from 0.99322 to 0.99313, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 0.9931 - categorical_accuracy: 0.6773 - val_loss: 0.1163 - val_categorical_accuracy: 0.5749\n",
      "Epoch 952/10000\n",
      "\n",
      "Epoch 00952: loss did not improve from 0.99313\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9944 - categorical_accuracy: 0.6737 - val_loss: 0.1164 - val_categorical_accuracy: 0.5695\n",
      "Epoch 953/10000\n",
      "\n",
      "Epoch 00953: loss did not improve from 0.99313\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9951 - categorical_accuracy: 0.6733 - val_loss: 0.1163 - val_categorical_accuracy: 0.5740\n",
      "Epoch 954/10000\n",
      "\n",
      "Epoch 00954: loss did not improve from 0.99313\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9941 - categorical_accuracy: 0.6769 - val_loss: 0.1165 - val_categorical_accuracy: 0.5711\n",
      "Epoch 955/10000\n",
      "\n",
      "Epoch 00955: loss did not improve from 0.99313\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9944 - categorical_accuracy: 0.6732 - val_loss: 0.1163 - val_categorical_accuracy: 0.5746\n",
      "Epoch 956/10000\n",
      "\n",
      "Epoch 00956: loss improved from 0.99313 to 0.99214, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9921 - categorical_accuracy: 0.6795 - val_loss: 0.1162 - val_categorical_accuracy: 0.5764\n",
      "Epoch 957/10000\n",
      "\n",
      "Epoch 00957: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9940 - categorical_accuracy: 0.6764 - val_loss: 0.1164 - val_categorical_accuracy: 0.5744\n",
      "Epoch 958/10000\n",
      "\n",
      "Epoch 00958: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9940 - categorical_accuracy: 0.6740 - val_loss: 0.1163 - val_categorical_accuracy: 0.5775\n",
      "Epoch 959/10000\n",
      "\n",
      "Epoch 00959: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9923 - categorical_accuracy: 0.6800 - val_loss: 0.1163 - val_categorical_accuracy: 0.5755\n",
      "Epoch 960/10000\n",
      "\n",
      "Epoch 00960: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9940 - categorical_accuracy: 0.6755 - val_loss: 0.1163 - val_categorical_accuracy: 0.5791\n",
      "Epoch 961/10000\n",
      "\n",
      "Epoch 00961: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9933 - categorical_accuracy: 0.6758 - val_loss: 0.1165 - val_categorical_accuracy: 0.5700\n",
      "Epoch 962/10000\n",
      "\n",
      "Epoch 00962: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9937 - categorical_accuracy: 0.6772 - val_loss: 0.1163 - val_categorical_accuracy: 0.5733\n",
      "Epoch 963/10000\n",
      "\n",
      "Epoch 00963: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9977 - categorical_accuracy: 0.6669 - val_loss: 0.1164 - val_categorical_accuracy: 0.5749\n",
      "Epoch 964/10000\n",
      "\n",
      "Epoch 00964: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9945 - categorical_accuracy: 0.6755 - val_loss: 0.1167 - val_categorical_accuracy: 0.5627\n",
      "Epoch 965/10000\n",
      "\n",
      "Epoch 00965: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 1.0015 - categorical_accuracy: 0.6568 - val_loss: 0.1164 - val_categorical_accuracy: 0.5722\n",
      "Epoch 966/10000\n",
      "\n",
      "Epoch 00966: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9950 - categorical_accuracy: 0.6723 - val_loss: 0.1163 - val_categorical_accuracy: 0.5746\n",
      "Epoch 967/10000\n",
      "\n",
      "Epoch 00967: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9983 - categorical_accuracy: 0.6667 - val_loss: 0.1163 - val_categorical_accuracy: 0.5740\n",
      "Epoch 968/10000\n",
      "\n",
      "Epoch 00968: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9948 - categorical_accuracy: 0.6725 - val_loss: 0.1165 - val_categorical_accuracy: 0.5722\n",
      "Epoch 969/10000\n",
      "\n",
      "Epoch 00969: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9933 - categorical_accuracy: 0.6781 - val_loss: 0.1162 - val_categorical_accuracy: 0.5773\n",
      "Epoch 970/10000\n",
      "\n",
      "Epoch 00970: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9947 - categorical_accuracy: 0.6742 - val_loss: 0.1166 - val_categorical_accuracy: 0.5695\n",
      "Epoch 971/10000\n",
      "\n",
      "Epoch 00971: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9937 - categorical_accuracy: 0.6773 - val_loss: 0.1166 - val_categorical_accuracy: 0.5680\n",
      "Epoch 972/10000\n",
      "\n",
      "Epoch 00972: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9981 - categorical_accuracy: 0.6662 - val_loss: 0.1165 - val_categorical_accuracy: 0.5744\n",
      "Epoch 973/10000\n",
      "\n",
      "Epoch 00973: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9970 - categorical_accuracy: 0.6676 - val_loss: 0.1164 - val_categorical_accuracy: 0.5726\n",
      "Epoch 974/10000\n",
      "\n",
      "Epoch 00974: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9981 - categorical_accuracy: 0.6638 - val_loss: 0.1166 - val_categorical_accuracy: 0.5735\n",
      "Epoch 975/10000\n",
      "\n",
      "Epoch 00975: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9980 - categorical_accuracy: 0.6666 - val_loss: 0.1164 - val_categorical_accuracy: 0.5733\n",
      "Epoch 976/10000\n",
      "\n",
      "Epoch 00976: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9960 - categorical_accuracy: 0.6700 - val_loss: 0.1162 - val_categorical_accuracy: 0.5737\n",
      "Epoch 977/10000\n",
      "\n",
      "Epoch 00977: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9934 - categorical_accuracy: 0.6773 - val_loss: 0.1165 - val_categorical_accuracy: 0.5742\n",
      "Epoch 978/10000\n",
      "\n",
      "Epoch 00978: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9941 - categorical_accuracy: 0.6768 - val_loss: 0.1163 - val_categorical_accuracy: 0.5731\n",
      "Epoch 979/10000\n",
      "\n",
      "Epoch 00979: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 0.9961 - categorical_accuracy: 0.6685 - val_loss: 0.1165 - val_categorical_accuracy: 0.5704\n",
      "Epoch 980/10000\n",
      "\n",
      "Epoch 00980: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9984 - categorical_accuracy: 0.6660 - val_loss: 0.1164 - val_categorical_accuracy: 0.5700\n",
      "Epoch 981/10000\n",
      "\n",
      "Epoch 00981: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9992 - categorical_accuracy: 0.6609 - val_loss: 0.1164 - val_categorical_accuracy: 0.5740\n",
      "Epoch 982/10000\n",
      "\n",
      "Epoch 00982: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9953 - categorical_accuracy: 0.6718 - val_loss: 0.1165 - val_categorical_accuracy: 0.5717\n",
      "Epoch 983/10000\n",
      "\n",
      "Epoch 00983: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9967 - categorical_accuracy: 0.6717 - val_loss: 0.1165 - val_categorical_accuracy: 0.5711\n",
      "Epoch 984/10000\n",
      "\n",
      "Epoch 00984: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9998 - categorical_accuracy: 0.6590 - val_loss: 0.1165 - val_categorical_accuracy: 0.5735\n",
      "Epoch 985/10000\n",
      "\n",
      "Epoch 00985: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9948 - categorical_accuracy: 0.6745 - val_loss: 0.1165 - val_categorical_accuracy: 0.5757\n",
      "Epoch 986/10000\n",
      "\n",
      "Epoch 00986: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9976 - categorical_accuracy: 0.6700 - val_loss: 0.1165 - val_categorical_accuracy: 0.5704\n",
      "Epoch 987/10000\n",
      "\n",
      "Epoch 00987: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0022 - categorical_accuracy: 0.6524 - val_loss: 0.1163 - val_categorical_accuracy: 0.5735\n",
      "Epoch 988/10000\n",
      "\n",
      "Epoch 00988: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9938 - categorical_accuracy: 0.6758 - val_loss: 0.1168 - val_categorical_accuracy: 0.5653\n",
      "Epoch 989/10000\n",
      "\n",
      "Epoch 00989: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0027 - categorical_accuracy: 0.6572 - val_loss: 0.1164 - val_categorical_accuracy: 0.5691\n",
      "Epoch 990/10000\n",
      "\n",
      "Epoch 00990: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0024 - categorical_accuracy: 0.6529 - val_loss: 0.1163 - val_categorical_accuracy: 0.5737\n",
      "Epoch 991/10000\n",
      "\n",
      "Epoch 00991: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9979 - categorical_accuracy: 0.6653 - val_loss: 0.1172 - val_categorical_accuracy: 0.5620\n",
      "Epoch 992/10000\n",
      "\n",
      "Epoch 00992: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0088 - categorical_accuracy: 0.6470 - val_loss: 0.1163 - val_categorical_accuracy: 0.5737\n",
      "Epoch 993/10000\n",
      "\n",
      "Epoch 00993: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9992 - categorical_accuracy: 0.6601 - val_loss: 0.1165 - val_categorical_accuracy: 0.5662\n",
      "Epoch 994/10000\n",
      "\n",
      "Epoch 00994: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 1.0027 - categorical_accuracy: 0.6539 - val_loss: 0.1163 - val_categorical_accuracy: 0.5695\n",
      "Epoch 995/10000\n",
      "\n",
      "Epoch 00995: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9978 - categorical_accuracy: 0.6679 - val_loss: 0.1165 - val_categorical_accuracy: 0.5722\n",
      "Epoch 996/10000\n",
      "\n",
      "Epoch 00996: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0009 - categorical_accuracy: 0.6587 - val_loss: 0.1165 - val_categorical_accuracy: 0.5713\n",
      "Epoch 997/10000\n",
      "\n",
      "Epoch 00997: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9969 - categorical_accuracy: 0.6661 - val_loss: 0.1164 - val_categorical_accuracy: 0.5764\n",
      "Epoch 998/10000\n",
      "\n",
      "Epoch 00998: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0007 - categorical_accuracy: 0.6594 - val_loss: 0.1164 - val_categorical_accuracy: 0.5735\n",
      "Epoch 999/10000\n",
      "\n",
      "Epoch 00999: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9971 - categorical_accuracy: 0.6692 - val_loss: 0.1161 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1000/10000\n",
      "\n",
      "Epoch 01000: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9972 - categorical_accuracy: 0.6661 - val_loss: 0.1164 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1001/10000\n",
      "\n",
      "Epoch 01001: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9975 - categorical_accuracy: 0.6679 - val_loss: 0.1163 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1002/10000\n",
      "\n",
      "Epoch 01002: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9945 - categorical_accuracy: 0.6756 - val_loss: 0.1163 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1003/10000\n",
      "\n",
      "Epoch 01003: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9969 - categorical_accuracy: 0.6680 - val_loss: 0.1163 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1004/10000\n",
      "\n",
      "Epoch 01004: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9939 - categorical_accuracy: 0.6754 - val_loss: 0.1162 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1005/10000\n",
      "\n",
      "Epoch 01005: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9942 - categorical_accuracy: 0.6747 - val_loss: 0.1163 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1006/10000\n",
      "\n",
      "Epoch 01006: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9948 - categorical_accuracy: 0.6728 - val_loss: 0.1163 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1007/10000\n",
      "\n",
      "Epoch 01007: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9930 - categorical_accuracy: 0.6769 - val_loss: 0.1163 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1008/10000\n",
      "\n",
      "Epoch 01008: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9940 - categorical_accuracy: 0.6764 - val_loss: 0.1164 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1009/10000\n",
      "\n",
      "Epoch 01009: loss did not improve from 0.99214\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9935 - categorical_accuracy: 0.6770 - val_loss: 0.1164 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1010/10000\n",
      "\n",
      "Epoch 01010: loss improved from 0.99214 to 0.99209, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9921 - categorical_accuracy: 0.6792 - val_loss: 0.1163 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1011/10000\n",
      "\n",
      "Epoch 01011: loss improved from 0.99209 to 0.99196, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9920 - categorical_accuracy: 0.6799 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1012/10000\n",
      "\n",
      "Epoch 01012: loss did not improve from 0.99196\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9921 - categorical_accuracy: 0.6796 - val_loss: 0.1164 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1013/10000\n",
      "\n",
      "Epoch 01013: loss did not improve from 0.99196\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9923 - categorical_accuracy: 0.6792 - val_loss: 0.1163 - val_categorical_accuracy: 0.5828\n",
      "Epoch 1014/10000\n",
      "\n",
      "Epoch 01014: loss did not improve from 0.99196\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9939 - categorical_accuracy: 0.6757 - val_loss: 0.1162 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1015/10000\n",
      "\n",
      "Epoch 01015: loss improved from 0.99196 to 0.99166, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9917 - categorical_accuracy: 0.6803 - val_loss: 0.1164 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1016/10000\n",
      "\n",
      "Epoch 01016: loss did not improve from 0.99166\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9926 - categorical_accuracy: 0.6785 - val_loss: 0.1163 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1017/10000\n",
      "\n",
      "Epoch 01017: loss did not improve from 0.99166\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9925 - categorical_accuracy: 0.6776 - val_loss: 0.1163 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1018/10000\n",
      "\n",
      "Epoch 01018: loss improved from 0.99166 to 0.99103, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9910 - categorical_accuracy: 0.6839 - val_loss: 0.1164 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1019/10000\n",
      "\n",
      "Epoch 01019: loss did not improve from 0.99103\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9923 - categorical_accuracy: 0.6791 - val_loss: 0.1165 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1020/10000\n",
      "\n",
      "Epoch 01020: loss did not improve from 0.99103\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9929 - categorical_accuracy: 0.6778 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1021/10000\n",
      "\n",
      "Epoch 01021: loss did not improve from 0.99103\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9925 - categorical_accuracy: 0.6788 - val_loss: 0.1167 - val_categorical_accuracy: 0.5693\n",
      "Epoch 1022/10000\n",
      "\n",
      "Epoch 01022: loss did not improve from 0.99103\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9938 - categorical_accuracy: 0.6758 - val_loss: 0.1163 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1023/10000\n",
      "\n",
      "Epoch 01023: loss did not improve from 0.99103\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9964 - categorical_accuracy: 0.6672 - val_loss: 0.1166 - val_categorical_accuracy: 0.5695\n",
      "Epoch 1024/10000\n",
      "\n",
      "Epoch 01024: loss did not improve from 0.99103\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9936 - categorical_accuracy: 0.6776 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1025/10000\n",
      "\n",
      "Epoch 01025: loss did not improve from 0.99103\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9927 - categorical_accuracy: 0.6758 - val_loss: 0.1163 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1026/10000\n",
      "\n",
      "Epoch 01026: loss improved from 0.99103 to 0.99062, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9906 - categorical_accuracy: 0.6829 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1027/10000\n",
      "\n",
      "Epoch 01027: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9908 - categorical_accuracy: 0.6831 - val_loss: 0.1163 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1028/10000\n",
      "\n",
      "Epoch 01028: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9923 - categorical_accuracy: 0.6766 - val_loss: 0.1167 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1029/10000\n",
      "\n",
      "Epoch 01029: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9934 - categorical_accuracy: 0.6773 - val_loss: 0.1163 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1030/10000\n",
      "\n",
      "Epoch 01030: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9952 - categorical_accuracy: 0.6712 - val_loss: 0.1165 - val_categorical_accuracy: 0.5689\n",
      "Epoch 1031/10000\n",
      "\n",
      "Epoch 01031: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9935 - categorical_accuracy: 0.6757 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1032/10000\n",
      "\n",
      "Epoch 01032: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9924 - categorical_accuracy: 0.6782 - val_loss: 0.1162 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1033/10000\n",
      "\n",
      "Epoch 01033: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9916 - categorical_accuracy: 0.6805 - val_loss: 0.1165 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1034/10000\n",
      "\n",
      "Epoch 01034: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9914 - categorical_accuracy: 0.6823 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1035/10000\n",
      "\n",
      "Epoch 01035: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9936 - categorical_accuracy: 0.6754 - val_loss: 0.1166 - val_categorical_accuracy: 0.5693\n",
      "Epoch 1036/10000\n",
      "\n",
      "Epoch 01036: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9936 - categorical_accuracy: 0.6772 - val_loss: 0.1164 - val_categorical_accuracy: 0.5700\n",
      "Epoch 1037/10000\n",
      "\n",
      "Epoch 01037: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9941 - categorical_accuracy: 0.6746 - val_loss: 0.1164 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1038/10000\n",
      "\n",
      "Epoch 01038: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9916 - categorical_accuracy: 0.6815 - val_loss: 0.1163 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1039/10000\n",
      "\n",
      "Epoch 01039: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9914 - categorical_accuracy: 0.6811 - val_loss: 0.1165 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1040/10000\n",
      "\n",
      "Epoch 01040: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9918 - categorical_accuracy: 0.6796 - val_loss: 0.1165 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1041/10000\n",
      "\n",
      "Epoch 01041: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9922 - categorical_accuracy: 0.6780 - val_loss: 0.1163 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1042/10000\n",
      "\n",
      "Epoch 01042: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9933 - categorical_accuracy: 0.6770 - val_loss: 0.1166 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1043/10000\n",
      "\n",
      "Epoch 01043: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9932 - categorical_accuracy: 0.6764 - val_loss: 0.1164 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1044/10000\n",
      "\n",
      "Epoch 01044: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9940 - categorical_accuracy: 0.6743 - val_loss: 0.1167 - val_categorical_accuracy: 0.5675\n",
      "Epoch 1045/10000\n",
      "\n",
      "Epoch 01045: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9994 - categorical_accuracy: 0.6620 - val_loss: 0.1167 - val_categorical_accuracy: 0.5682\n",
      "Epoch 1046/10000\n",
      "\n",
      "Epoch 01046: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9984 - categorical_accuracy: 0.6650 - val_loss: 0.1162 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1047/10000\n",
      "\n",
      "Epoch 01047: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9922 - categorical_accuracy: 0.6781 - val_loss: 0.1165 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1048/10000\n",
      "\n",
      "Epoch 01048: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9945 - categorical_accuracy: 0.6751 - val_loss: 0.1167 - val_categorical_accuracy: 0.5646\n",
      "Epoch 1049/10000\n",
      "\n",
      "Epoch 01049: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 1.0007 - categorical_accuracy: 0.6584 - val_loss: 0.1165 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1050/10000\n",
      "\n",
      "Epoch 01050: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9966 - categorical_accuracy: 0.6692 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1051/10000\n",
      "\n",
      "Epoch 01051: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9982 - categorical_accuracy: 0.6676 - val_loss: 0.1165 - val_categorical_accuracy: 0.5678\n",
      "Epoch 1052/10000\n",
      "\n",
      "Epoch 01052: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9993 - categorical_accuracy: 0.6599 - val_loss: 0.1165 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1053/10000\n",
      "\n",
      "Epoch 01053: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9973 - categorical_accuracy: 0.6646 - val_loss: 0.1165 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1054/10000\n",
      "\n",
      "Epoch 01054: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9979 - categorical_accuracy: 0.6695 - val_loss: 0.1164 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1055/10000\n",
      "\n",
      "Epoch 01055: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9959 - categorical_accuracy: 0.6691 - val_loss: 0.1166 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1056/10000\n",
      "\n",
      "Epoch 01056: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9943 - categorical_accuracy: 0.6746 - val_loss: 0.1164 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1057/10000\n",
      "\n",
      "Epoch 01057: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9960 - categorical_accuracy: 0.6722 - val_loss: 0.1164 - val_categorical_accuracy: 0.5715\n",
      "Epoch 1058/10000\n",
      "\n",
      "Epoch 01058: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9960 - categorical_accuracy: 0.6683 - val_loss: 0.1165 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1059/10000\n",
      "\n",
      "Epoch 01059: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9941 - categorical_accuracy: 0.6751 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1060/10000\n",
      "\n",
      "Epoch 01060: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9929 - categorical_accuracy: 0.6790 - val_loss: 0.1163 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1061/10000\n",
      "\n",
      "Epoch 01061: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9949 - categorical_accuracy: 0.6720 - val_loss: 0.1165 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1062/10000\n",
      "\n",
      "Epoch 01062: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9935 - categorical_accuracy: 0.6767 - val_loss: 0.1165 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1063/10000\n",
      "\n",
      "Epoch 01063: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 5s 70us/sample - loss: 0.9924 - categorical_accuracy: 0.6801 - val_loss: 0.1163 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1064/10000\n",
      "\n",
      "Epoch 01064: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9928 - categorical_accuracy: 0.6769 - val_loss: 0.1164 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1065/10000\n",
      "\n",
      "Epoch 01065: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9912 - categorical_accuracy: 0.6823 - val_loss: 0.1164 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1066/10000\n",
      "\n",
      "Epoch 01066: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9920 - categorical_accuracy: 0.6803 - val_loss: 0.1163 - val_categorical_accuracy: 0.5704\n",
      "Epoch 1067/10000\n",
      "\n",
      "Epoch 01067: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9916 - categorical_accuracy: 0.6812 - val_loss: 0.1163 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1068/10000\n",
      "\n",
      "Epoch 01068: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9916 - categorical_accuracy: 0.6811 - val_loss: 0.1164 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1069/10000\n",
      "\n",
      "Epoch 01069: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9907 - categorical_accuracy: 0.6831 - val_loss: 0.1164 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1070/10000\n",
      "\n",
      "Epoch 01070: loss did not improve from 0.99062\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9912 - categorical_accuracy: 0.6818 - val_loss: 0.1163 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1071/10000\n",
      "\n",
      "Epoch 01071: loss improved from 0.99062 to 0.99059, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9906 - categorical_accuracy: 0.6834 - val_loss: 0.1163 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1072/10000\n",
      "\n",
      "Epoch 01072: loss did not improve from 0.99059\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9913 - categorical_accuracy: 0.6807 - val_loss: 0.1164 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1073/10000\n",
      "\n",
      "Epoch 01073: loss improved from 0.99059 to 0.99011, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9901 - categorical_accuracy: 0.6835 - val_loss: 0.1165 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1074/10000\n",
      "\n",
      "Epoch 01074: loss improved from 0.99011 to 0.98958, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9896 - categorical_accuracy: 0.6869 - val_loss: 0.1163 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1075/10000\n",
      "\n",
      "Epoch 01075: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9903 - categorical_accuracy: 0.6827 - val_loss: 0.1165 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1076/10000\n",
      "\n",
      "Epoch 01076: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9909 - categorical_accuracy: 0.6822 - val_loss: 0.1162 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1077/10000\n",
      "\n",
      "Epoch 01077: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 0.9913 - categorical_accuracy: 0.6791 - val_loss: 0.1167 - val_categorical_accuracy: 0.5697\n",
      "Epoch 1078/10000\n",
      "\n",
      "Epoch 01078: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9927 - categorical_accuracy: 0.6791 - val_loss: 0.1164 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1079/10000\n",
      "\n",
      "Epoch 01079: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9959 - categorical_accuracy: 0.6689 - val_loss: 0.1167 - val_categorical_accuracy: 0.5702\n",
      "Epoch 1080/10000\n",
      "\n",
      "Epoch 01080: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9933 - categorical_accuracy: 0.6772 - val_loss: 0.1165 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1081/10000\n",
      "\n",
      "Epoch 01081: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9933 - categorical_accuracy: 0.6750 - val_loss: 0.1164 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1082/10000\n",
      "\n",
      "Epoch 01082: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9906 - categorical_accuracy: 0.6830 - val_loss: 0.1164 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1083/10000\n",
      "\n",
      "Epoch 01083: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9904 - categorical_accuracy: 0.6832 - val_loss: 0.1165 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1084/10000\n",
      "\n",
      "Epoch 01084: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9939 - categorical_accuracy: 0.6736 - val_loss: 0.1167 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1085/10000\n",
      "\n",
      "Epoch 01085: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9945 - categorical_accuracy: 0.6735 - val_loss: 0.1164 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1086/10000\n",
      "\n",
      "Epoch 01086: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9964 - categorical_accuracy: 0.6670 - val_loss: 0.1165 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1087/10000\n",
      "\n",
      "Epoch 01087: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9923 - categorical_accuracy: 0.6779 - val_loss: 0.1164 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1088/10000\n",
      "\n",
      "Epoch 01088: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9896 - categorical_accuracy: 0.6851 - val_loss: 0.1163 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1089/10000\n",
      "\n",
      "Epoch 01089: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9933 - categorical_accuracy: 0.6754 - val_loss: 0.1168 - val_categorical_accuracy: 0.5662\n",
      "Epoch 1090/10000\n",
      "\n",
      "Epoch 01090: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 5s 63us/sample - loss: 0.9954 - categorical_accuracy: 0.6710 - val_loss: 0.1163 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1091/10000\n",
      "\n",
      "Epoch 01091: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9947 - categorical_accuracy: 0.6713 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1092/10000\n",
      "\n",
      "Epoch 01092: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9908 - categorical_accuracy: 0.6822 - val_loss: 0.1166 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1093/10000\n",
      "\n",
      "Epoch 01093: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9932 - categorical_accuracy: 0.6767 - val_loss: 0.1165 - val_categorical_accuracy: 0.5680\n",
      "Epoch 1094/10000\n",
      "\n",
      "Epoch 01094: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9964 - categorical_accuracy: 0.6676 - val_loss: 0.1163 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1095/10000\n",
      "\n",
      "Epoch 01095: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9908 - categorical_accuracy: 0.6825 - val_loss: 0.1167 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1096/10000\n",
      "\n",
      "Epoch 01096: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9916 - categorical_accuracy: 0.6817 - val_loss: 0.1166 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1097/10000\n",
      "\n",
      "Epoch 01097: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9965 - categorical_accuracy: 0.6683 - val_loss: 0.1163 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1098/10000\n",
      "\n",
      "Epoch 01098: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9912 - categorical_accuracy: 0.6808 - val_loss: 0.1163 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1099/10000\n",
      "\n",
      "Epoch 01099: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9905 - categorical_accuracy: 0.6826 - val_loss: 0.1164 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1100/10000\n",
      "\n",
      "Epoch 01100: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9924 - categorical_accuracy: 0.6783 - val_loss: 0.1166 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1101/10000\n",
      "\n",
      "Epoch 01101: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9908 - categorical_accuracy: 0.6831 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1102/10000\n",
      "\n",
      "Epoch 01102: loss did not improve from 0.98958\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9897 - categorical_accuracy: 0.6844 - val_loss: 0.1163 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1103/10000\n",
      "\n",
      "Epoch 01103: loss improved from 0.98958 to 0.98950, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9895 - categorical_accuracy: 0.6859 - val_loss: 0.1166 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1104/10000\n",
      "\n",
      "Epoch 01104: loss did not improve from 0.98950\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9910 - categorical_accuracy: 0.6819 - val_loss: 0.1164 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1105/10000\n",
      "\n",
      "Epoch 01105: loss did not improve from 0.98950\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9898 - categorical_accuracy: 0.6841 - val_loss: 0.1163 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1106/10000\n",
      "\n",
      "Epoch 01106: loss improved from 0.98950 to 0.98830, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9883 - categorical_accuracy: 0.6879 - val_loss: 0.1165 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1107/10000\n",
      "\n",
      "Epoch 01107: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9904 - categorical_accuracy: 0.6822 - val_loss: 0.1166 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1108/10000\n",
      "\n",
      "Epoch 01108: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9920 - categorical_accuracy: 0.6794 - val_loss: 0.1165 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1109/10000\n",
      "\n",
      "Epoch 01109: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9938 - categorical_accuracy: 0.6733 - val_loss: 0.1168 - val_categorical_accuracy: 0.5664\n",
      "Epoch 1110/10000\n",
      "\n",
      "Epoch 01110: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9971 - categorical_accuracy: 0.6702 - val_loss: 0.1166 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1111/10000\n",
      "\n",
      "Epoch 01111: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 1.0004 - categorical_accuracy: 0.6585 - val_loss: 0.1164 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1112/10000\n",
      "\n",
      "Epoch 01112: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9916 - categorical_accuracy: 0.6811 - val_loss: 0.1166 - val_categorical_accuracy: 0.5695\n",
      "Epoch 1113/10000\n",
      "\n",
      "Epoch 01113: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9915 - categorical_accuracy: 0.6824 - val_loss: 0.1166 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1114/10000\n",
      "\n",
      "Epoch 01114: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9991 - categorical_accuracy: 0.6621 - val_loss: 0.1166 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1115/10000\n",
      "\n",
      "Epoch 01115: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9939 - categorical_accuracy: 0.6763 - val_loss: 0.1164 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1116/10000\n",
      "\n",
      "Epoch 01116: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9920 - categorical_accuracy: 0.6803 - val_loss: 0.1164 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1117/10000\n",
      "\n",
      "Epoch 01117: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9939 - categorical_accuracy: 0.6735 - val_loss: 0.1167 - val_categorical_accuracy: 0.5715\n",
      "Epoch 1118/10000\n",
      "\n",
      "Epoch 01118: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9920 - categorical_accuracy: 0.6809 - val_loss: 0.1165 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1119/10000\n",
      "\n",
      "Epoch 01119: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9927 - categorical_accuracy: 0.6779 - val_loss: 0.1163 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1120/10000\n",
      "\n",
      "Epoch 01120: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9919 - categorical_accuracy: 0.6784 - val_loss: 0.1168 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1121/10000\n",
      "\n",
      "Epoch 01121: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9931 - categorical_accuracy: 0.6777 - val_loss: 0.1165 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1122/10000\n",
      "\n",
      "Epoch 01122: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9939 - categorical_accuracy: 0.6749 - val_loss: 0.1163 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1123/10000\n",
      "\n",
      "Epoch 01123: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9897 - categorical_accuracy: 0.6860 - val_loss: 0.1167 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1124/10000\n",
      "\n",
      "Epoch 01124: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9938 - categorical_accuracy: 0.6742 - val_loss: 0.1163 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1125/10000\n",
      "\n",
      "Epoch 01125: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9923 - categorical_accuracy: 0.6769 - val_loss: 0.1166 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1126/10000\n",
      "\n",
      "Epoch 01126: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9945 - categorical_accuracy: 0.6753 - val_loss: 0.1165 - val_categorical_accuracy: 0.5715\n",
      "Epoch 1127/10000\n",
      "\n",
      "Epoch 01127: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9933 - categorical_accuracy: 0.6745 - val_loss: 0.1163 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1128/10000\n",
      "\n",
      "Epoch 01128: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9906 - categorical_accuracy: 0.6818 - val_loss: 0.1168 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1129/10000\n",
      "\n",
      "Epoch 01129: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9964 - categorical_accuracy: 0.6713 - val_loss: 0.1164 - val_categorical_accuracy: 0.5695\n",
      "Epoch 1130/10000\n",
      "\n",
      "Epoch 01130: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9932 - categorical_accuracy: 0.6753 - val_loss: 0.1163 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1131/10000\n",
      "\n",
      "Epoch 01131: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9921 - categorical_accuracy: 0.6799 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1132/10000\n",
      "\n",
      "Epoch 01132: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 67us/sample - loss: 0.9925 - categorical_accuracy: 0.6811 - val_loss: 0.1167 - val_categorical_accuracy: 0.5693\n",
      "Epoch 1133/10000\n",
      "\n",
      "Epoch 01133: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9954 - categorical_accuracy: 0.6709 - val_loss: 0.1165 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1134/10000\n",
      "\n",
      "Epoch 01134: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9902 - categorical_accuracy: 0.6841 - val_loss: 0.1166 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1135/10000\n",
      "\n",
      "Epoch 01135: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9938 - categorical_accuracy: 0.6785 - val_loss: 0.1166 - val_categorical_accuracy: 0.5697\n",
      "Epoch 1136/10000\n",
      "\n",
      "Epoch 01136: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9965 - categorical_accuracy: 0.6667 - val_loss: 0.1165 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1137/10000\n",
      "\n",
      "Epoch 01137: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9898 - categorical_accuracy: 0.6855 - val_loss: 0.1170 - val_categorical_accuracy: 0.5664\n",
      "Epoch 1138/10000\n",
      "\n",
      "Epoch 01138: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9992 - categorical_accuracy: 0.6648 - val_loss: 0.1164 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1139/10000\n",
      "\n",
      "Epoch 01139: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0003 - categorical_accuracy: 0.6561 - val_loss: 0.1164 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1140/10000\n",
      "\n",
      "Epoch 01140: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9932 - categorical_accuracy: 0.6754 - val_loss: 0.1174 - val_categorical_accuracy: 0.5582\n",
      "Epoch 1141/10000\n",
      "\n",
      "Epoch 01141: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 1.0039 - categorical_accuracy: 0.6576 - val_loss: 0.1163 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1142/10000\n",
      "\n",
      "Epoch 01142: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9974 - categorical_accuracy: 0.6649 - val_loss: 0.1164 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1143/10000\n",
      "\n",
      "Epoch 01143: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9987 - categorical_accuracy: 0.6619 - val_loss: 0.1164 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1144/10000\n",
      "\n",
      "Epoch 01144: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9948 - categorical_accuracy: 0.6757 - val_loss: 0.1165 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1145/10000\n",
      "\n",
      "Epoch 01145: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9935 - categorical_accuracy: 0.6787 - val_loss: 0.1167 - val_categorical_accuracy: 0.5680\n",
      "Epoch 1146/10000\n",
      "\n",
      "Epoch 01146: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 0.9975 - categorical_accuracy: 0.6645 - val_loss: 0.1164 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1147/10000\n",
      "\n",
      "Epoch 01147: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9925 - categorical_accuracy: 0.6769 - val_loss: 0.1166 - val_categorical_accuracy: 0.5686\n",
      "Epoch 1148/10000\n",
      "\n",
      "Epoch 01148: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9965 - categorical_accuracy: 0.6727 - val_loss: 0.1165 - val_categorical_accuracy: 0.5715\n",
      "Epoch 1149/10000\n",
      "\n",
      "Epoch 01149: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9953 - categorical_accuracy: 0.6721 - val_loss: 0.1165 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1150/10000\n",
      "\n",
      "Epoch 01150: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9961 - categorical_accuracy: 0.6683 - val_loss: 0.1164 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1151/10000\n",
      "\n",
      "Epoch 01151: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9958 - categorical_accuracy: 0.6732 - val_loss: 0.1163 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1152/10000\n",
      "\n",
      "Epoch 01152: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9905 - categorical_accuracy: 0.6831 - val_loss: 0.1167 - val_categorical_accuracy: 0.5689\n",
      "Epoch 1153/10000\n",
      "\n",
      "Epoch 01153: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 62us/sample - loss: 0.9948 - categorical_accuracy: 0.6726 - val_loss: 0.1164 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1154/10000\n",
      "\n",
      "Epoch 01154: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9918 - categorical_accuracy: 0.6787 - val_loss: 0.1165 - val_categorical_accuracy: 0.5715\n",
      "Epoch 1155/10000\n",
      "\n",
      "Epoch 01155: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9946 - categorical_accuracy: 0.6739 - val_loss: 0.1165 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1156/10000\n",
      "\n",
      "Epoch 01156: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9923 - categorical_accuracy: 0.6769 - val_loss: 0.1164 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1157/10000\n",
      "\n",
      "Epoch 01157: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9902 - categorical_accuracy: 0.6838 - val_loss: 0.1168 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1158/10000\n",
      "\n",
      "Epoch 01158: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9955 - categorical_accuracy: 0.6743 - val_loss: 0.1164 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1159/10000\n",
      "\n",
      "Epoch 01159: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9936 - categorical_accuracy: 0.6746 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1160/10000\n",
      "\n",
      "Epoch 01160: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 68us/sample - loss: 0.9927 - categorical_accuracy: 0.6763 - val_loss: 0.1165 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1161/10000\n",
      "\n",
      "Epoch 01161: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9915 - categorical_accuracy: 0.6816 - val_loss: 0.1163 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1162/10000\n",
      "\n",
      "Epoch 01162: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9912 - categorical_accuracy: 0.6813 - val_loss: 0.1166 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1163/10000\n",
      "\n",
      "Epoch 01163: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9944 - categorical_accuracy: 0.6713 - val_loss: 0.1163 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1164/10000\n",
      "\n",
      "Epoch 01164: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9895 - categorical_accuracy: 0.6849 - val_loss: 0.1166 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1165/10000\n",
      "\n",
      "Epoch 01165: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9914 - categorical_accuracy: 0.6837 - val_loss: 0.1165 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1166/10000\n",
      "\n",
      "Epoch 01166: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9910 - categorical_accuracy: 0.6811 - val_loss: 0.1163 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1167/10000\n",
      "\n",
      "Epoch 01167: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9905 - categorical_accuracy: 0.6823 - val_loss: 0.1166 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1168/10000\n",
      "\n",
      "Epoch 01168: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9920 - categorical_accuracy: 0.6805 - val_loss: 0.1164 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1169/10000\n",
      "\n",
      "Epoch 01169: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9884 - categorical_accuracy: 0.6864 - val_loss: 0.1165 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1170/10000\n",
      "\n",
      "Epoch 01170: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9902 - categorical_accuracy: 0.6831 - val_loss: 0.1165 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1171/10000\n",
      "\n",
      "Epoch 01171: loss did not improve from 0.98830\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9901 - categorical_accuracy: 0.6844 - val_loss: 0.1164 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1172/10000\n",
      "\n",
      "Epoch 01172: loss improved from 0.98830 to 0.98829, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9883 - categorical_accuracy: 0.6884 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1173/10000\n",
      "\n",
      "Epoch 01173: loss did not improve from 0.98829\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9897 - categorical_accuracy: 0.6840 - val_loss: 0.1164 - val_categorical_accuracy: 0.5800\n",
      "Epoch 1174/10000\n",
      "\n",
      "Epoch 01174: loss improved from 0.98829 to 0.98804, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9880 - categorical_accuracy: 0.6893 - val_loss: 0.1162 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1175/10000\n",
      "\n",
      "Epoch 01175: loss improved from 0.98804 to 0.98770, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9877 - categorical_accuracy: 0.6893 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1176/10000\n",
      "\n",
      "Epoch 01176: loss did not improve from 0.98770\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9880 - categorical_accuracy: 0.6874 - val_loss: 0.1165 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1177/10000\n",
      "\n",
      "Epoch 01177: loss improved from 0.98770 to 0.98724, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9872 - categorical_accuracy: 0.6895 - val_loss: 0.1165 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1178/10000\n",
      "\n",
      "Epoch 01178: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9883 - categorical_accuracy: 0.6892 - val_loss: 0.1164 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1179/10000\n",
      "\n",
      "Epoch 01179: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9877 - categorical_accuracy: 0.6882 - val_loss: 0.1166 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1180/10000\n",
      "\n",
      "Epoch 01180: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9881 - categorical_accuracy: 0.6884 - val_loss: 0.1165 - val_categorical_accuracy: 0.5784\n",
      "Epoch 1181/10000\n",
      "\n",
      "Epoch 01181: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9900 - categorical_accuracy: 0.6845 - val_loss: 0.1166 - val_categorical_accuracy: 0.5691\n",
      "Epoch 1182/10000\n",
      "\n",
      "Epoch 01182: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9902 - categorical_accuracy: 0.6837 - val_loss: 0.1165 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1183/10000\n",
      "\n",
      "Epoch 01183: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9913 - categorical_accuracy: 0.6796 - val_loss: 0.1167 - val_categorical_accuracy: 0.5684\n",
      "Epoch 1184/10000\n",
      "\n",
      "Epoch 01184: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9896 - categorical_accuracy: 0.6867 - val_loss: 0.1166 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1185/10000\n",
      "\n",
      "Epoch 01185: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9910 - categorical_accuracy: 0.6810 - val_loss: 0.1167 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1186/10000\n",
      "\n",
      "Epoch 01186: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9929 - categorical_accuracy: 0.6776 - val_loss: 0.1165 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1187/10000\n",
      "\n",
      "Epoch 01187: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9915 - categorical_accuracy: 0.6802 - val_loss: 0.1165 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1188/10000\n",
      "\n",
      "Epoch 01188: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 5s 66us/sample - loss: 0.9890 - categorical_accuracy: 0.6857 - val_loss: 0.1167 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1189/10000\n",
      "\n",
      "Epoch 01189: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9888 - categorical_accuracy: 0.6875 - val_loss: 0.1164 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1190/10000\n",
      "\n",
      "Epoch 01190: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9914 - categorical_accuracy: 0.6789 - val_loss: 0.1168 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1191/10000\n",
      "\n",
      "Epoch 01191: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9909 - categorical_accuracy: 0.6838 - val_loss: 0.1166 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1192/10000\n",
      "\n",
      "Epoch 01192: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9954 - categorical_accuracy: 0.6711 - val_loss: 0.1164 - val_categorical_accuracy: 0.5697\n",
      "Epoch 1193/10000\n",
      "\n",
      "Epoch 01193: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9927 - categorical_accuracy: 0.6773 - val_loss: 0.1165 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1194/10000\n",
      "\n",
      "Epoch 01194: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9897 - categorical_accuracy: 0.6856 - val_loss: 0.1166 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1195/10000\n",
      "\n",
      "Epoch 01195: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9910 - categorical_accuracy: 0.6815 - val_loss: 0.1166 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1196/10000\n",
      "\n",
      "Epoch 01196: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9906 - categorical_accuracy: 0.6848 - val_loss: 0.1164 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1197/10000\n",
      "\n",
      "Epoch 01197: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9911 - categorical_accuracy: 0.6818 - val_loss: 0.1165 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1198/10000\n",
      "\n",
      "Epoch 01198: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9884 - categorical_accuracy: 0.6883 - val_loss: 0.1167 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1199/10000\n",
      "\n",
      "Epoch 01199: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9882 - categorical_accuracy: 0.6874 - val_loss: 0.1163 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1200/10000\n",
      "\n",
      "Epoch 01200: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9906 - categorical_accuracy: 0.6814 - val_loss: 0.1165 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1201/10000\n",
      "\n",
      "Epoch 01201: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9895 - categorical_accuracy: 0.6859 - val_loss: 0.1165 - val_categorical_accuracy: 0.5784\n",
      "Epoch 1202/10000\n",
      "\n",
      "Epoch 01202: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9897 - categorical_accuracy: 0.6838 - val_loss: 0.1166 - val_categorical_accuracy: 0.5686\n",
      "Epoch 1203/10000\n",
      "\n",
      "Epoch 01203: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 5s 64us/sample - loss: 0.9899 - categorical_accuracy: 0.6846 - val_loss: 0.1164 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1204/10000\n",
      "\n",
      "Epoch 01204: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9897 - categorical_accuracy: 0.6845 - val_loss: 0.1165 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1205/10000\n",
      "\n",
      "Epoch 01205: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9898 - categorical_accuracy: 0.6838 - val_loss: 0.1166 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1206/10000\n",
      "\n",
      "Epoch 01206: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9890 - categorical_accuracy: 0.6869 - val_loss: 0.1166 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1207/10000\n",
      "\n",
      "Epoch 01207: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9879 - categorical_accuracy: 0.6892 - val_loss: 0.1164 - val_categorical_accuracy: 0.5793\n",
      "Epoch 1208/10000\n",
      "\n",
      "Epoch 01208: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9892 - categorical_accuracy: 0.6852 - val_loss: 0.1165 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1209/10000\n",
      "\n",
      "Epoch 01209: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9880 - categorical_accuracy: 0.6897 - val_loss: 0.1165 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1210/10000\n",
      "\n",
      "Epoch 01210: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9881 - categorical_accuracy: 0.6891 - val_loss: 0.1165 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1211/10000\n",
      "\n",
      "Epoch 01211: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9886 - categorical_accuracy: 0.6860 - val_loss: 0.1165 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1212/10000\n",
      "\n",
      "Epoch 01212: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9889 - categorical_accuracy: 0.6878 - val_loss: 0.1166 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1213/10000\n",
      "\n",
      "Epoch 01213: loss did not improve from 0.98724\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9907 - categorical_accuracy: 0.6826 - val_loss: 0.1165 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1214/10000\n",
      "\n",
      "Epoch 01214: loss improved from 0.98724 to 0.98672, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9867 - categorical_accuracy: 0.6924 - val_loss: 0.1165 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1215/10000\n",
      "\n",
      "Epoch 01215: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9885 - categorical_accuracy: 0.6874 - val_loss: 0.1166 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1216/10000\n",
      "\n",
      "Epoch 01216: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9903 - categorical_accuracy: 0.6823 - val_loss: 0.1166 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1217/10000\n",
      "\n",
      "Epoch 01217: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9876 - categorical_accuracy: 0.6902 - val_loss: 0.1165 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1218/10000\n",
      "\n",
      "Epoch 01218: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9885 - categorical_accuracy: 0.6865 - val_loss: 0.1165 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1219/10000\n",
      "\n",
      "Epoch 01219: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9875 - categorical_accuracy: 0.6885 - val_loss: 0.1166 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1220/10000\n",
      "\n",
      "Epoch 01220: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9878 - categorical_accuracy: 0.6889 - val_loss: 0.1167 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1221/10000\n",
      "\n",
      "Epoch 01221: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9896 - categorical_accuracy: 0.6859 - val_loss: 0.1164 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1222/10000\n",
      "\n",
      "Epoch 01222: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9886 - categorical_accuracy: 0.6861 - val_loss: 0.1167 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1223/10000\n",
      "\n",
      "Epoch 01223: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9892 - categorical_accuracy: 0.6872 - val_loss: 0.1166 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1224/10000\n",
      "\n",
      "Epoch 01224: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9908 - categorical_accuracy: 0.6811 - val_loss: 0.1169 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1225/10000\n",
      "\n",
      "Epoch 01225: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9903 - categorical_accuracy: 0.6835 - val_loss: 0.1164 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1226/10000\n",
      "\n",
      "Epoch 01226: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9925 - categorical_accuracy: 0.6773 - val_loss: 0.1165 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1227/10000\n",
      "\n",
      "Epoch 01227: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9890 - categorical_accuracy: 0.6861 - val_loss: 0.1168 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1228/10000\n",
      "\n",
      "Epoch 01228: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9896 - categorical_accuracy: 0.6859 - val_loss: 0.1164 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1229/10000\n",
      "\n",
      "Epoch 01229: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9907 - categorical_accuracy: 0.6802 - val_loss: 0.1166 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1230/10000\n",
      "\n",
      "Epoch 01230: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9887 - categorical_accuracy: 0.6885 - val_loss: 0.1168 - val_categorical_accuracy: 0.5678\n",
      "Epoch 1231/10000\n",
      "\n",
      "Epoch 01231: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9931 - categorical_accuracy: 0.6758 - val_loss: 0.1166 - val_categorical_accuracy: 0.5700\n",
      "Epoch 1232/10000\n",
      "\n",
      "Epoch 01232: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9907 - categorical_accuracy: 0.6820 - val_loss: 0.1166 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1233/10000\n",
      "\n",
      "Epoch 01233: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9904 - categorical_accuracy: 0.6828 - val_loss: 0.1165 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1234/10000\n",
      "\n",
      "Epoch 01234: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9901 - categorical_accuracy: 0.6837 - val_loss: 0.1166 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1235/10000\n",
      "\n",
      "Epoch 01235: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 0.9896 - categorical_accuracy: 0.6837 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1236/10000\n",
      "\n",
      "Epoch 01236: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9892 - categorical_accuracy: 0.6858 - val_loss: 0.1164 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1237/10000\n",
      "\n",
      "Epoch 01237: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9880 - categorical_accuracy: 0.6894 - val_loss: 0.1166 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1238/10000\n",
      "\n",
      "Epoch 01238: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9888 - categorical_accuracy: 0.6872 - val_loss: 0.1164 - val_categorical_accuracy: 0.5797\n",
      "Epoch 1239/10000\n",
      "\n",
      "Epoch 01239: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9890 - categorical_accuracy: 0.6866 - val_loss: 0.1166 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1240/10000\n",
      "\n",
      "Epoch 01240: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9897 - categorical_accuracy: 0.6854 - val_loss: 0.1166 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1241/10000\n",
      "\n",
      "Epoch 01241: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9909 - categorical_accuracy: 0.6818 - val_loss: 0.1165 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1242/10000\n",
      "\n",
      "Epoch 01242: loss did not improve from 0.98672\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9887 - categorical_accuracy: 0.6874 - val_loss: 0.1165 - val_categorical_accuracy: 0.5795\n",
      "Epoch 1243/10000\n",
      "\n",
      "Epoch 01243: loss improved from 0.98672 to 0.98670, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9867 - categorical_accuracy: 0.6921 - val_loss: 0.1166 - val_categorical_accuracy: 0.5786\n",
      "Epoch 1244/10000\n",
      "\n",
      "Epoch 01244: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9871 - categorical_accuracy: 0.6903 - val_loss: 0.1165 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1245/10000\n",
      "\n",
      "Epoch 01245: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9881 - categorical_accuracy: 0.6889 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1246/10000\n",
      "\n",
      "Epoch 01246: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9887 - categorical_accuracy: 0.6871 - val_loss: 0.1165 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1247/10000\n",
      "\n",
      "Epoch 01247: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9875 - categorical_accuracy: 0.6913 - val_loss: 0.1167 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1248/10000\n",
      "\n",
      "Epoch 01248: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9876 - categorical_accuracy: 0.6908 - val_loss: 0.1165 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1249/10000\n",
      "\n",
      "Epoch 01249: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9901 - categorical_accuracy: 0.6810 - val_loss: 0.1167 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1250/10000\n",
      "\n",
      "Epoch 01250: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9888 - categorical_accuracy: 0.6892 - val_loss: 0.1164 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1251/10000\n",
      "\n",
      "Epoch 01251: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9909 - categorical_accuracy: 0.6814 - val_loss: 0.1165 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1252/10000\n",
      "\n",
      "Epoch 01252: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9871 - categorical_accuracy: 0.6897 - val_loss: 0.1165 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1253/10000\n",
      "\n",
      "Epoch 01253: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9874 - categorical_accuracy: 0.6894 - val_loss: 0.1164 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1254/10000\n",
      "\n",
      "Epoch 01254: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9883 - categorical_accuracy: 0.6869 - val_loss: 0.1169 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1255/10000\n",
      "\n",
      "Epoch 01255: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9906 - categorical_accuracy: 0.6842 - val_loss: 0.1167 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1256/10000\n",
      "\n",
      "Epoch 01256: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9941 - categorical_accuracy: 0.6726 - val_loss: 0.1165 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1257/10000\n",
      "\n",
      "Epoch 01257: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 0.9873 - categorical_accuracy: 0.6902 - val_loss: 0.1166 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1258/10000\n",
      "\n",
      "Epoch 01258: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9890 - categorical_accuracy: 0.6876 - val_loss: 0.1164 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1259/10000\n",
      "\n",
      "Epoch 01259: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9895 - categorical_accuracy: 0.6846 - val_loss: 0.1168 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1260/10000\n",
      "\n",
      "Epoch 01260: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 5s 65us/sample - loss: 0.9901 - categorical_accuracy: 0.6844 - val_loss: 0.1166 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1261/10000\n",
      "\n",
      "Epoch 01261: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9876 - categorical_accuracy: 0.6895 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1262/10000\n",
      "\n",
      "Epoch 01262: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9870 - categorical_accuracy: 0.6914 - val_loss: 0.1166 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1263/10000\n",
      "\n",
      "Epoch 01263: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9878 - categorical_accuracy: 0.6899 - val_loss: 0.1164 - val_categorical_accuracy: 0.5786\n",
      "Epoch 1264/10000\n",
      "\n",
      "Epoch 01264: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9894 - categorical_accuracy: 0.6836 - val_loss: 0.1167 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1265/10000\n",
      "\n",
      "Epoch 01265: loss did not improve from 0.98670\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9877 - categorical_accuracy: 0.6900 - val_loss: 0.1165 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1266/10000\n",
      "\n",
      "Epoch 01266: loss improved from 0.98670 to 0.98536, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9854 - categorical_accuracy: 0.6940 - val_loss: 0.1164 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1267/10000\n",
      "\n",
      "Epoch 01267: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9867 - categorical_accuracy: 0.6911 - val_loss: 0.1167 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1268/10000\n",
      "\n",
      "Epoch 01268: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9859 - categorical_accuracy: 0.6935 - val_loss: 0.1165 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1269/10000\n",
      "\n",
      "Epoch 01269: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9882 - categorical_accuracy: 0.6874 - val_loss: 0.1167 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1270/10000\n",
      "\n",
      "Epoch 01270: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9879 - categorical_accuracy: 0.6898 - val_loss: 0.1165 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1271/10000\n",
      "\n",
      "Epoch 01271: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9882 - categorical_accuracy: 0.6861 - val_loss: 0.1166 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1272/10000\n",
      "\n",
      "Epoch 01272: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9865 - categorical_accuracy: 0.6923 - val_loss: 0.1167 - val_categorical_accuracy: 0.5700\n",
      "Epoch 1273/10000\n",
      "\n",
      "Epoch 01273: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9882 - categorical_accuracy: 0.6888 - val_loss: 0.1166 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1274/10000\n",
      "\n",
      "Epoch 01274: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9906 - categorical_accuracy: 0.6807 - val_loss: 0.1167 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1275/10000\n",
      "\n",
      "Epoch 01275: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9892 - categorical_accuracy: 0.6876 - val_loss: 0.1165 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1276/10000\n",
      "\n",
      "Epoch 01276: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9884 - categorical_accuracy: 0.6872 - val_loss: 0.1167 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1277/10000\n",
      "\n",
      "Epoch 01277: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9888 - categorical_accuracy: 0.6865 - val_loss: 0.1168 - val_categorical_accuracy: 0.5711\n",
      "Epoch 1278/10000\n",
      "\n",
      "Epoch 01278: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9945 - categorical_accuracy: 0.6723 - val_loss: 0.1168 - val_categorical_accuracy: 0.5686\n",
      "Epoch 1279/10000\n",
      "\n",
      "Epoch 01279: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9971 - categorical_accuracy: 0.6660 - val_loss: 0.1164 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1280/10000\n",
      "\n",
      "Epoch 01280: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9877 - categorical_accuracy: 0.6890 - val_loss: 0.1166 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1281/10000\n",
      "\n",
      "Epoch 01281: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9897 - categorical_accuracy: 0.6837 - val_loss: 0.1169 - val_categorical_accuracy: 0.5682\n",
      "Epoch 1282/10000\n",
      "\n",
      "Epoch 01282: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9958 - categorical_accuracy: 0.6679 - val_loss: 0.1165 - val_categorical_accuracy: 0.5793\n",
      "Epoch 1283/10000\n",
      "\n",
      "Epoch 01283: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9894 - categorical_accuracy: 0.6858 - val_loss: 0.1164 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1284/10000\n",
      "\n",
      "Epoch 01284: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9918 - categorical_accuracy: 0.6800 - val_loss: 0.1166 - val_categorical_accuracy: 0.5711\n",
      "Epoch 1285/10000\n",
      "\n",
      "Epoch 01285: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9895 - categorical_accuracy: 0.6836 - val_loss: 0.1166 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1286/10000\n",
      "\n",
      "Epoch 01286: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9891 - categorical_accuracy: 0.6869 - val_loss: 0.1165 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1287/10000\n",
      "\n",
      "Epoch 01287: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9927 - categorical_accuracy: 0.6796 - val_loss: 0.1164 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1288/10000\n",
      "\n",
      "Epoch 01288: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9902 - categorical_accuracy: 0.6814 - val_loss: 0.1167 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1289/10000\n",
      "\n",
      "Epoch 01289: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9875 - categorical_accuracy: 0.6901 - val_loss: 0.1167 - val_categorical_accuracy: 0.5780\n",
      "Epoch 1290/10000\n",
      "\n",
      "Epoch 01290: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9894 - categorical_accuracy: 0.6875 - val_loss: 0.1165 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1291/10000\n",
      "\n",
      "Epoch 01291: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9897 - categorical_accuracy: 0.6814 - val_loss: 0.1163 - val_categorical_accuracy: 0.5786\n",
      "Epoch 1292/10000\n",
      "\n",
      "Epoch 01292: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9868 - categorical_accuracy: 0.6913 - val_loss: 0.1168 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1293/10000\n",
      "\n",
      "Epoch 01293: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9908 - categorical_accuracy: 0.6840 - val_loss: 0.1167 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1294/10000\n",
      "\n",
      "Epoch 01294: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9913 - categorical_accuracy: 0.6792 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1295/10000\n",
      "\n",
      "Epoch 01295: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9887 - categorical_accuracy: 0.6859 - val_loss: 0.1165 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1296/10000\n",
      "\n",
      "Epoch 01296: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9895 - categorical_accuracy: 0.6861 - val_loss: 0.1167 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1297/10000\n",
      "\n",
      "Epoch 01297: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9874 - categorical_accuracy: 0.6901 - val_loss: 0.1165 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1298/10000\n",
      "\n",
      "Epoch 01298: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9875 - categorical_accuracy: 0.6899 - val_loss: 0.1166 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1299/10000\n",
      "\n",
      "Epoch 01299: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9901 - categorical_accuracy: 0.6843 - val_loss: 0.1167 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1300/10000\n",
      "\n",
      "Epoch 01300: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9877 - categorical_accuracy: 0.6899 - val_loss: 0.1165 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1301/10000\n",
      "\n",
      "Epoch 01301: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9866 - categorical_accuracy: 0.6902 - val_loss: 0.1164 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1302/10000\n",
      "\n",
      "Epoch 01302: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9867 - categorical_accuracy: 0.6916 - val_loss: 0.1166 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1303/10000\n",
      "\n",
      "Epoch 01303: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9860 - categorical_accuracy: 0.6939 - val_loss: 0.1166 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1304/10000\n",
      "\n",
      "Epoch 01304: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9871 - categorical_accuracy: 0.6893 - val_loss: 0.1166 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1305/10000\n",
      "\n",
      "Epoch 01305: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9869 - categorical_accuracy: 0.6925 - val_loss: 0.1165 - val_categorical_accuracy: 0.5704\n",
      "Epoch 1306/10000\n",
      "\n",
      "Epoch 01306: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9896 - categorical_accuracy: 0.6835 - val_loss: 0.1166 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1307/10000\n",
      "\n",
      "Epoch 01307: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9871 - categorical_accuracy: 0.6913 - val_loss: 0.1165 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1308/10000\n",
      "\n",
      "Epoch 01308: loss did not improve from 0.98536\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9866 - categorical_accuracy: 0.6911 - val_loss: 0.1165 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1309/10000\n",
      "\n",
      "Epoch 01309: loss improved from 0.98536 to 0.98502, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9850 - categorical_accuracy: 0.6948 - val_loss: 0.1164 - val_categorical_accuracy: 0.5797\n",
      "Epoch 1310/10000\n",
      "\n",
      "Epoch 01310: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9854 - categorical_accuracy: 0.6950 - val_loss: 0.1167 - val_categorical_accuracy: 0.5711\n",
      "Epoch 1311/10000\n",
      "\n",
      "Epoch 01311: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9864 - categorical_accuracy: 0.6916 - val_loss: 0.1167 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1312/10000\n",
      "\n",
      "Epoch 01312: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9868 - categorical_accuracy: 0.6923 - val_loss: 0.1165 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1313/10000\n",
      "\n",
      "Epoch 01313: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9885 - categorical_accuracy: 0.6870 - val_loss: 0.1165 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1314/10000\n",
      "\n",
      "Epoch 01314: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9857 - categorical_accuracy: 0.6932 - val_loss: 0.1167 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1315/10000\n",
      "\n",
      "Epoch 01315: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9856 - categorical_accuracy: 0.6938 - val_loss: 0.1166 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1316/10000\n",
      "\n",
      "Epoch 01316: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9879 - categorical_accuracy: 0.6881 - val_loss: 0.1166 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1317/10000\n",
      "\n",
      "Epoch 01317: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9861 - categorical_accuracy: 0.6944 - val_loss: 0.1165 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1318/10000\n",
      "\n",
      "Epoch 01318: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9881 - categorical_accuracy: 0.6870 - val_loss: 0.1168 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1319/10000\n",
      "\n",
      "Epoch 01319: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9876 - categorical_accuracy: 0.6904 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1320/10000\n",
      "\n",
      "Epoch 01320: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9866 - categorical_accuracy: 0.6911 - val_loss: 0.1166 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1321/10000\n",
      "\n",
      "Epoch 01321: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9867 - categorical_accuracy: 0.6905 - val_loss: 0.1166 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1322/10000\n",
      "\n",
      "Epoch 01322: loss did not improve from 0.98502\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9866 - categorical_accuracy: 0.6932 - val_loss: 0.1166 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1323/10000\n",
      "\n",
      "Epoch 01323: loss improved from 0.98502 to 0.98409, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9841 - categorical_accuracy: 0.6966 - val_loss: 0.1167 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1324/10000\n",
      "\n",
      "Epoch 01324: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9856 - categorical_accuracy: 0.6938 - val_loss: 0.1165 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1325/10000\n",
      "\n",
      "Epoch 01325: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9883 - categorical_accuracy: 0.6871 - val_loss: 0.1167 - val_categorical_accuracy: 0.5693\n",
      "Epoch 1326/10000\n",
      "\n",
      "Epoch 01326: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9873 - categorical_accuracy: 0.6908 - val_loss: 0.1166 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1327/10000\n",
      "\n",
      "Epoch 01327: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9914 - categorical_accuracy: 0.6794 - val_loss: 0.1168 - val_categorical_accuracy: 0.5678\n",
      "Epoch 1328/10000\n",
      "\n",
      "Epoch 01328: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9906 - categorical_accuracy: 0.6827 - val_loss: 0.1166 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1329/10000\n",
      "\n",
      "Epoch 01329: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9919 - categorical_accuracy: 0.6784 - val_loss: 0.1167 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1330/10000\n",
      "\n",
      "Epoch 01330: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9924 - categorical_accuracy: 0.6772 - val_loss: 0.1166 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1331/10000\n",
      "\n",
      "Epoch 01331: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9872 - categorical_accuracy: 0.6908 - val_loss: 0.1164 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1332/10000\n",
      "\n",
      "Epoch 01332: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9871 - categorical_accuracy: 0.6893 - val_loss: 0.1170 - val_categorical_accuracy: 0.5680\n",
      "Epoch 1333/10000\n",
      "\n",
      "Epoch 01333: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9904 - categorical_accuracy: 0.6840 - val_loss: 0.1164 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1334/10000\n",
      "\n",
      "Epoch 01334: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9913 - categorical_accuracy: 0.6800 - val_loss: 0.1165 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1335/10000\n",
      "\n",
      "Epoch 01335: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9880 - categorical_accuracy: 0.6881 - val_loss: 0.1167 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1336/10000\n",
      "\n",
      "Epoch 01336: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9871 - categorical_accuracy: 0.6921 - val_loss: 0.1165 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1337/10000\n",
      "\n",
      "Epoch 01337: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9901 - categorical_accuracy: 0.6821 - val_loss: 0.1166 - val_categorical_accuracy: 0.5697\n",
      "Epoch 1338/10000\n",
      "\n",
      "Epoch 01338: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9903 - categorical_accuracy: 0.6851 - val_loss: 0.1165 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1339/10000\n",
      "\n",
      "Epoch 01339: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9907 - categorical_accuracy: 0.6816 - val_loss: 0.1166 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1340/10000\n",
      "\n",
      "Epoch 01340: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9867 - categorical_accuracy: 0.6910 - val_loss: 0.1166 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1341/10000\n",
      "\n",
      "Epoch 01341: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9877 - categorical_accuracy: 0.6901 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1342/10000\n",
      "\n",
      "Epoch 01342: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9906 - categorical_accuracy: 0.6809 - val_loss: 0.1167 - val_categorical_accuracy: 0.5704\n",
      "Epoch 1343/10000\n",
      "\n",
      "Epoch 01343: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9859 - categorical_accuracy: 0.6944 - val_loss: 0.1165 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1344/10000\n",
      "\n",
      "Epoch 01344: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9883 - categorical_accuracy: 0.6883 - val_loss: 0.1166 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1345/10000\n",
      "\n",
      "Epoch 01345: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9898 - categorical_accuracy: 0.6811 - val_loss: 0.1167 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1346/10000\n",
      "\n",
      "Epoch 01346: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9862 - categorical_accuracy: 0.6929 - val_loss: 0.1166 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1347/10000\n",
      "\n",
      "Epoch 01347: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9892 - categorical_accuracy: 0.6868 - val_loss: 0.1165 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1348/10000\n",
      "\n",
      "Epoch 01348: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9879 - categorical_accuracy: 0.6872 - val_loss: 0.1169 - val_categorical_accuracy: 0.5726\n",
      "Epoch 1349/10000\n",
      "\n",
      "Epoch 01349: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9910 - categorical_accuracy: 0.6814 - val_loss: 0.1166 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1350/10000\n",
      "\n",
      "Epoch 01350: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9894 - categorical_accuracy: 0.6846 - val_loss: 0.1166 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1351/10000\n",
      "\n",
      "Epoch 01351: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9875 - categorical_accuracy: 0.6897 - val_loss: 0.1167 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1352/10000\n",
      "\n",
      "Epoch 01352: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9902 - categorical_accuracy: 0.6832 - val_loss: 0.1164 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1353/10000\n",
      "\n",
      "Epoch 01353: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9864 - categorical_accuracy: 0.6919 - val_loss: 0.1168 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1354/10000\n",
      "\n",
      "Epoch 01354: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9892 - categorical_accuracy: 0.6870 - val_loss: 0.1165 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1355/10000\n",
      "\n",
      "Epoch 01355: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9855 - categorical_accuracy: 0.6942 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1356/10000\n",
      "\n",
      "Epoch 01356: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9871 - categorical_accuracy: 0.6900 - val_loss: 0.1166 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1357/10000\n",
      "\n",
      "Epoch 01357: loss did not improve from 0.98409\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9868 - categorical_accuracy: 0.6920 - val_loss: 0.1165 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1358/10000\n",
      "\n",
      "Epoch 01358: loss improved from 0.98409 to 0.98407, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9841 - categorical_accuracy: 0.6980 - val_loss: 0.1167 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1359/10000\n",
      "\n",
      "Epoch 01359: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9862 - categorical_accuracy: 0.6912 - val_loss: 0.1164 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1360/10000\n",
      "\n",
      "Epoch 01360: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9856 - categorical_accuracy: 0.6930 - val_loss: 0.1166 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1361/10000\n",
      "\n",
      "Epoch 01361: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9855 - categorical_accuracy: 0.6960 - val_loss: 0.1166 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1362/10000\n",
      "\n",
      "Epoch 01362: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9870 - categorical_accuracy: 0.6904 - val_loss: 0.1165 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1363/10000\n",
      "\n",
      "Epoch 01363: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9858 - categorical_accuracy: 0.6936 - val_loss: 0.1165 - val_categorical_accuracy: 0.5797\n",
      "Epoch 1364/10000\n",
      "\n",
      "Epoch 01364: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9873 - categorical_accuracy: 0.6895 - val_loss: 0.1167 - val_categorical_accuracy: 0.5784\n",
      "Epoch 1365/10000\n",
      "\n",
      "Epoch 01365: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9850 - categorical_accuracy: 0.6955 - val_loss: 0.1165 - val_categorical_accuracy: 0.5711\n",
      "Epoch 1366/10000\n",
      "\n",
      "Epoch 01366: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9872 - categorical_accuracy: 0.6886 - val_loss: 0.1167 - val_categorical_accuracy: 0.5686\n",
      "Epoch 1367/10000\n",
      "\n",
      "Epoch 01367: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9860 - categorical_accuracy: 0.6933 - val_loss: 0.1168 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1368/10000\n",
      "\n",
      "Epoch 01368: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9905 - categorical_accuracy: 0.6835 - val_loss: 0.1167 - val_categorical_accuracy: 0.5686\n",
      "Epoch 1369/10000\n",
      "\n",
      "Epoch 01369: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9939 - categorical_accuracy: 0.6729 - val_loss: 0.1163 - val_categorical_accuracy: 0.5780\n",
      "Epoch 1370/10000\n",
      "\n",
      "Epoch 01370: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9881 - categorical_accuracy: 0.6872 - val_loss: 0.1169 - val_categorical_accuracy: 0.5715\n",
      "Epoch 1371/10000\n",
      "\n",
      "Epoch 01371: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9928 - categorical_accuracy: 0.6781 - val_loss: 0.1167 - val_categorical_accuracy: 0.5695\n",
      "Epoch 1372/10000\n",
      "\n",
      "Epoch 01372: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9895 - categorical_accuracy: 0.6847 - val_loss: 0.1164 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1373/10000\n",
      "\n",
      "Epoch 01373: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 0.9905 - categorical_accuracy: 0.6831 - val_loss: 0.1165 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1374/10000\n",
      "\n",
      "Epoch 01374: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9868 - categorical_accuracy: 0.6907 - val_loss: 0.1169 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1375/10000\n",
      "\n",
      "Epoch 01375: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9937 - categorical_accuracy: 0.6750 - val_loss: 0.1164 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1376/10000\n",
      "\n",
      "Epoch 01376: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9885 - categorical_accuracy: 0.6872 - val_loss: 0.1166 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1377/10000\n",
      "\n",
      "Epoch 01377: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9908 - categorical_accuracy: 0.6819 - val_loss: 0.1167 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1378/10000\n",
      "\n",
      "Epoch 01378: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9904 - categorical_accuracy: 0.6825 - val_loss: 0.1166 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1379/10000\n",
      "\n",
      "Epoch 01379: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9886 - categorical_accuracy: 0.6880 - val_loss: 0.1164 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1380/10000\n",
      "\n",
      "Epoch 01380: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9891 - categorical_accuracy: 0.6859 - val_loss: 0.1165 - val_categorical_accuracy: 0.5695\n",
      "Epoch 1381/10000\n",
      "\n",
      "Epoch 01381: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9885 - categorical_accuracy: 0.6861 - val_loss: 0.1166 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1382/10000\n",
      "\n",
      "Epoch 01382: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9855 - categorical_accuracy: 0.6957 - val_loss: 0.1166 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1383/10000\n",
      "\n",
      "Epoch 01383: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9890 - categorical_accuracy: 0.6876 - val_loss: 0.1165 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1384/10000\n",
      "\n",
      "Epoch 01384: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9884 - categorical_accuracy: 0.6858 - val_loss: 0.1164 - val_categorical_accuracy: 0.5780\n",
      "Epoch 1385/10000\n",
      "\n",
      "Epoch 01385: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9853 - categorical_accuracy: 0.6947 - val_loss: 0.1166 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1386/10000\n",
      "\n",
      "Epoch 01386: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9871 - categorical_accuracy: 0.6912 - val_loss: 0.1167 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1387/10000\n",
      "\n",
      "Epoch 01387: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9886 - categorical_accuracy: 0.6856 - val_loss: 0.1164 - val_categorical_accuracy: 0.5786\n",
      "Epoch 1388/10000\n",
      "\n",
      "Epoch 01388: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 0.9868 - categorical_accuracy: 0.6904 - val_loss: 0.1165 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1389/10000\n",
      "\n",
      "Epoch 01389: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9857 - categorical_accuracy: 0.6947 - val_loss: 0.1167 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1390/10000\n",
      "\n",
      "Epoch 01390: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9879 - categorical_accuracy: 0.6887 - val_loss: 0.1165 - val_categorical_accuracy: 0.5800\n",
      "Epoch 1391/10000\n",
      "\n",
      "Epoch 01391: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9855 - categorical_accuracy: 0.6935 - val_loss: 0.1166 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1392/10000\n",
      "\n",
      "Epoch 01392: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9858 - categorical_accuracy: 0.6941 - val_loss: 0.1166 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1393/10000\n",
      "\n",
      "Epoch 01393: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9901 - categorical_accuracy: 0.6827 - val_loss: 0.1167 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1394/10000\n",
      "\n",
      "Epoch 01394: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9856 - categorical_accuracy: 0.6962 - val_loss: 0.1167 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1395/10000\n",
      "\n",
      "Epoch 01395: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9886 - categorical_accuracy: 0.6858 - val_loss: 0.1165 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1396/10000\n",
      "\n",
      "Epoch 01396: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9883 - categorical_accuracy: 0.6877 - val_loss: 0.1165 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1397/10000\n",
      "\n",
      "Epoch 01397: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9858 - categorical_accuracy: 0.6946 - val_loss: 0.1166 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1398/10000\n",
      "\n",
      "Epoch 01398: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9871 - categorical_accuracy: 0.6906 - val_loss: 0.1166 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1399/10000\n",
      "\n",
      "Epoch 01399: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9890 - categorical_accuracy: 0.6845 - val_loss: 0.1164 - val_categorical_accuracy: 0.5795\n",
      "Epoch 1400/10000\n",
      "\n",
      "Epoch 01400: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9846 - categorical_accuracy: 0.6968 - val_loss: 0.1167 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1401/10000\n",
      "\n",
      "Epoch 01401: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9888 - categorical_accuracy: 0.6874 - val_loss: 0.1165 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1402/10000\n",
      "\n",
      "Epoch 01402: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9868 - categorical_accuracy: 0.6896 - val_loss: 0.1165 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1403/10000\n",
      "\n",
      "Epoch 01403: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9863 - categorical_accuracy: 0.6918 - val_loss: 0.1166 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1404/10000\n",
      "\n",
      "Epoch 01404: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9870 - categorical_accuracy: 0.6897 - val_loss: 0.1165 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1405/10000\n",
      "\n",
      "Epoch 01405: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9856 - categorical_accuracy: 0.6939 - val_loss: 0.1164 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1406/10000\n",
      "\n",
      "Epoch 01406: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9849 - categorical_accuracy: 0.6939 - val_loss: 0.1168 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1407/10000\n",
      "\n",
      "Epoch 01407: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9846 - categorical_accuracy: 0.6961 - val_loss: 0.1166 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1408/10000\n",
      "\n",
      "Epoch 01408: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9857 - categorical_accuracy: 0.6941 - val_loss: 0.1166 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1409/10000\n",
      "\n",
      "Epoch 01409: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9854 - categorical_accuracy: 0.6944 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1410/10000\n",
      "\n",
      "Epoch 01410: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9852 - categorical_accuracy: 0.6937 - val_loss: 0.1169 - val_categorical_accuracy: 0.5700\n",
      "Epoch 1411/10000\n",
      "\n",
      "Epoch 01411: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9865 - categorical_accuracy: 0.6927 - val_loss: 0.1165 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1412/10000\n",
      "\n",
      "Epoch 01412: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9877 - categorical_accuracy: 0.6883 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1413/10000\n",
      "\n",
      "Epoch 01413: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9854 - categorical_accuracy: 0.6952 - val_loss: 0.1166 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1414/10000\n",
      "\n",
      "Epoch 01414: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9873 - categorical_accuracy: 0.6883 - val_loss: 0.1166 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1415/10000\n",
      "\n",
      "Epoch 01415: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9843 - categorical_accuracy: 0.6980 - val_loss: 0.1164 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1416/10000\n",
      "\n",
      "Epoch 01416: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9843 - categorical_accuracy: 0.6969 - val_loss: 0.1166 - val_categorical_accuracy: 0.5740\n",
      "Epoch 1417/10000\n",
      "\n",
      "Epoch 01417: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9877 - categorical_accuracy: 0.6878 - val_loss: 0.1169 - val_categorical_accuracy: 0.5684\n",
      "Epoch 1418/10000\n",
      "\n",
      "Epoch 01418: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9865 - categorical_accuracy: 0.6931 - val_loss: 0.1165 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1419/10000\n",
      "\n",
      "Epoch 01419: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9898 - categorical_accuracy: 0.6833 - val_loss: 0.1167 - val_categorical_accuracy: 0.5673\n",
      "Epoch 1420/10000\n",
      "\n",
      "Epoch 01420: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9922 - categorical_accuracy: 0.6785 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1421/10000\n",
      "\n",
      "Epoch 01421: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9848 - categorical_accuracy: 0.6963 - val_loss: 0.1165 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1422/10000\n",
      "\n",
      "Epoch 01422: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9845 - categorical_accuracy: 0.6962 - val_loss: 0.1168 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1423/10000\n",
      "\n",
      "Epoch 01423: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9883 - categorical_accuracy: 0.6889 - val_loss: 0.1165 - val_categorical_accuracy: 0.5780\n",
      "Epoch 1424/10000\n",
      "\n",
      "Epoch 01424: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9912 - categorical_accuracy: 0.6790 - val_loss: 0.1167 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1425/10000\n",
      "\n",
      "Epoch 01425: loss did not improve from 0.98407\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9873 - categorical_accuracy: 0.6909 - val_loss: 0.1164 - val_categorical_accuracy: 0.5797\n",
      "Epoch 1426/10000\n",
      "\n",
      "Epoch 01426: loss improved from 0.98407 to 0.98380, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9838 - categorical_accuracy: 0.6978 - val_loss: 0.1165 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1427/10000\n",
      "\n",
      "Epoch 01427: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9856 - categorical_accuracy: 0.6931 - val_loss: 0.1169 - val_categorical_accuracy: 0.5691\n",
      "Epoch 1428/10000\n",
      "\n",
      "Epoch 01428: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9877 - categorical_accuracy: 0.6895 - val_loss: 0.1167 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1429/10000\n",
      "\n",
      "Epoch 01429: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9898 - categorical_accuracy: 0.6854 - val_loss: 0.1166 - val_categorical_accuracy: 0.5702\n",
      "Epoch 1430/10000\n",
      "\n",
      "Epoch 01430: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9878 - categorical_accuracy: 0.6869 - val_loss: 0.1166 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1431/10000\n",
      "\n",
      "Epoch 01431: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9868 - categorical_accuracy: 0.6920 - val_loss: 0.1165 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1432/10000\n",
      "\n",
      "Epoch 01432: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9895 - categorical_accuracy: 0.6830 - val_loss: 0.1167 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1433/10000\n",
      "\n",
      "Epoch 01433: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9866 - categorical_accuracy: 0.6901 - val_loss: 0.1166 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1434/10000\n",
      "\n",
      "Epoch 01434: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9854 - categorical_accuracy: 0.6949 - val_loss: 0.1165 - val_categorical_accuracy: 0.5788\n",
      "Epoch 1435/10000\n",
      "\n",
      "Epoch 01435: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 0.9878 - categorical_accuracy: 0.6878 - val_loss: 0.1169 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1436/10000\n",
      "\n",
      "Epoch 01436: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9880 - categorical_accuracy: 0.6904 - val_loss: 0.1164 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1437/10000\n",
      "\n",
      "Epoch 01437: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9874 - categorical_accuracy: 0.6889 - val_loss: 0.1166 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1438/10000\n",
      "\n",
      "Epoch 01438: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9845 - categorical_accuracy: 0.6963 - val_loss: 0.1167 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1439/10000\n",
      "\n",
      "Epoch 01439: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9863 - categorical_accuracy: 0.6921 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1440/10000\n",
      "\n",
      "Epoch 01440: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9886 - categorical_accuracy: 0.6869 - val_loss: 0.1167 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1441/10000\n",
      "\n",
      "Epoch 01441: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9852 - categorical_accuracy: 0.6957 - val_loss: 0.1165 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1442/10000\n",
      "\n",
      "Epoch 01442: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9841 - categorical_accuracy: 0.6975 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1443/10000\n",
      "\n",
      "Epoch 01443: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9886 - categorical_accuracy: 0.6860 - val_loss: 0.1166 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1444/10000\n",
      "\n",
      "Epoch 01444: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9846 - categorical_accuracy: 0.6970 - val_loss: 0.1167 - val_categorical_accuracy: 0.5773\n",
      "Epoch 1445/10000\n",
      "\n",
      "Epoch 01445: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 0.9862 - categorical_accuracy: 0.6929 - val_loss: 0.1167 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1446/10000\n",
      "\n",
      "Epoch 01446: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9902 - categorical_accuracy: 0.6811 - val_loss: 0.1165 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1447/10000\n",
      "\n",
      "Epoch 01447: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9851 - categorical_accuracy: 0.6957 - val_loss: 0.1167 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1448/10000\n",
      "\n",
      "Epoch 01448: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9857 - categorical_accuracy: 0.6932 - val_loss: 0.1166 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1449/10000\n",
      "\n",
      "Epoch 01449: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9875 - categorical_accuracy: 0.6884 - val_loss: 0.1165 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1450/10000\n",
      "\n",
      "Epoch 01450: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9844 - categorical_accuracy: 0.6962 - val_loss: 0.1167 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1451/10000\n",
      "\n",
      "Epoch 01451: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9875 - categorical_accuracy: 0.6881 - val_loss: 0.1165 - val_categorical_accuracy: 0.5802\n",
      "Epoch 1452/10000\n",
      "\n",
      "Epoch 01452: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9850 - categorical_accuracy: 0.6944 - val_loss: 0.1165 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1453/10000\n",
      "\n",
      "Epoch 01453: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9839 - categorical_accuracy: 0.6983 - val_loss: 0.1166 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1454/10000\n",
      "\n",
      "Epoch 01454: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9848 - categorical_accuracy: 0.6955 - val_loss: 0.1165 - val_categorical_accuracy: 0.5786\n",
      "Epoch 1455/10000\n",
      "\n",
      "Epoch 01455: loss did not improve from 0.98380\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9849 - categorical_accuracy: 0.6956 - val_loss: 0.1166 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1456/10000\n",
      "\n",
      "Epoch 01456: loss improved from 0.98380 to 0.98255, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9826 - categorical_accuracy: 0.7002 - val_loss: 0.1169 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1457/10000\n",
      "\n",
      "Epoch 01457: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9845 - categorical_accuracy: 0.6957 - val_loss: 0.1166 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1458/10000\n",
      "\n",
      "Epoch 01458: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9868 - categorical_accuracy: 0.6902 - val_loss: 0.1166 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1459/10000\n",
      "\n",
      "Epoch 01459: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9847 - categorical_accuracy: 0.6965 - val_loss: 0.1167 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1460/10000\n",
      "\n",
      "Epoch 01460: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9868 - categorical_accuracy: 0.6899 - val_loss: 0.1169 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1461/10000\n",
      "\n",
      "Epoch 01461: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9872 - categorical_accuracy: 0.6916 - val_loss: 0.1164 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1462/10000\n",
      "\n",
      "Epoch 01462: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9881 - categorical_accuracy: 0.6861 - val_loss: 0.1168 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1463/10000\n",
      "\n",
      "Epoch 01463: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9863 - categorical_accuracy: 0.6929 - val_loss: 0.1168 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1464/10000\n",
      "\n",
      "Epoch 01464: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9874 - categorical_accuracy: 0.6897 - val_loss: 0.1166 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1465/10000\n",
      "\n",
      "Epoch 01465: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9855 - categorical_accuracy: 0.6936 - val_loss: 0.1167 - val_categorical_accuracy: 0.5720\n",
      "Epoch 1466/10000\n",
      "\n",
      "Epoch 01466: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9877 - categorical_accuracy: 0.6906 - val_loss: 0.1164 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1467/10000\n",
      "\n",
      "Epoch 01467: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9862 - categorical_accuracy: 0.6908 - val_loss: 0.1168 - val_categorical_accuracy: 0.5766\n",
      "Epoch 1468/10000\n",
      "\n",
      "Epoch 01468: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9855 - categorical_accuracy: 0.6947 - val_loss: 0.1167 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1469/10000\n",
      "\n",
      "Epoch 01469: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9853 - categorical_accuracy: 0.6943 - val_loss: 0.1165 - val_categorical_accuracy: 0.5793\n",
      "Epoch 1470/10000\n",
      "\n",
      "Epoch 01470: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9883 - categorical_accuracy: 0.6883 - val_loss: 0.1168 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1471/10000\n",
      "\n",
      "Epoch 01471: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9925 - categorical_accuracy: 0.6779 - val_loss: 0.1167 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1472/10000\n",
      "\n",
      "Epoch 01472: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9860 - categorical_accuracy: 0.6922 - val_loss: 0.1164 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1473/10000\n",
      "\n",
      "Epoch 01473: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9840 - categorical_accuracy: 0.6961 - val_loss: 0.1165 - val_categorical_accuracy: 0.5793\n",
      "Epoch 1474/10000\n",
      "\n",
      "Epoch 01474: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9865 - categorical_accuracy: 0.6925 - val_loss: 0.1166 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1475/10000\n",
      "\n",
      "Epoch 01475: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9900 - categorical_accuracy: 0.6842 - val_loss: 0.1168 - val_categorical_accuracy: 0.5655\n",
      "Epoch 1476/10000\n",
      "\n",
      "Epoch 01476: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 5s 60us/sample - loss: 0.9918 - categorical_accuracy: 0.6792 - val_loss: 0.1167 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1477/10000\n",
      "\n",
      "Epoch 01477: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9867 - categorical_accuracy: 0.6926 - val_loss: 0.1164 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1478/10000\n",
      "\n",
      "Epoch 01478: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9879 - categorical_accuracy: 0.6875 - val_loss: 0.1167 - val_categorical_accuracy: 0.5749\n",
      "Epoch 1479/10000\n",
      "\n",
      "Epoch 01479: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9888 - categorical_accuracy: 0.6876 - val_loss: 0.1165 - val_categorical_accuracy: 0.5795\n",
      "Epoch 1480/10000\n",
      "\n",
      "Epoch 01480: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9853 - categorical_accuracy: 0.6945 - val_loss: 0.1166 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1481/10000\n",
      "\n",
      "Epoch 01481: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 5s 61us/sample - loss: 0.9869 - categorical_accuracy: 0.6885 - val_loss: 0.1169 - val_categorical_accuracy: 0.5702\n",
      "Epoch 1482/10000\n",
      "\n",
      "Epoch 01482: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9869 - categorical_accuracy: 0.6921 - val_loss: 0.1163 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1483/10000\n",
      "\n",
      "Epoch 01483: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9851 - categorical_accuracy: 0.6950 - val_loss: 0.1166 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1484/10000\n",
      "\n",
      "Epoch 01484: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9849 - categorical_accuracy: 0.6942 - val_loss: 0.1168 - val_categorical_accuracy: 0.5671\n",
      "Epoch 1485/10000\n",
      "\n",
      "Epoch 01485: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9860 - categorical_accuracy: 0.6926 - val_loss: 0.1163 - val_categorical_accuracy: 0.5795\n",
      "Epoch 1486/10000\n",
      "\n",
      "Epoch 01486: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9866 - categorical_accuracy: 0.6903 - val_loss: 0.1166 - val_categorical_accuracy: 0.5731\n",
      "Epoch 1487/10000\n",
      "\n",
      "Epoch 01487: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9826 - categorical_accuracy: 0.6998 - val_loss: 0.1168 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1488/10000\n",
      "\n",
      "Epoch 01488: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9833 - categorical_accuracy: 0.6990 - val_loss: 0.1166 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1489/10000\n",
      "\n",
      "Epoch 01489: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9864 - categorical_accuracy: 0.6900 - val_loss: 0.1166 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1490/10000\n",
      "\n",
      "Epoch 01490: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9845 - categorical_accuracy: 0.6969 - val_loss: 0.1165 - val_categorical_accuracy: 0.5753\n",
      "Epoch 1491/10000\n",
      "\n",
      "Epoch 01491: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9836 - categorical_accuracy: 0.6982 - val_loss: 0.1165 - val_categorical_accuracy: 0.5795\n",
      "Epoch 1492/10000\n",
      "\n",
      "Epoch 01492: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9850 - categorical_accuracy: 0.6948 - val_loss: 0.1168 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1493/10000\n",
      "\n",
      "Epoch 01493: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9864 - categorical_accuracy: 0.6930 - val_loss: 0.1164 - val_categorical_accuracy: 0.5780\n",
      "Epoch 1494/10000\n",
      "\n",
      "Epoch 01494: loss did not improve from 0.98255\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9842 - categorical_accuracy: 0.6958 - val_loss: 0.1165 - val_categorical_accuracy: 0.5786\n",
      "Epoch 1495/10000\n",
      "\n",
      "Epoch 01495: loss improved from 0.98255 to 0.98181, saving model to models/FINAL/SAGE_15_UNSORTED.h5\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9818 - categorical_accuracy: 0.7020 - val_loss: 0.1168 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1496/10000\n",
      "\n",
      "Epoch 01496: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9831 - categorical_accuracy: 0.7004 - val_loss: 0.1165 - val_categorical_accuracy: 0.5791\n",
      "Epoch 1497/10000\n",
      "\n",
      "Epoch 01497: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9838 - categorical_accuracy: 0.6970 - val_loss: 0.1167 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1498/10000\n",
      "\n",
      "Epoch 01498: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9828 - categorical_accuracy: 0.7012 - val_loss: 0.1166 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1499/10000\n",
      "\n",
      "Epoch 01499: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9819 - categorical_accuracy: 0.7017 - val_loss: 0.1166 - val_categorical_accuracy: 0.5751\n",
      "Epoch 1500/10000\n",
      "\n",
      "Epoch 01500: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9828 - categorical_accuracy: 0.6989 - val_loss: 0.1167 - val_categorical_accuracy: 0.5735\n",
      "Epoch 1501/10000\n",
      "\n",
      "Epoch 01501: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9829 - categorical_accuracy: 0.7008 - val_loss: 0.1166 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1502/10000\n",
      "\n",
      "Epoch 01502: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9837 - categorical_accuracy: 0.6972 - val_loss: 0.1165 - val_categorical_accuracy: 0.5777\n",
      "Epoch 1503/10000\n",
      "\n",
      "Epoch 01503: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9819 - categorical_accuracy: 0.7021 - val_loss: 0.1167 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1504/10000\n",
      "\n",
      "Epoch 01504: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9825 - categorical_accuracy: 0.7000 - val_loss: 0.1166 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1505/10000\n",
      "\n",
      "Epoch 01505: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9832 - categorical_accuracy: 0.6998 - val_loss: 0.1167 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1506/10000\n",
      "\n",
      "Epoch 01506: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9829 - categorical_accuracy: 0.6993 - val_loss: 0.1168 - val_categorical_accuracy: 0.5744\n",
      "Epoch 1507/10000\n",
      "\n",
      "Epoch 01507: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9833 - categorical_accuracy: 0.6998 - val_loss: 0.1166 - val_categorical_accuracy: 0.5746\n",
      "Epoch 1508/10000\n",
      "\n",
      "Epoch 01508: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9866 - categorical_accuracy: 0.6895 - val_loss: 0.1171 - val_categorical_accuracy: 0.5624\n",
      "Epoch 1509/10000\n",
      "\n",
      "Epoch 01509: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9878 - categorical_accuracy: 0.6893 - val_loss: 0.1166 - val_categorical_accuracy: 0.5715\n",
      "Epoch 1510/10000\n",
      "\n",
      "Epoch 01510: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9905 - categorical_accuracy: 0.6798 - val_loss: 0.1165 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1511/10000\n",
      "\n",
      "Epoch 01511: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9839 - categorical_accuracy: 0.6964 - val_loss: 0.1168 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1512/10000\n",
      "\n",
      "Epoch 01512: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9838 - categorical_accuracy: 0.6989 - val_loss: 0.1166 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1513/10000\n",
      "\n",
      "Epoch 01513: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9882 - categorical_accuracy: 0.6868 - val_loss: 0.1168 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1514/10000\n",
      "\n",
      "Epoch 01514: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9852 - categorical_accuracy: 0.6941 - val_loss: 0.1167 - val_categorical_accuracy: 0.5695\n",
      "Epoch 1515/10000\n",
      "\n",
      "Epoch 01515: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9852 - categorical_accuracy: 0.6948 - val_loss: 0.1165 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1516/10000\n",
      "\n",
      "Epoch 01516: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9868 - categorical_accuracy: 0.6905 - val_loss: 0.1166 - val_categorical_accuracy: 0.5702\n",
      "Epoch 1517/10000\n",
      "\n",
      "Epoch 01517: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9857 - categorical_accuracy: 0.6931 - val_loss: 0.1164 - val_categorical_accuracy: 0.5760\n",
      "Epoch 1518/10000\n",
      "\n",
      "Epoch 01518: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9827 - categorical_accuracy: 0.7007 - val_loss: 0.1168 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1519/10000\n",
      "\n",
      "Epoch 01519: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9859 - categorical_accuracy: 0.6919 - val_loss: 0.1168 - val_categorical_accuracy: 0.5729\n",
      "Epoch 1520/10000\n",
      "\n",
      "Epoch 01520: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9876 - categorical_accuracy: 0.6897 - val_loss: 0.1165 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1521/10000\n",
      "\n",
      "Epoch 01521: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9863 - categorical_accuracy: 0.6909 - val_loss: 0.1165 - val_categorical_accuracy: 0.5733\n",
      "Epoch 1522/10000\n",
      "\n",
      "Epoch 01522: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9836 - categorical_accuracy: 0.6976 - val_loss: 0.1169 - val_categorical_accuracy: 0.5693\n",
      "Epoch 1523/10000\n",
      "\n",
      "Epoch 01523: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9872 - categorical_accuracy: 0.6922 - val_loss: 0.1165 - val_categorical_accuracy: 0.5737\n",
      "Epoch 1524/10000\n",
      "\n",
      "Epoch 01524: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9895 - categorical_accuracy: 0.6814 - val_loss: 0.1164 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1525/10000\n",
      "\n",
      "Epoch 01525: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9844 - categorical_accuracy: 0.6960 - val_loss: 0.1171 - val_categorical_accuracy: 0.5671\n",
      "Epoch 1526/10000\n",
      "\n",
      "Epoch 01526: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9896 - categorical_accuracy: 0.6880 - val_loss: 0.1167 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1527/10000\n",
      "\n",
      "Epoch 01527: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9912 - categorical_accuracy: 0.6779 - val_loss: 0.1164 - val_categorical_accuracy: 0.5764\n",
      "Epoch 1528/10000\n",
      "\n",
      "Epoch 01528: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9853 - categorical_accuracy: 0.6948 - val_loss: 0.1166 - val_categorical_accuracy: 0.5722\n",
      "Epoch 1529/10000\n",
      "\n",
      "Epoch 01529: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 55us/sample - loss: 0.9874 - categorical_accuracy: 0.6929 - val_loss: 0.1170 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1530/10000\n",
      "\n",
      "Epoch 01530: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9952 - categorical_accuracy: 0.6709 - val_loss: 0.1168 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1531/10000\n",
      "\n",
      "Epoch 01531: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9879 - categorical_accuracy: 0.6877 - val_loss: 0.1169 - val_categorical_accuracy: 0.5669\n",
      "Epoch 1532/10000\n",
      "\n",
      "Epoch 01532: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9931 - categorical_accuracy: 0.6780 - val_loss: 0.1165 - val_categorical_accuracy: 0.5711\n",
      "Epoch 1533/10000\n",
      "\n",
      "Epoch 01533: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9912 - categorical_accuracy: 0.6786 - val_loss: 0.1166 - val_categorical_accuracy: 0.5742\n",
      "Epoch 1534/10000\n",
      "\n",
      "Epoch 01534: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9896 - categorical_accuracy: 0.6845 - val_loss: 0.1171 - val_categorical_accuracy: 0.5675\n",
      "Epoch 1535/10000\n",
      "\n",
      "Epoch 01535: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9907 - categorical_accuracy: 0.6861 - val_loss: 0.1167 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1536/10000\n",
      "\n",
      "Epoch 01536: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9921 - categorical_accuracy: 0.6770 - val_loss: 0.1165 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1537/10000\n",
      "\n",
      "Epoch 01537: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9883 - categorical_accuracy: 0.6866 - val_loss: 0.1171 - val_categorical_accuracy: 0.5658\n",
      "Epoch 1538/10000\n",
      "\n",
      "Epoch 01538: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9932 - categorical_accuracy: 0.6785 - val_loss: 0.1167 - val_categorical_accuracy: 0.5711\n",
      "Epoch 1539/10000\n",
      "\n",
      "Epoch 01539: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9891 - categorical_accuracy: 0.6836 - val_loss: 0.1164 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1540/10000\n",
      "\n",
      "Epoch 01540: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9895 - categorical_accuracy: 0.6831 - val_loss: 0.1167 - val_categorical_accuracy: 0.5704\n",
      "Epoch 1541/10000\n",
      "\n",
      "Epoch 01541: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9889 - categorical_accuracy: 0.6871 - val_loss: 0.1168 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1542/10000\n",
      "\n",
      "Epoch 01542: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9849 - categorical_accuracy: 0.6952 - val_loss: 0.1167 - val_categorical_accuracy: 0.5724\n",
      "Epoch 1543/10000\n",
      "\n",
      "Epoch 01543: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9873 - categorical_accuracy: 0.6888 - val_loss: 0.1167 - val_categorical_accuracy: 0.5706\n",
      "Epoch 1544/10000\n",
      "\n",
      "Epoch 01544: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9849 - categorical_accuracy: 0.6972 - val_loss: 0.1166 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1545/10000\n",
      "\n",
      "Epoch 01545: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9863 - categorical_accuracy: 0.6921 - val_loss: 0.1166 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1546/10000\n",
      "\n",
      "Epoch 01546: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9876 - categorical_accuracy: 0.6886 - val_loss: 0.1165 - val_categorical_accuracy: 0.5795\n",
      "Epoch 1547/10000\n",
      "\n",
      "Epoch 01547: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 60us/sample - loss: 0.9834 - categorical_accuracy: 0.6997 - val_loss: 0.1168 - val_categorical_accuracy: 0.5713\n",
      "Epoch 1548/10000\n",
      "\n",
      "Epoch 01548: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9867 - categorical_accuracy: 0.6907 - val_loss: 0.1165 - val_categorical_accuracy: 0.5775\n",
      "Epoch 1549/10000\n",
      "\n",
      "Epoch 01549: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9865 - categorical_accuracy: 0.6906 - val_loss: 0.1165 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1550/10000\n",
      "\n",
      "Epoch 01550: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9847 - categorical_accuracy: 0.6952 - val_loss: 0.1169 - val_categorical_accuracy: 0.5682\n",
      "Epoch 1551/10000\n",
      "\n",
      "Epoch 01551: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9871 - categorical_accuracy: 0.6900 - val_loss: 0.1165 - val_categorical_accuracy: 0.5757\n",
      "Epoch 1552/10000\n",
      "\n",
      "Epoch 01552: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9833 - categorical_accuracy: 0.6997 - val_loss: 0.1164 - val_categorical_accuracy: 0.5771\n",
      "Epoch 1553/10000\n",
      "\n",
      "Epoch 01553: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9837 - categorical_accuracy: 0.6973 - val_loss: 0.1167 - val_categorical_accuracy: 0.5755\n",
      "Epoch 1554/10000\n",
      "\n",
      "Epoch 01554: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9833 - categorical_accuracy: 0.6984 - val_loss: 0.1167 - val_categorical_accuracy: 0.5782\n",
      "Epoch 1555/10000\n",
      "\n",
      "Epoch 01555: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9849 - categorical_accuracy: 0.6967 - val_loss: 0.1166 - val_categorical_accuracy: 0.5717\n",
      "Epoch 1556/10000\n",
      "\n",
      "Epoch 01556: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9857 - categorical_accuracy: 0.6923 - val_loss: 0.1165 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1557/10000\n",
      "\n",
      "Epoch 01557: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9869 - categorical_accuracy: 0.6908 - val_loss: 0.1167 - val_categorical_accuracy: 0.5711\n",
      "Epoch 1558/10000\n",
      "\n",
      "Epoch 01558: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 59us/sample - loss: 0.9860 - categorical_accuracy: 0.6918 - val_loss: 0.1166 - val_categorical_accuracy: 0.5768\n",
      "Epoch 1559/10000\n",
      "\n",
      "Epoch 01559: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9828 - categorical_accuracy: 0.7005 - val_loss: 0.1164 - val_categorical_accuracy: 0.5800\n",
      "Epoch 1560/10000\n",
      "\n",
      "Epoch 01560: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9848 - categorical_accuracy: 0.6945 - val_loss: 0.1169 - val_categorical_accuracy: 0.5682\n",
      "Epoch 1561/10000\n",
      "\n",
      "Epoch 01561: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9871 - categorical_accuracy: 0.6896 - val_loss: 0.1167 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1562/10000\n",
      "\n",
      "Epoch 01562: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9849 - categorical_accuracy: 0.6977 - val_loss: 0.1166 - val_categorical_accuracy: 0.5709\n",
      "Epoch 1563/10000\n",
      "\n",
      "Epoch 01563: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 57us/sample - loss: 0.9850 - categorical_accuracy: 0.6931 - val_loss: 0.1166 - val_categorical_accuracy: 0.5762\n",
      "Epoch 1564/10000\n",
      "\n",
      "Epoch 01564: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 56us/sample - loss: 0.9844 - categorical_accuracy: 0.6958 - val_loss: 0.1165 - val_categorical_accuracy: 0.5780\n",
      "Epoch 1565/10000\n",
      "\n",
      "Epoch 01565: loss did not improve from 0.98181\n",
      "74491/74491 [==============================] - 4s 58us/sample - loss: 0.9854 - categorical_accuracy: 0.6942 - val_loss: 0.1169 - val_categorical_accuracy: 0.5733\n"
     ]
    }
   ],
   "source": [
    "# Define validation data\n",
    "if modelType == 'SAGE':\n",
    "    bs = N\n",
    "else:\n",
    "    bs = N\n",
    "\n",
    "validation_data = ([X, fltr], Y, val_mask)\n",
    "history = model.fit([X, fltr],\n",
    "          Y,\n",
    "          sample_weight=train_mask,\n",
    "          epochs=10000,\n",
    "          batch_size=bs,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 368.925 248.518125\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 248.518125 \n",
       "L 368.925 248.518125 \n",
       "L 368.925 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 361.725 224.64 \n",
       "L 361.725 7.2 \n",
       "L 26.925 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mc1d9eb7ed2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.143182\" xlink:href=\"#mc1d9eb7ed2\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(38.961932 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.076843\" xlink:href=\"#mc1d9eb7ed2\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(93.533093 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.010504\" xlink:href=\"#mc1d9eb7ed2\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(154.466754 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.944165\" xlink:href=\"#mc1d9eb7ed2\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(215.400415 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.877826\" xlink:href=\"#mc1d9eb7ed2\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(276.334076 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.811486\" xlink:href=\"#mc1d9eb7ed2\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1000 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(334.086486 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"me77a37614e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#me77a37614e\" y=\"214.942868\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(13.5625 218.742087)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#me77a37614e\" y=\"172.530751\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(7.2 176.329969)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#me77a37614e\" y=\"130.118633\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(7.2 133.917852)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#me77a37614e\" y=\"87.706516\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(7.2 91.505735)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#me77a37614e\" y=\"45.294399\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(7.2 49.093618)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path clip-path=\"url(#p99ddeec1a0)\" d=\"M 42.143182 17.083636 \n",
       "L 42.44785 101.440186 \n",
       "L 42.752518 106.919321 \n",
       "L 43.361855 185.695416 \n",
       "L 43.666523 184.293122 \n",
       "L 43.971192 184.612705 \n",
       "L 44.27586 195.719951 \n",
       "L 44.885197 205.1226 \n",
       "L 45.189865 205.449896 \n",
       "L 45.494533 203.187148 \n",
       "L 46.10387 206.85322 \n",
       "L 46.713206 208.926187 \n",
       "L 47.017875 209.593406 \n",
       "L 47.322543 210.80494 \n",
       "L 48.236548 211.861477 \n",
       "L 48.845885 212.468876 \n",
       "L 52.806572 212.878509 \n",
       "L 54.634582 212.917582 \n",
       "L 59.509275 212.971721 \n",
       "L 73.828686 213.058188 \n",
       "L 130.192322 213.059702 \n",
       "L 130.49699 212.92959 \n",
       "L 130.801658 213.052029 \n",
       "L 131.106327 212.970314 \n",
       "L 131.715663 213.01751 \n",
       "L 132.325 212.982989 \n",
       "L 132.629668 212.798075 \n",
       "L 132.934337 212.993726 \n",
       "L 133.239005 212.72465 \n",
       "L 134.15301 212.858558 \n",
       "L 158.526474 213.149768 \n",
       "L 165.533845 213.083969 \n",
       "L 166.143182 213.079305 \n",
       "L 166.44785 213.152908 \n",
       "L 167.057187 213.058054 \n",
       "L 167.361855 213.131366 \n",
       "L 167.971192 213.093246 \n",
       "L 168.885197 213.15104 \n",
       "L 169.799201 213.125475 \n",
       "L 172.845885 213.153368 \n",
       "L 174.369226 213.144033 \n",
       "L 185.337285 213.164722 \n",
       "L 186.25129 213.178912 \n",
       "L 188.0793 213.143391 \n",
       "L 189.297973 213.144324 \n",
       "L 192.039988 213.173056 \n",
       "L 196.914681 213.186031 \n",
       "L 218.54613 213.19978 \n",
       "L 222.20215 213.174468 \n",
       "L 223.420823 213.202799 \n",
       "L 229.209521 213.195764 \n",
       "L 232.256204 213.189218 \n",
       "L 270.64441 213.16151 \n",
       "L 270.949079 213.069673 \n",
       "L 271.558415 213.236695 \n",
       "L 273.386425 213.112447 \n",
       "L 274.30043 213.168295 \n",
       "L 277.347113 213.197548 \n",
       "L 279.479791 213.228528 \n",
       "L 296.236548 213.246472 \n",
       "L 298.369226 213.25683 \n",
       "L 300.197236 213.215432 \n",
       "L 303.243919 213.197212 \n",
       "L 303.853256 213.222224 \n",
       "L 305.071929 213.186612 \n",
       "L 306.59527 213.261366 \n",
       "L 310.25129 213.269566 \n",
       "L 320.610012 213.189916 \n",
       "L 320.914681 213.082212 \n",
       "L 321.524017 213.25272 \n",
       "L 321.828686 213.158805 \n",
       "L 322.438022 213.225234 \n",
       "L 325.180037 213.197943 \n",
       "L 326.094042 213.249124 \n",
       "L 327.312715 213.258614 \n",
       "L 330.968735 213.276489 \n",
       "L 346.506818 213.295225 \n",
       "L 346.506818 213.295225 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path clip-path=\"url(#p99ddeec1a0)\" d=\"M 42.143182 202.378114 \n",
       "L 42.44785 203.195387 \n",
       "L 43.057187 211.825572 \n",
       "L 43.361855 211.607156 \n",
       "L 43.666523 211.623773 \n",
       "L 43.971192 212.864865 \n",
       "L 44.580528 213.926415 \n",
       "L 44.885197 213.958797 \n",
       "L 45.189865 213.680265 \n",
       "L 45.799201 214.0733 \n",
       "L 47.017875 214.494025 \n",
       "L 49.150553 214.675476 \n",
       "L 53.720577 214.710354 \n",
       "L 74.74269 214.732562 \n",
       "L 346.506818 214.756364 \n",
       "L 346.506818 214.756364 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 26.925 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 361.725 224.64 \n",
       "L 361.725 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 361.725 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 26.925 7.2 \n",
       "L 361.725 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 283.584375 44.834375 \n",
       "L 354.725 44.834375 \n",
       "Q 356.725 44.834375 356.725 42.834375 \n",
       "L 356.725 14.2 \n",
       "Q 356.725 12.2 354.725 12.2 \n",
       "L 283.584375 12.2 \n",
       "Q 281.584375 12.2 281.584375 14.2 \n",
       "L 281.584375 42.834375 \n",
       "Q 281.584375 44.834375 283.584375 44.834375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_14\">\n",
       "     <path d=\"M 285.584375 20.298437 \n",
       "L 305.584375 20.298437 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\"/>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(313.584375 23.798437)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <path d=\"M 285.584375 34.976562 \n",
       "L 305.584375 34.976562 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\"/>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- val_loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 50.984375 -16.609375 \n",
       "L 50.984375 -23.578125 \n",
       "L -0.984375 -23.578125 \n",
       "L -0.984375 -16.609375 \n",
       "z\n",
       "\" id=\"DejaVuSans-95\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(313.584375 38.476562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n",
       "      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p99ddeec1a0\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history['loss'], label='loss')\n",
    "ax.plot(history.history['val_loss'], label='val_loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 372.103125 248.518125 \n",
       "L 372.103125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 30.103125 224.64 \n",
       "L 364.903125 224.64 \n",
       "L 364.903125 7.2 \n",
       "L 30.103125 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mf627f6511f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mf627f6511f\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(42.140057 239.238438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.254968\" xlink:href=\"#mf627f6511f\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(96.711218 239.238438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.188629\" xlink:href=\"#mf627f6511f\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(157.644879 239.238438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.12229\" xlink:href=\"#mf627f6511f\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(218.57854 239.238438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.055951\" xlink:href=\"#mf627f6511f\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(279.512201 239.238438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.989611\" xlink:href=\"#mf627f6511f\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1000 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(337.264611 239.238438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"ma24ac9a57e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma24ac9a57e\" y=\"188.335843\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.1 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(7.2 192.135062)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma24ac9a57e\" y=\"149.090483\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(7.2 152.889702)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma24ac9a57e\" y=\"109.845124\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.3 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(7.2 113.644343)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma24ac9a57e\" y=\"70.599765\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(7.2 74.398983)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma24ac9a57e\" y=\"31.354405\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(7.2 35.153624)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path clip-path=\"url(#p39242617b3)\" d=\"M 45.321307 214.756364 \n",
       "L 45.625975 84.177074 \n",
       "L 45.930643 177.858356 \n",
       "L 46.235312 142.981758 \n",
       "L 46.53998 85.376373 \n",
       "L 46.844648 81.478636 \n",
       "L 47.149317 170.217615 \n",
       "L 47.453985 197.221347 \n",
       "L 47.758653 93.549066 \n",
       "L 48.063322 72.377442 \n",
       "L 48.36799 149.606961 \n",
       "L 48.672658 161.677397 \n",
       "L 48.977326 51.757121 \n",
       "L 49.281995 41.31155 \n",
       "L 49.586663 144.171396 \n",
       "L 49.891331 167.083945 \n",
       "L 50.196 133.609765 \n",
       "L 50.500668 59.842761 \n",
       "L 50.805336 96.547331 \n",
       "L 51.414673 134.28679 \n",
       "L 51.719341 119.924137 \n",
       "L 52.02401 48.691148 \n",
       "L 52.633346 40.141257 \n",
       "L 52.938014 39.706025 \n",
       "L 53.242683 43.71016 \n",
       "L 53.547351 43.40066 \n",
       "L 53.852019 84.979831 \n",
       "L 54.156688 89.283794 \n",
       "L 54.461356 45.596168 \n",
       "L 54.766024 44.928804 \n",
       "L 55.070693 45.29634 \n",
       "L 55.680029 46.282865 \n",
       "L 55.984697 46.369907 \n",
       "L 56.289366 46.041073 \n",
       "L 56.594034 45.95402 \n",
       "L 57.203371 39.735042 \n",
       "L 57.508039 39.599637 \n",
       "L 57.812707 39.589965 \n",
       "L 58.422044 40.083221 \n",
       "L 58.726712 40.063888 \n",
       "L 59.031381 38.883922 \n",
       "L 59.336049 38.990309 \n",
       "L 59.640717 39.522257 \n",
       "L 59.945385 40.334698 \n",
       "L 60.250054 40.605508 \n",
       "L 60.554722 40.412067 \n",
       "L 60.85939 40.915008 \n",
       "L 61.468727 39.696352 \n",
       "L 61.773395 39.57062 \n",
       "L 62.078064 39.599637 \n",
       "L 62.6874 39.261119 \n",
       "L 63.601405 39.367506 \n",
       "L 63.906073 39.686679 \n",
       "L 64.210742 39.647989 \n",
       "L 64.51541 39.309482 \n",
       "L 64.820078 38.738845 \n",
       "L 65.429415 38.787196 \n",
       "L 65.734083 38.87425 \n",
       "L 66.038752 39.18375 \n",
       "L 66.34342 39.203095 \n",
       "L 66.648088 39.096697 \n",
       "L 66.952756 39.386852 \n",
       "L 67.257425 39.435215 \n",
       "L 67.562093 39.657661 \n",
       "L 67.866761 39.164405 \n",
       "L 68.17143 38.961292 \n",
       "L 68.780766 39.096697 \n",
       "L 69.085435 39.048345 \n",
       "L 69.694771 38.468035 \n",
       "L 70.304108 38.400327 \n",
       "L 71.522781 38.119844 \n",
       "L 72.132118 38.284267 \n",
       "L 72.436786 38.100499 \n",
       "L 73.046123 38.129517 \n",
       "L 73.960127 37.800671 \n",
       "L 74.264796 37.897398 \n",
       "L 74.569464 37.655594 \n",
       "L 74.874132 37.568552 \n",
       "L 75.483469 37.71363 \n",
       "L 75.788137 37.529862 \n",
       "L 76.397474 37.587897 \n",
       "L 77.311479 37.123647 \n",
       "L 77.920815 36.9012 \n",
       "L 78.530152 36.988242 \n",
       "L 79.139489 36.852837 \n",
       "L 80.66283 36.514319 \n",
       "L 80.967498 36.572354 \n",
       "L 83.709513 36.030735 \n",
       "L 84.014182 36.108104 \n",
       "L 84.623518 35.817949 \n",
       "L 84.928186 35.750252 \n",
       "L 86.14686 35.053882 \n",
       "L 86.756196 34.918478 \n",
       "L 87.365533 35.131252 \n",
       "L 87.670201 35.169942 \n",
       "L 87.974869 34.841097 \n",
       "L 88.279538 34.812079 \n",
       "L 88.584206 34.473572 \n",
       "L 88.888874 34.705692 \n",
       "L 89.193543 34.231769 \n",
       "L 89.498211 34.686346 \n",
       "L 89.802879 33.989977 \n",
       "L 90.107548 34.657329 \n",
       "L 90.412216 33.361303 \n",
       "L 90.716884 35.266657 \n",
       "L 91.021553 33.728839 \n",
       "L 91.326221 35.711562 \n",
       "L 91.630889 33.699822 \n",
       "L 91.935557 33.893262 \n",
       "L 92.240226 35.04421 \n",
       "L 92.544894 33.264589 \n",
       "L 93.154231 34.028667 \n",
       "L 93.458899 32.906737 \n",
       "L 93.763567 33.332297 \n",
       "L 94.068236 33.332297 \n",
       "L 94.372904 33.216226 \n",
       "L 94.677572 33.225898 \n",
       "L 94.98224 32.413469 \n",
       "L 95.591577 32.993779 \n",
       "L 95.896245 32.094296 \n",
       "L 96.200914 32.703624 \n",
       "L 96.505582 32.751987 \n",
       "L 96.81025 32.045945 \n",
       "L 97.419587 32.297409 \n",
       "L 97.724255 31.688081 \n",
       "L 98.333592 32.113641 \n",
       "L 98.63826 31.349563 \n",
       "L 99.247597 31.871849 \n",
       "L 99.552265 31.136789 \n",
       "L 99.856933 31.156134 \n",
       "L 100.161602 31.513986 \n",
       "L 100.46627 30.98205 \n",
       "L 100.770938 30.972366 \n",
       "L 101.075607 31.156134 \n",
       "L 101.380275 30.575824 \n",
       "L 101.684943 30.60483 \n",
       "L 101.989611 30.885324 \n",
       "L 102.29428 30.469437 \n",
       "L 102.598948 30.566163 \n",
       "L 102.903616 30.894985 \n",
       "L 103.208285 30.121246 \n",
       "L 103.512953 30.488782 \n",
       "L 104.12229 30.150252 \n",
       "L 104.731626 30.508127 \n",
       "L 105.036295 29.850436 \n",
       "L 105.340963 31.059408 \n",
       "L 105.645631 29.831091 \n",
       "L 105.950299 30.643521 \n",
       "L 106.254968 29.976169 \n",
       "L 106.559636 29.918133 \n",
       "L 106.864304 29.395858 \n",
       "L 107.168973 29.966508 \n",
       "L 107.473641 28.805887 \n",
       "L 107.778309 29.395858 \n",
       "L 108.082978 29.453894 \n",
       "L 108.387646 29.337823 \n",
       "L 108.692314 31.630057 \n",
       "L 108.996982 31.678409 \n",
       "L 109.301651 33.051815 \n",
       "L 109.606319 30.285669 \n",
       "L 109.910987 28.825233 \n",
       "L 110.215656 30.09224 \n",
       "L 110.520324 30.614514 \n",
       "L 110.824992 32.49085 \n",
       "L 111.129661 28.912274 \n",
       "L 111.434329 30.769253 \n",
       "L 111.738997 30.40174 \n",
       "L 112.043666 29.78274 \n",
       "L 112.348334 32.132986 \n",
       "L 112.653002 28.196548 \n",
       "L 112.95767 28.786542 \n",
       "L 113.262339 32.577892 \n",
       "L 113.567007 29.376513 \n",
       "L 113.871675 28.235238 \n",
       "L 114.176344 30.159937 \n",
       "L 114.481012 28.825233 \n",
       "L 114.78568 28.293274 \n",
       "L 115.090349 29.540936 \n",
       "L 115.395017 28.003119 \n",
       "L 115.699685 28.148196 \n",
       "L 116.004354 28.544738 \n",
       "L 116.309022 27.925738 \n",
       "L 116.61369 29.192745 \n",
       "L 116.918358 29.028322 \n",
       "L 117.223027 30.208288 \n",
       "L 117.527695 32.655261 \n",
       "L 117.832363 32.055617 \n",
       "L 118.137032 34.550942 \n",
       "L 118.4417 29.792401 \n",
       "L 118.746368 27.587231 \n",
       "L 119.051037 29.260465 \n",
       "L 119.355705 28.32228 \n",
       "L 119.660373 28.11919 \n",
       "L 119.965041 30.13093 \n",
       "L 120.26971 27.983774 \n",
       "L 120.879046 28.573745 \n",
       "L 121.183715 27.577547 \n",
       "L 121.488383 28.264268 \n",
       "L 121.793051 28.409345 \n",
       "L 122.09772 30.469437 \n",
       "L 122.402388 34.048012 \n",
       "L 122.707056 32.887392 \n",
       "L 123.316393 28.341625 \n",
       "L 123.621061 28.128851 \n",
       "L 123.925729 31.223831 \n",
       "L 124.230398 29.695698 \n",
       "L 124.535066 27.355112 \n",
       "L 124.839734 28.32228 \n",
       "L 125.144403 28.273929 \n",
       "L 125.449071 27.712964 \n",
       "L 125.753739 29.598972 \n",
       "L 126.058408 27.567886 \n",
       "L 126.363076 27.606577 \n",
       "L 126.667744 29.086358 \n",
       "L 126.972412 27.355112 \n",
       "L 127.277081 29.115388 \n",
       "L 127.581749 33.206565 \n",
       "L 127.886417 31.194825 \n",
       "L 128.191086 32.839029 \n",
       "L 128.800422 28.05147 \n",
       "L 129.105091 28.786542 \n",
       "L 129.409759 27.345428 \n",
       "L 129.714427 27.239041 \n",
       "L 130.019096 29.492585 \n",
       "L 130.323764 27.374457 \n",
       "L 130.628432 29.076697 \n",
       "L 130.9331 33.167874 \n",
       "L 131.237769 27.171344 \n",
       "L 131.542437 28.419006 \n",
       "L 131.847105 34.434882 \n",
       "L 132.151774 27.413148 \n",
       "L 132.456442 29.463555 \n",
       "L 132.76111 35.111906 \n",
       "L 133.065779 27.239041 \n",
       "L 133.370447 41.176145 \n",
       "L 133.675115 65.404059 \n",
       "L 133.979783 35.305347 \n",
       "L 134.284452 54.088023 \n",
       "L 134.58912 33.390321 \n",
       "L 134.893788 34.115709 \n",
       "L 135.198457 41.824152 \n",
       "L 135.503125 40.470103 \n",
       "L 136.112462 46.157133 \n",
       "L 136.41713 80.076212 \n",
       "L 136.721798 72.570882 \n",
       "L 137.026467 43.449023 \n",
       "L 137.331135 62.908735 \n",
       "L 137.635803 39.261119 \n",
       "L 137.940471 46.456961 \n",
       "L 138.24514 39.45456 \n",
       "L 138.549808 39.367506 \n",
       "L 138.854476 46.340901 \n",
       "L 139.159145 40.528127 \n",
       "L 139.463813 39.928471 \n",
       "L 139.768481 38.787196 \n",
       "L 140.07315 37.104302 \n",
       "L 140.682486 36.494985 \n",
       "L 140.987154 35.305347 \n",
       "L 141.291823 30.91433 \n",
       "L 141.596491 30.87564 \n",
       "L 141.901159 35.508449 \n",
       "L 142.205828 37.307415 \n",
       "L 142.510496 36.804474 \n",
       "L 142.815164 32.384451 \n",
       "L 143.119833 31.378581 \n",
       "L 143.424501 32.307082 \n",
       "L 143.729169 34.057685 \n",
       "L 144.033838 33.931941 \n",
       "L 144.338506 31.765462 \n",
       "L 144.947842 30.711241 \n",
       "L 145.252511 31.136789 \n",
       "L 145.557179 32.06529 \n",
       "L 145.861847 31.059408 \n",
       "L 146.166516 31.069092 \n",
       "L 146.471184 30.749908 \n",
       "L 146.775852 30.778937 \n",
       "L 147.080521 31.107783 \n",
       "L 147.689857 30.488782 \n",
       "L 147.994525 30.479098 \n",
       "L 148.299194 30.111585 \n",
       "L 148.603862 30.314675 \n",
       "L 149.213199 30.217972 \n",
       "L 149.822535 29.473239 \n",
       "L 150.127204 29.637662 \n",
       "L 150.431872 29.521591 \n",
       "L 151.041209 28.602774 \n",
       "L 151.345877 28.776858 \n",
       "L 151.650545 28.564084 \n",
       "L 151.955213 28.128851 \n",
       "L 152.869218 28.070815 \n",
       "L 153.173887 27.74197 \n",
       "L 153.478555 27.761315 \n",
       "L 153.783223 28.041809 \n",
       "L 154.087892 27.935422 \n",
       "L 154.39256 27.480844 \n",
       "L 154.697228 27.625922 \n",
       "L 155.001896 27.587231 \n",
       "L 155.611233 27.277731 \n",
       "L 156.525238 27.103647 \n",
       "L 156.829906 26.871505 \n",
       "L 157.134575 26.910195 \n",
       "L 157.439243 26.678076 \n",
       "L 158.962584 26.755457 \n",
       "L 159.267253 26.919879 \n",
       "L 159.571921 26.765118 \n",
       "L 160.181258 26.765118 \n",
       "L 160.485926 26.562028 \n",
       "L 160.790594 26.610379 \n",
       "L 162.923272 26.223498 \n",
       "L 163.532609 26.242843 \n",
       "L 164.446614 26.165462 \n",
       "L 165.969955 26.068759 \n",
       "L 167.188629 25.884991 \n",
       "L 167.493297 25.913998 \n",
       "L 167.797965 25.749575 \n",
       "L 168.102634 26.049414 \n",
       "L 168.407302 26.842498 \n",
       "L 168.71197 29.966508 \n",
       "L 169.016639 44.203428 \n",
       "L 169.321307 30.111585 \n",
       "L 169.625975 26.300879 \n",
       "L 169.930643 33.399994 \n",
       "L 170.235312 32.974434 \n",
       "L 170.53998 28.815548 \n",
       "L 170.844648 37.394457 \n",
       "L 171.149317 35.875985 \n",
       "L 171.453985 28.003119 \n",
       "L 171.758653 31.717099 \n",
       "L 172.063322 26.89085 \n",
       "L 172.36799 34.531596 \n",
       "L 172.672658 30.179282 \n",
       "L 172.977326 30.91433 \n",
       "L 173.281995 28.032125 \n",
       "L 173.586663 28.215893 \n",
       "L 173.891331 29.038007 \n",
       "L 174.196 28.448012 \n",
       "L 174.500668 27.190689 \n",
       "L 174.805336 31.75579 \n",
       "L 175.110005 26.58135 \n",
       "L 175.414673 27.693618 \n",
       "L 175.719341 27.674273 \n",
       "L 176.02401 26.204153 \n",
       "L 176.328678 30.836973 \n",
       "L 176.633346 27.132654 \n",
       "L 176.938014 29.395858 \n",
       "L 177.242683 26.591034 \n",
       "L 177.547351 28.622119 \n",
       "L 177.852019 26.852183 \n",
       "L 178.156688 27.732309 \n",
       "L 178.461356 26.639385 \n",
       "L 178.766024 27.761315 \n",
       "L 179.070693 26.184807 \n",
       "L 179.375361 26.774802 \n",
       "L 179.680029 25.904337 \n",
       "L 179.984697 26.368575 \n",
       "L 180.289366 25.904337 \n",
       "L 180.594034 25.943027 \n",
       "L 180.898702 25.83664 \n",
       "L 181.203371 26.07842 \n",
       "L 181.508039 25.943027 \n",
       "L 181.812707 25.962372 \n",
       "L 182.117376 25.643188 \n",
       "L 182.422044 25.720569 \n",
       "L 182.726712 26.368575 \n",
       "L 183.031381 25.546485 \n",
       "L 183.336049 25.614182 \n",
       "L 183.640717 25.79795 \n",
       "L 183.945385 25.652872 \n",
       "L 184.554722 25.643188 \n",
       "L 184.85939 26.030069 \n",
       "L 185.773395 25.681878 \n",
       "L 186.078064 26.10745 \n",
       "L 186.382732 26.184807 \n",
       "L 186.6874 26.600695 \n",
       "L 186.992068 27.722625 \n",
       "L 187.296737 29.386198 \n",
       "L 187.601405 30.09224 \n",
       "L 187.906073 27.47116 \n",
       "L 188.210742 25.83664 \n",
       "L 188.51541 26.64907 \n",
       "L 188.820078 29.086358 \n",
       "L 189.124747 28.186887 \n",
       "L 189.429415 25.643188 \n",
       "L 189.734083 28.099845 \n",
       "L 190.038752 28.244922 \n",
       "L 190.34342 26.784463 \n",
       "L 190.648088 26.165462 \n",
       "L 190.952756 29.71502 \n",
       "L 191.562093 26.629724 \n",
       "L 191.866761 26.910195 \n",
       "L 192.17143 32.084623 \n",
       "L 192.476098 29.386198 \n",
       "L 192.780766 28.0805 \n",
       "L 193.085435 29.657007 \n",
       "L 193.694771 26.194492 \n",
       "L 193.999439 26.736111 \n",
       "L 194.304108 26.136456 \n",
       "L 194.608776 25.80761 \n",
       "L 195.218113 25.923682 \n",
       "L 195.522781 26.030069 \n",
       "L 195.827449 25.372378 \n",
       "L 196.132118 25.478765 \n",
       "L 196.436786 25.80761 \n",
       "L 196.741454 25.420753 \n",
       "L 197.046123 25.314342 \n",
       "L 197.350791 25.382062 \n",
       "L 197.655459 25.169265 \n",
       "L 197.960127 26.126795 \n",
       "L 198.264796 25.333687 \n",
       "L 198.569464 26.049414 \n",
       "L 198.874132 25.324026 \n",
       "L 199.178801 26.088105 \n",
       "L 199.483469 25.430414 \n",
       "L 199.788137 25.594836 \n",
       "L 200.092806 25.430414 \n",
       "L 200.397474 25.981718 \n",
       "L 200.702142 25.826956 \n",
       "L 201.311479 28.496387 \n",
       "L 201.616147 29.086358 \n",
       "L 201.920815 28.167542 \n",
       "L 202.225484 25.49811 \n",
       "L 202.530152 26.484647 \n",
       "L 202.83482 28.428667 \n",
       "L 203.139489 28.805887 \n",
       "L 203.444157 25.353033 \n",
       "L 203.748825 26.03973 \n",
       "L 204.053494 28.39 \n",
       "L 204.358162 29.067013 \n",
       "L 204.66283 25.788265 \n",
       "L 204.967498 31.91054 \n",
       "L 205.272167 33.100166 \n",
       "L 205.576835 29.270126 \n",
       "L 205.881503 27.84838 \n",
       "L 206.186172 35.89533 \n",
       "L 206.49084 25.49811 \n",
       "L 206.795508 28.148196 \n",
       "L 207.100177 25.80761 \n",
       "L 207.404845 26.252527 \n",
       "L 207.709513 28.583429 \n",
       "L 208.014182 29.521591 \n",
       "L 208.31885 26.62004 \n",
       "L 208.623518 30.575824 \n",
       "L 208.928186 26.629724 \n",
       "L 209.537523 27.26807 \n",
       "L 209.842191 25.943027 \n",
       "L 210.14686 25.2273 \n",
       "L 210.451528 27.006921 \n",
       "L 210.756196 25.672217 \n",
       "L 211.060865 25.556146 \n",
       "L 211.365533 27.906393 \n",
       "L 211.670201 26.03973 \n",
       "L 211.974869 25.440074 \n",
       "L 212.279538 27.770999 \n",
       "L 212.584206 25.962372 \n",
       "L 212.888874 25.952688 \n",
       "L 213.193543 25.80761 \n",
       "L 213.498211 25.962372 \n",
       "L 213.802879 25.875307 \n",
       "L 214.107548 25.207955 \n",
       "L 214.412216 25.072562 \n",
       "L 214.716884 26.37826 \n",
       "L 215.021553 25.681878 \n",
       "L 215.326221 25.382062 \n",
       "L 215.630889 24.888794 \n",
       "L 215.935557 25.072562 \n",
       "L 216.240226 25.826956 \n",
       "L 216.849562 25.043532 \n",
       "L 217.458899 24.559948 \n",
       "L 217.763567 25.207955 \n",
       "L 218.068236 25.430414 \n",
       "L 218.372904 26.097766 \n",
       "L 218.677572 26.252527 \n",
       "L 218.98224 26.136456 \n",
       "L 219.286909 25.101568 \n",
       "L 219.591577 24.743716 \n",
       "L 219.896245 25.140258 \n",
       "L 220.200914 25.18861 \n",
       "L 220.81025 26.774802 \n",
       "L 221.114919 26.678076 \n",
       "L 221.419587 26.37826 \n",
       "L 222.028924 24.666335 \n",
       "L 222.333592 25.082223 \n",
       "L 222.63826 26.020385 \n",
       "L 222.942928 26.088105 \n",
       "L 223.247597 25.652872 \n",
       "L 223.552265 24.95649 \n",
       "L 223.856933 24.763062 \n",
       "L 224.161602 24.772722 \n",
       "L 224.46627 24.656674 \n",
       "L 224.770938 25.236985 \n",
       "L 225.075607 26.629724 \n",
       "L 225.380275 27.20035 \n",
       "L 225.684943 29.115388 \n",
       "L 225.989611 28.235238 \n",
       "L 226.598948 25.401407 \n",
       "L 226.903616 25.120913 \n",
       "L 227.208285 25.469104 \n",
       "L 227.512953 26.629724 \n",
       "L 227.817621 24.908139 \n",
       "L 228.12229 24.308484 \n",
       "L 228.731626 26.001039 \n",
       "L 229.036295 25.80761 \n",
       "L 229.645631 24.530942 \n",
       "L 229.950299 24.94683 \n",
       "L 230.254968 25.720569 \n",
       "L 230.559636 26.92954 \n",
       "L 230.864304 27.384118 \n",
       "L 231.168973 26.910195 \n",
       "L 231.473641 25.2273 \n",
       "L 231.778309 24.617984 \n",
       "L 232.082978 24.95649 \n",
       "L 232.387646 25.575491 \n",
       "L 232.692314 27.006921 \n",
       "L 232.996982 26.62004 \n",
       "L 233.301651 26.010724 \n",
       "L 233.606319 24.850103 \n",
       "L 233.910987 24.366519 \n",
       "L 234.215656 24.492252 \n",
       "L 234.824992 26.358914 \n",
       "L 235.129661 26.871505 \n",
       "L 235.434329 26.64907 \n",
       "L 235.738997 26.097766 \n",
       "L 236.043666 25.004842 \n",
       "L 236.348334 24.395526 \n",
       "L 236.653002 24.231103 \n",
       "L 236.95767 25.353033 \n",
       "L 237.262339 25.962372 \n",
       "L 237.871675 29.20243 \n",
       "L 238.78568 24.521258 \n",
       "L 239.090349 24.705026 \n",
       "L 239.395017 25.285336 \n",
       "L 239.699685 26.861844 \n",
       "L 240.004354 26.136456 \n",
       "L 240.61369 24.163406 \n",
       "L 240.918358 24.327829 \n",
       "L 241.832363 26.842498 \n",
       "L 242.137032 26.233182 \n",
       "L 242.4417 24.830758 \n",
       "L 243.051037 24.115055 \n",
       "L 243.355705 24.211758 \n",
       "L 243.660373 24.153722 \n",
       "L 243.965041 24.67602 \n",
       "L 244.26971 25.49811 \n",
       "L 244.574378 27.016582 \n",
       "L 244.879046 29.134733 \n",
       "L 245.488383 30.537134 \n",
       "L 245.793051 24.086025 \n",
       "L 246.402388 28.757513 \n",
       "L 246.707056 27.384118 \n",
       "L 247.011725 25.2273 \n",
       "L 247.316393 27.393802 \n",
       "L 247.621061 26.813492 \n",
       "L 247.925729 24.937145 \n",
       "L 248.230398 28.148196 \n",
       "L 248.535066 27.026266 \n",
       "L 248.839734 26.339569 \n",
       "L 249.144403 25.072562 \n",
       "L 249.449071 27.374457 \n",
       "L 249.753739 25.236985 \n",
       "L 250.058408 26.397605 \n",
       "L 250.363076 24.347174 \n",
       "L 250.667744 25.943027 \n",
       "L 250.972412 24.366519 \n",
       "L 251.277081 25.652872 \n",
       "L 251.581749 24.637329 \n",
       "L 252.191086 28.921935 \n",
       "L 252.495754 27.74197 \n",
       "L 252.800422 24.801752 \n",
       "L 253.105091 24.182751 \n",
       "L 253.409759 25.855962 \n",
       "L 253.714427 25.865646 \n",
       "L 254.019096 24.492252 \n",
       "L 254.323764 30.575824 \n",
       "L 254.628432 26.387921 \n",
       "L 254.9331 26.503992 \n",
       "L 255.237769 28.341625 \n",
       "L 255.542437 30.94336 \n",
       "L 255.847105 25.507794 \n",
       "L 256.151774 26.803808 \n",
       "L 256.456442 26.532998 \n",
       "L 256.76111 23.341292 \n",
       "L 257.065779 26.910195 \n",
       "L 257.675115 23.689483 \n",
       "L 257.979783 26.252527 \n",
       "L 258.284452 25.778604 \n",
       "L 258.58912 25.604497 \n",
       "L 258.893788 25.681878 \n",
       "L 259.198457 25.033871 \n",
       "L 259.503125 24.705026 \n",
       "L 259.807793 23.447679 \n",
       "L 260.41713 25.149919 \n",
       "L 261.026467 23.534745 \n",
       "L 261.331135 23.766864 \n",
       "L 261.940471 24.018329 \n",
       "L 262.24514 23.205899 \n",
       "L 262.549808 23.989322 \n",
       "L 262.854476 23.786209 \n",
       "L 263.159145 22.780327 \n",
       "L 263.463813 22.964095 \n",
       "L 263.768481 23.670138 \n",
       "L 264.07315 23.921602 \n",
       "L 264.377818 23.099512 \n",
       "L 264.987154 26.03973 \n",
       "L 265.291823 26.832838 \n",
       "L 265.596491 26.513653 \n",
       "L 265.901159 26.881189 \n",
       "L 266.205828 24.530942 \n",
       "L 266.510496 23.21556 \n",
       "L 266.815164 23.650793 \n",
       "L 267.119833 25.304681 \n",
       "L 267.424501 23.979638 \n",
       "L 267.729169 23.409012 \n",
       "L 268.033838 22.538547 \n",
       "L 268.338506 23.467025 \n",
       "L 268.947842 24.124716 \n",
       "L 269.252511 24.714687 \n",
       "L 269.557179 24.95649 \n",
       "L 269.861847 24.115055 \n",
       "L 270.166516 24.898455 \n",
       "L 270.471184 25.062878 \n",
       "L 270.775852 23.592757 \n",
       "L 271.080521 23.24459 \n",
       "L 271.385189 23.118857 \n",
       "L 271.689857 22.819018 \n",
       "L 271.994525 22.306404 \n",
       "L 272.299194 22.70297 \n",
       "L 272.603862 22.770666 \n",
       "L 272.90853 23.080167 \n",
       "L 273.213199 23.737834 \n",
       "L 273.822535 29.4829 \n",
       "L 274.127204 37.142992 \n",
       "L 274.431872 28.564084 \n",
       "L 274.73654 23.447679 \n",
       "L 275.345877 27.606577 \n",
       "L 275.650545 25.207955 \n",
       "L 275.955213 24.327829 \n",
       "L 276.259882 33.873917 \n",
       "L 276.56455 29.134733 \n",
       "L 276.869218 26.997237 \n",
       "L 277.173887 37.452492 \n",
       "L 277.478555 26.155801 \n",
       "L 277.783223 28.196548 \n",
       "L 278.087892 25.79795 \n",
       "L 278.39256 30.179282 \n",
       "L 278.697228 28.573745 \n",
       "L 279.001896 32.635927 \n",
       "L 279.306565 24.888794 \n",
       "L 279.611233 32.955089 \n",
       "L 279.915901 23.747519 \n",
       "L 280.22057 27.935422 \n",
       "L 280.525238 27.142314 \n",
       "L 280.829906 23.612102 \n",
       "L 281.134575 28.97031 \n",
       "L 281.439243 22.016249 \n",
       "L 281.743911 25.643188 \n",
       "L 282.04858 24.550287 \n",
       "L 282.353248 24.94683 \n",
       "L 282.657916 23.515399 \n",
       "L 282.962584 23.844245 \n",
       "L 283.267253 22.287059 \n",
       "L 283.876589 24.173067 \n",
       "L 284.181258 23.438018 \n",
       "L 284.485926 23.853906 \n",
       "L 284.790594 23.496054 \n",
       "L 285.095263 23.786209 \n",
       "L 285.399931 21.639052 \n",
       "L 285.704599 23.399328 \n",
       "L 286.009268 22.712631 \n",
       "L 286.313936 23.24459 \n",
       "L 286.618604 23.21556 \n",
       "L 286.923272 20.874974 \n",
       "L 287.227941 20.942694 \n",
       "L 287.532609 22.470827 \n",
       "L 287.837277 22.519202 \n",
       "L 288.141946 22.296743 \n",
       "L 288.751282 20.004509 \n",
       "L 289.055951 21.716433 \n",
       "L 289.360619 22.09363 \n",
       "L 289.665287 22.70297 \n",
       "L 289.969955 22.70297 \n",
       "L 290.274624 23.128518 \n",
       "L 290.579292 22.935089 \n",
       "L 290.88396 26.339569 \n",
       "L 291.188629 25.817295 \n",
       "L 291.493297 24.124716 \n",
       "L 291.797965 20.565497 \n",
       "L 292.102634 20.865313 \n",
       "L 292.407302 22.896399 \n",
       "L 292.71197 23.641132 \n",
       "L 293.016639 25.091907 \n",
       "L 293.625975 21.967898 \n",
       "L 293.930643 20.604164 \n",
       "L 294.235312 23.418673 \n",
       "L 294.53998 24.10537 \n",
       "L 294.844648 22.857708 \n",
       "L 295.149317 23.225244 \n",
       "L 295.453985 23.157524 \n",
       "L 295.758653 21.232849 \n",
       "L 296.063322 20.749242 \n",
       "L 296.36799 23.302602 \n",
       "L 296.672658 22.074285 \n",
       "L 297.281995 24.09571 \n",
       "L 298.196 20.149586 \n",
       "L 298.500668 24.02799 \n",
       "L 298.805336 22.586898 \n",
       "L 299.110005 22.509518 \n",
       "L 299.414673 22.316089 \n",
       "L 299.719341 21.416617 \n",
       "L 300.02401 20.991046 \n",
       "L 300.328678 20.758926 \n",
       "L 300.633346 22.287059 \n",
       "L 300.938014 20.217307 \n",
       "L 301.242683 20.294664 \n",
       "L 301.547351 22.557892 \n",
       "L 302.156688 20.130265 \n",
       "L 302.461356 20.39139 \n",
       "L 302.766024 23.167209 \n",
       "L 303.070693 22.200017 \n",
       "L 303.680029 25.855962 \n",
       "L 303.984697 24.782407 \n",
       "L 304.289366 21.484314 \n",
       "L 304.594034 20.139926 \n",
       "L 304.898702 26.030069 \n",
       "L 305.508039 21.842166 \n",
       "L 305.812707 24.192412 \n",
       "L 306.117376 23.805554 \n",
       "L 306.422044 24.086025 \n",
       "L 306.726712 20.362384 \n",
       "L 307.031381 25.449759 \n",
       "L 307.336049 28.196548 \n",
       "L 307.945385 20.459087 \n",
       "L 308.250054 31.368908 \n",
       "L 308.554722 22.05494 \n",
       "L 308.85939 23.041476 \n",
       "L 309.164059 22.180672 \n",
       "L 309.468727 22.40313 \n",
       "L 309.773395 19.298467 \n",
       "L 310.078064 19.50158 \n",
       "L 310.382732 23.428334 \n",
       "L 310.992068 19.317812 \n",
       "L 311.296737 20.014193 \n",
       "L 311.601405 22.828702 \n",
       "L 311.906073 21.948553 \n",
       "L 312.210742 19.830425 \n",
       "L 312.51541 19.163073 \n",
       "L 312.820078 22.180672 \n",
       "L 313.429415 19.695032 \n",
       "L 313.734083 20.372045 \n",
       "L 314.038752 20.265658 \n",
       "L 314.34342 18.776192 \n",
       "L 314.648088 19.714354 \n",
       "L 314.952756 22.538547 \n",
       "L 315.257425 20.710575 \n",
       "L 315.562093 23.737834 \n",
       "L 315.866761 21.54235 \n",
       "L 316.17143 21.310207 \n",
       "L 316.476098 19.859431 \n",
       "L 316.780766 19.240454 \n",
       "L 317.085435 21.145784 \n",
       "L 317.390103 20.023854 \n",
       "L 317.694771 21.832505 \n",
       "L 317.999439 21.348897 \n",
       "L 318.304108 22.644934 \n",
       "L 318.608776 19.917467 \n",
       "L 318.913444 19.588622 \n",
       "L 319.218113 20.904004 \n",
       "L 319.522781 19.820764 \n",
       "L 319.827449 20.217307 \n",
       "L 320.132118 20.217307 \n",
       "L 320.436786 21.561695 \n",
       "L 320.741454 19.511264 \n",
       "L 321.046123 20.39139 \n",
       "L 321.350791 19.017996 \n",
       "L 321.655459 19.182419 \n",
       "L 321.960127 19.627312 \n",
       "L 322.264796 18.979305 \n",
       "L 322.569464 19.656342 \n",
       "L 322.874132 18.776192 \n",
       "L 323.178801 20.449426 \n",
       "L 323.483469 20.642855 \n",
       "L 323.788137 26.736111 \n",
       "L 324.092806 39.087024 \n",
       "L 324.397474 26.997237 \n",
       "L 324.702142 20.304348 \n",
       "L 325.006811 29.540936 \n",
       "L 325.311479 21.745439 \n",
       "L 325.616147 23.24459 \n",
       "L 325.920815 22.490172 \n",
       "L 326.225484 19.588622 \n",
       "L 326.530152 26.059075 \n",
       "L 326.83482 28.496387 \n",
       "L 327.139489 22.848047 \n",
       "L 327.444157 28.370655 \n",
       "L 327.748825 27.50019 \n",
       "L 328.053494 22.896399 \n",
       "L 328.358162 27.113308 \n",
       "L 328.66283 22.132321 \n",
       "L 328.967498 26.165462 \n",
       "L 329.272167 23.109173 \n",
       "L 329.576835 21.600362 \n",
       "L 329.881503 19.172734 \n",
       "L 330.186172 20.207622 \n",
       "L 330.49084 22.548208 \n",
       "L 330.795508 23.776525 \n",
       "L 331.100177 18.95996 \n",
       "L 331.709513 20.991046 \n",
       "L 332.014182 22.064624 \n",
       "L 332.31885 18.65046 \n",
       "L 332.623518 20.497777 \n",
       "L 332.928186 18.389334 \n",
       "L 333.232855 21.126462 \n",
       "L 333.537523 20.991046 \n",
       "L 333.842191 19.472574 \n",
       "L 334.14686 18.524727 \n",
       "L 334.451528 18.544073 \n",
       "L 334.756196 18.302269 \n",
       "L 335.060865 19.636996 \n",
       "L 335.365533 19.163073 \n",
       "L 335.670201 17.383452 \n",
       "L 335.974869 17.750988 \n",
       "L 336.279538 19.008311 \n",
       "L 336.584206 19.424222 \n",
       "L 336.888874 17.393137 \n",
       "L 337.193543 17.741304 \n",
       "L 337.498211 17.905727 \n",
       "L 337.802879 19.646657 \n",
       "L 338.107548 20.014193 \n",
       "L 338.412216 22.287059 \n",
       "L 338.716884 26.92954 \n",
       "L 339.021553 25.207955 \n",
       "L 339.326221 21.54235 \n",
       "L 339.630889 19.559615 \n",
       "L 339.935557 22.760982 \n",
       "L 340.240226 19.511264 \n",
       "L 340.544894 18.099179 \n",
       "L 340.849562 22.790012 \n",
       "L 341.154231 23.031792 \n",
       "L 341.458899 19.114722 \n",
       "L 341.763567 21.523004 \n",
       "L 342.068236 21.532665 \n",
       "L 342.372904 18.350644 \n",
       "L 342.677572 19.675687 \n",
       "L 342.98224 20.159271 \n",
       "L 343.286909 19.830425 \n",
       "L 343.591577 18.621454 \n",
       "L 343.896245 19.685348 \n",
       "L 344.505582 18.253918 \n",
       "L 344.81025 18.340959 \n",
       "L 345.114919 21.54235 \n",
       "L 345.419587 20.991046 \n",
       "L 345.724255 19.433883 \n",
       "L 346.028924 18.486037 \n",
       "L 346.333592 18.495721 \n",
       "L 346.63826 20.845968 \n",
       "L 346.942928 18.282924 \n",
       "L 347.247597 18.282924 \n",
       "L 347.552265 17.809024 \n",
       "L 347.856933 17.857375 \n",
       "L 348.161602 18.718156 \n",
       "L 348.46627 17.083636 \n",
       "L 348.770938 18.37965 \n",
       "L 349.075607 18.147531 \n",
       "L 349.380275 17.52853 \n",
       "L 349.684943 18.021798 \n",
       "L 349.684943 18.021798 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path clip-path=\"url(#p39242617b3)\" d=\"M 45.321307 85.970648 \n",
       "L 45.625975 177.708524 \n",
       "L 45.930643 143.67673 \n",
       "L 46.235312 86.666948 \n",
       "L 46.53998 83.620623 \n",
       "L 46.844648 171.180684 \n",
       "L 47.149317 197.988338 \n",
       "L 47.453985 94.587393 \n",
       "L 47.758653 72.827938 \n",
       "L 48.063322 148.724923 \n",
       "L 48.36799 165.436191 \n",
       "L 48.672658 53.331453 \n",
       "L 48.977326 43.322109 \n",
       "L 49.281995 145.330449 \n",
       "L 49.586663 165.958419 \n",
       "L 49.891331 131.665511 \n",
       "L 50.196 59.424102 \n",
       "L 50.500668 99.113358 \n",
       "L 50.805336 113.64868 \n",
       "L 51.110005 135.408135 \n",
       "L 51.414673 122.526538 \n",
       "L 51.719341 49.762901 \n",
       "L 52.328678 40.101713 \n",
       "L 52.633346 41.40727 \n",
       "L 52.938014 44.627678 \n",
       "L 53.242683 44.366564 \n",
       "L 53.547351 87.885475 \n",
       "L 53.852019 91.366997 \n",
       "L 54.156688 46.977691 \n",
       "L 54.461356 45.236936 \n",
       "L 54.766024 46.020278 \n",
       "L 55.070693 47.499919 \n",
       "L 55.375361 47.674002 \n",
       "L 55.680029 46.977691 \n",
       "L 55.984697 47.848074 \n",
       "L 56.289366 48.109188 \n",
       "L 56.898702 40.449857 \n",
       "L 57.508039 40.536899 \n",
       "L 57.812707 41.059126 \n",
       "L 58.117376 40.972085 \n",
       "L 58.422044 40.710971 \n",
       "L 58.726712 39.840599 \n",
       "L 59.336049 40.885043 \n",
       "L 59.640717 41.581354 \n",
       "L 60.250054 41.40727 \n",
       "L 60.554722 41.668384 \n",
       "L 60.85939 41.494312 \n",
       "L 61.164059 40.449857 \n",
       "L 61.468727 39.927629 \n",
       "L 62.078064 40.188743 \n",
       "L 62.992068 39.840599 \n",
       "L 63.296737 39.666515 \n",
       "L 63.601405 40.362827 \n",
       "L 63.906073 40.101713 \n",
       "L 64.51541 38.883174 \n",
       "L 64.820078 38.709102 \n",
       "L 65.734083 39.057258 \n",
       "L 66.34342 39.579485 \n",
       "L 66.648088 39.492443 \n",
       "L 66.952756 40.014671 \n",
       "L 67.257425 39.840599 \n",
       "L 67.562093 39.492443 \n",
       "L 67.866761 39.840599 \n",
       "L 68.17143 39.492443 \n",
       "L 68.476098 38.970216 \n",
       "L 68.780766 39.318371 \n",
       "L 69.085435 39.405402 \n",
       "L 69.694771 38.099832 \n",
       "L 69.999439 37.664647 \n",
       "L 70.304108 37.490575 \n",
       "L 70.608776 37.490575 \n",
       "L 70.913444 37.838719 \n",
       "L 71.218113 38.012802 \n",
       "L 71.522781 37.664647 \n",
       "L 71.827449 37.577605 \n",
       "L 72.132118 37.664647 \n",
       "L 72.436786 37.577605 \n",
       "L 72.741454 37.664647 \n",
       "L 73.350791 37.316503 \n",
       "L 73.960127 37.490575 \n",
       "L 74.264796 37.403533 \n",
       "L 75.178801 37.577605 \n",
       "L 77.006811 37.142419 \n",
       "L 77.616147 37.229461 \n",
       "L 77.920815 37.142419 \n",
       "L 78.225484 37.229461 \n",
       "L 79.139489 37.055389 \n",
       "L 79.444157 37.055389 \n",
       "L 79.748825 36.881305 \n",
       "L 80.053494 36.968347 \n",
       "L 80.358162 37.229461 \n",
       "L 80.66283 37.229461 \n",
       "L 80.967498 37.403533 \n",
       "L 81.576835 37.316503 \n",
       "L 82.186172 37.403533 \n",
       "L 82.795508 37.316503 \n",
       "L 83.404845 37.229461 \n",
       "L 83.709513 37.229461 \n",
       "L 84.014182 36.968347 \n",
       "L 84.31885 36.881305 \n",
       "L 84.623518 37.142419 \n",
       "L 85.232855 36.707233 \n",
       "L 85.537523 36.185005 \n",
       "L 85.842191 36.185005 \n",
       "L 86.14686 36.359077 \n",
       "L 86.451528 36.272047 \n",
       "L 86.756196 36.359077 \n",
       "L 87.060865 36.010933 \n",
       "L 87.365533 36.010933 \n",
       "L 87.974869 35.74982 \n",
       "L 88.279538 35.314622 \n",
       "L 88.584206 35.662778 \n",
       "L 88.888874 35.14055 \n",
       "L 89.193543 35.401664 \n",
       "L 89.498211 34.879436 \n",
       "L 89.802879 35.575736 \n",
       "L 90.107548 34.53128 \n",
       "L 90.412216 36.446119 \n",
       "L 90.716884 34.618322 \n",
       "L 91.021553 36.794275 \n",
       "L 91.326221 34.096095 \n",
       "L 91.630889 34.270178 \n",
       "L 91.935557 35.314622 \n",
       "L 92.240226 33.399795 \n",
       "L 92.544894 34.53128 \n",
       "L 92.849562 34.792394 \n",
       "L 93.154231 33.747951 \n",
       "L 93.458899 35.053508 \n",
       "L 93.763567 34.270178 \n",
       "L 94.372904 34.966478 \n",
       "L 94.677572 33.312753 \n",
       "L 95.286909 34.879436 \n",
       "L 95.591577 33.399795 \n",
       "L 95.896245 33.486837 \n",
       "L 96.200914 34.53128 \n",
       "L 96.505582 33.138681 \n",
       "L 96.81025 33.225723 \n",
       "L 97.114919 33.660909 \n",
       "L 97.419587 32.703495 \n",
       "L 97.724255 33.747951 \n",
       "L 98.028924 32.964609 \n",
       "L 98.333592 32.616453 \n",
       "L 98.63826 33.225723 \n",
       "L 98.942928 32.964609 \n",
       "L 99.247597 33.138681 \n",
       "L 99.552265 32.35534 \n",
       "L 99.856933 32.703495 \n",
       "L 100.161602 32.790525 \n",
       "L 100.46627 32.529412 \n",
       "L 100.770938 33.138681 \n",
       "L 101.075607 32.529412 \n",
       "L 101.380275 32.529412 \n",
       "L 101.684943 32.877567 \n",
       "L 101.989611 32.181268 \n",
       "L 102.29428 32.442381 \n",
       "L 103.817621 32.007184 \n",
       "L 104.12229 32.35534 \n",
       "L 104.426958 32.268298 \n",
       "L 104.731626 31.65904 \n",
       "L 105.036295 32.529412 \n",
       "L 105.950299 32.35534 \n",
       "L 106.254968 31.65904 \n",
       "L 106.864304 31.74607 \n",
       "L 107.168973 31.136812 \n",
       "L 107.473641 31.65904 \n",
       "L 108.082978 31.223854 \n",
       "L 108.387646 32.877567 \n",
       "L 108.692314 33.747951 \n",
       "L 108.996982 33.486837 \n",
       "L 109.301651 31.484956 \n",
       "L 109.606319 31.04977 \n",
       "L 110.215656 32.442381 \n",
       "L 110.520324 33.138681 \n",
       "L 110.824992 30.962729 \n",
       "L 111.129661 32.877567 \n",
       "L 111.434329 33.051639 \n",
       "L 111.738997 31.74607 \n",
       "L 112.043666 33.486837 \n",
       "L 112.348334 30.092357 \n",
       "L 112.653002 30.788668 \n",
       "L 112.95767 35.14055 \n",
       "L 113.262339 31.65904 \n",
       "L 113.567007 30.788668 \n",
       "L 113.871675 32.35534 \n",
       "L 114.176344 30.701626 \n",
       "L 114.481012 30.875687 \n",
       "L 114.78568 31.571998 \n",
       "L 115.090349 30.179399 \n",
       "L 115.395017 30.005315 \n",
       "L 115.699685 30.701626 \n",
       "L 116.004354 30.353459 \n",
       "L 116.61369 32.007184 \n",
       "L 116.918358 32.094226 \n",
       "L 117.223027 34.966478 \n",
       "L 117.527695 34.357208 \n",
       "L 117.832363 36.881305 \n",
       "L 118.137032 31.833112 \n",
       "L 118.4417 29.657171 \n",
       "L 118.746368 31.833112 \n",
       "L 119.051037 30.179399 \n",
       "L 119.355705 30.614585 \n",
       "L 119.660373 32.094226 \n",
       "L 119.965041 30.788668 \n",
       "L 120.26971 30.440501 \n",
       "L 120.574378 30.527543 \n",
       "L 120.879046 30.005315 \n",
       "L 121.183715 29.831255 \n",
       "L 121.793051 32.529412 \n",
       "L 122.09772 35.923892 \n",
       "L 122.402388 34.705364 \n",
       "L 123.011725 30.092357 \n",
       "L 123.316393 30.788668 \n",
       "L 123.621061 32.703495 \n",
       "L 124.230398 30.440501 \n",
       "L 124.535066 30.440501 \n",
       "L 124.839734 30.092357 \n",
       "L 125.449071 31.136812 \n",
       "L 125.753739 30.440501 \n",
       "L 126.058408 30.353459 \n",
       "L 126.363076 31.136812 \n",
       "L 126.667744 29.918273 \n",
       "L 126.972412 31.484956 \n",
       "L 127.277081 35.662778 \n",
       "L 127.581749 33.051639 \n",
       "L 127.886417 34.792394 \n",
       "L 128.191086 31.484956 \n",
       "L 128.495754 30.962729 \n",
       "L 128.800422 31.223854 \n",
       "L 129.105091 29.744213 \n",
       "L 129.409759 30.353459 \n",
       "L 129.714427 31.397926 \n",
       "L 130.019096 30.353459 \n",
       "L 130.323764 31.74607 \n",
       "L 130.628432 34.966478 \n",
       "L 130.9331 29.831255 \n",
       "L 131.237769 31.04977 \n",
       "L 131.542437 36.881305 \n",
       "L 131.847105 29.657171 \n",
       "L 132.456442 37.316503 \n",
       "L 132.76111 29.831255 \n",
       "L 133.065779 40.275785 \n",
       "L 133.370447 67.866775 \n",
       "L 133.675115 37.229461 \n",
       "L 133.979783 54.027765 \n",
       "L 134.284452 36.272047 \n",
       "L 134.58912 37.664647 \n",
       "L 134.893788 45.49805 \n",
       "L 135.198457 42.01654 \n",
       "L 135.503125 45.41102 \n",
       "L 135.807793 44.627678 \n",
       "L 136.112462 81.879868 \n",
       "L 136.41713 75.78722 \n",
       "L 136.721798 46.455475 \n",
       "L 137.026467 60.642641 \n",
       "L 137.331135 41.32024 \n",
       "L 137.635803 49.849943 \n",
       "L 137.940471 40.014671 \n",
       "L 138.24514 40.449857 \n",
       "L 138.549808 49.762901 \n",
       "L 138.854476 43.496181 \n",
       "L 139.159145 42.71284 \n",
       "L 139.463813 42.364696 \n",
       "L 139.768481 39.579485 \n",
       "L 140.07315 39.144288 \n",
       "L 140.682486 36.881305 \n",
       "L 140.987154 32.529412 \n",
       "L 141.291823 33.051639 \n",
       "L 141.596491 36.272047 \n",
       "L 141.901159 38.273916 \n",
       "L 142.205828 37.490575 \n",
       "L 142.510496 33.486837 \n",
       "L 142.815164 32.790525 \n",
       "L 143.119833 33.834981 \n",
       "L 143.424501 35.74982 \n",
       "L 143.729169 35.575736 \n",
       "L 144.033838 32.790525 \n",
       "L 144.338506 33.225723 \n",
       "L 144.643174 32.529412 \n",
       "L 144.947842 32.964609 \n",
       "L 145.252511 34.009065 \n",
       "L 145.557179 32.094226 \n",
       "L 146.166516 32.790525 \n",
       "L 146.471184 33.051639 \n",
       "L 147.080521 32.442381 \n",
       "L 147.385189 32.703495 \n",
       "L 147.689857 32.094226 \n",
       "L 148.603862 32.007184 \n",
       "L 148.90853 31.920154 \n",
       "L 149.213199 31.571998 \n",
       "L 149.517867 32.007184 \n",
       "L 150.431872 31.484956 \n",
       "L 150.73654 30.875687 \n",
       "L 151.345877 31.484956 \n",
       "L 151.650545 30.875687 \n",
       "L 152.56455 30.962729 \n",
       "L 153.173887 30.353459 \n",
       "L 153.478555 30.788668 \n",
       "L 153.783223 30.527543 \n",
       "L 154.39256 30.440501 \n",
       "L 154.697228 30.353459 \n",
       "L 155.001896 30.005315 \n",
       "L 155.306565 30.005315 \n",
       "L 155.915901 29.744213 \n",
       "L 157.134575 29.396046 \n",
       "L 157.439243 29.570129 \n",
       "L 157.743911 29.396046 \n",
       "L 158.04858 29.570129 \n",
       "L 158.353248 29.221985 \n",
       "L 158.657916 29.744213 \n",
       "L 158.962584 29.570129 \n",
       "L 159.267253 29.221985 \n",
       "L 159.876589 29.657171 \n",
       "L 160.181258 29.483087 \n",
       "L 160.485926 29.483087 \n",
       "L 160.790594 29.221985 \n",
       "L 161.095263 29.657171 \n",
       "L 161.704599 28.96086 \n",
       "L 162.009268 29.483087 \n",
       "L 162.313936 29.221985 \n",
       "L 162.618604 29.309027 \n",
       "L 162.923272 29.221985 \n",
       "L 163.532609 29.483087 \n",
       "L 166.274624 29.396046 \n",
       "L 166.579292 29.309027 \n",
       "L 167.188629 29.483087 \n",
       "L 167.493297 29.047902 \n",
       "L 167.797965 28.96086 \n",
       "L 168.102634 30.353459 \n",
       "L 168.407302 30.962729 \n",
       "L 168.71197 45.323978 \n",
       "L 169.016639 31.310896 \n",
       "L 169.321307 29.221985 \n",
       "L 169.625975 35.227592 \n",
       "L 169.930643 34.009065 \n",
       "L 170.235312 30.353459 \n",
       "L 170.53998 39.318371 \n",
       "L 171.149317 30.701626 \n",
       "L 171.453985 32.442381 \n",
       "L 171.758653 29.744213 \n",
       "L 172.063322 33.399795 \n",
       "L 172.36799 31.920154 \n",
       "L 172.672658 33.051639 \n",
       "L 172.977326 29.657171 \n",
       "L 173.281995 30.962729 \n",
       "L 173.891331 30.701626 \n",
       "L 174.196 29.396046 \n",
       "L 174.500668 34.618322 \n",
       "L 174.805336 29.396046 \n",
       "L 175.110005 30.788668 \n",
       "L 175.414673 30.353459 \n",
       "L 175.719341 29.657171 \n",
       "L 176.02401 31.65904 \n",
       "L 176.328678 29.831255 \n",
       "L 176.633346 31.484956 \n",
       "L 176.938014 28.96086 \n",
       "L 177.242683 30.701626 \n",
       "L 177.547351 29.570129 \n",
       "L 177.852019 30.005315 \n",
       "L 178.156688 29.570129 \n",
       "L 178.461356 30.701626 \n",
       "L 178.766024 28.786799 \n",
       "L 179.070693 30.179399 \n",
       "L 179.375361 29.047902 \n",
       "L 179.680029 29.570129 \n",
       "L 180.289366 28.786799 \n",
       "L 180.594034 29.483087 \n",
       "L 180.898702 28.786799 \n",
       "L 181.203371 29.570129 \n",
       "L 181.508039 29.657171 \n",
       "L 181.812707 29.483087 \n",
       "L 182.117376 28.438632 \n",
       "L 182.422044 29.831255 \n",
       "L 182.726712 28.438632 \n",
       "L 183.031381 29.831255 \n",
       "L 183.336049 29.657171 \n",
       "L 183.640717 28.786799 \n",
       "L 183.945385 29.396046 \n",
       "L 184.250054 28.873818 \n",
       "L 184.554722 29.134943 \n",
       "L 184.85939 28.003446 \n",
       "L 185.164059 29.047902 \n",
       "L 185.468727 28.612716 \n",
       "L 185.773395 29.221985 \n",
       "L 186.078064 29.396046 \n",
       "L 186.6874 30.440501 \n",
       "L 186.992068 31.571998 \n",
       "L 187.296737 31.136812 \n",
       "L 187.601405 30.353459 \n",
       "L 187.906073 28.96086 \n",
       "L 188.210742 30.266441 \n",
       "L 188.51541 30.875687 \n",
       "L 188.820078 29.309027 \n",
       "L 189.124747 29.396046 \n",
       "L 189.429415 30.353459 \n",
       "L 189.734083 29.744213 \n",
       "L 190.038752 29.570129 \n",
       "L 190.34342 28.35159 \n",
       "L 190.648088 32.442381 \n",
       "L 190.952756 29.570129 \n",
       "L 191.562093 30.005315 \n",
       "L 191.866761 33.051639 \n",
       "L 192.17143 30.440501 \n",
       "L 192.780766 31.484956 \n",
       "L 193.085435 31.04977 \n",
       "L 193.390103 29.570129 \n",
       "L 193.694771 29.309027 \n",
       "L 193.999439 30.179399 \n",
       "L 194.304108 28.525674 \n",
       "L 194.608776 29.309027 \n",
       "L 194.913444 28.264572 \n",
       "L 195.218113 28.699758 \n",
       "L 195.522781 28.525674 \n",
       "L 195.827449 28.96086 \n",
       "L 196.132118 28.264572 \n",
       "L 196.741454 28.35159 \n",
       "L 197.046123 29.047902 \n",
       "L 197.350791 29.134943 \n",
       "L 197.655459 28.612716 \n",
       "L 197.960127 28.612716 \n",
       "L 198.264796 29.396046 \n",
       "L 198.569464 28.438632 \n",
       "L 198.874132 28.525674 \n",
       "L 199.178801 29.047902 \n",
       "L 199.483469 29.134943 \n",
       "L 199.788137 28.003446 \n",
       "L 200.092806 28.525674 \n",
       "L 200.397474 28.525674 \n",
       "L 201.006811 30.614585 \n",
       "L 201.311479 31.223854 \n",
       "L 201.920815 28.786799 \n",
       "L 202.225484 28.96086 \n",
       "L 202.530152 30.614585 \n",
       "L 203.139489 29.221985 \n",
       "L 203.444157 29.134943 \n",
       "L 203.748825 30.353459 \n",
       "L 204.053494 30.614585 \n",
       "L 204.358162 29.570129 \n",
       "L 204.967498 34.879436 \n",
       "L 205.576835 30.005315 \n",
       "L 205.881503 38.62206 \n",
       "L 206.186172 28.438632 \n",
       "L 206.49084 31.223854 \n",
       "L 206.795508 28.35159 \n",
       "L 207.100177 28.17753 \n",
       "L 207.709513 32.268298 \n",
       "L 208.014182 29.918273 \n",
       "L 208.31885 31.65904 \n",
       "L 208.623518 28.699758 \n",
       "L 208.928186 30.092357 \n",
       "L 209.232855 29.831255 \n",
       "L 209.842191 28.96086 \n",
       "L 210.14686 29.570129 \n",
       "L 210.451528 28.699758 \n",
       "L 210.756196 28.525674 \n",
       "L 211.060865 29.396046 \n",
       "L 211.365533 28.525674 \n",
       "L 211.670201 29.744213 \n",
       "L 211.974869 29.047902 \n",
       "L 212.279538 29.657171 \n",
       "L 212.584206 29.570129 \n",
       "L 212.888874 28.17753 \n",
       "L 213.193543 29.309027 \n",
       "L 213.498211 28.699758 \n",
       "L 213.802879 29.744213 \n",
       "L 214.107548 28.525674 \n",
       "L 214.412216 29.134943 \n",
       "L 214.716884 27.916404 \n",
       "L 215.021553 29.483087 \n",
       "L 215.326221 28.873818 \n",
       "L 215.630889 28.873818 \n",
       "L 215.935557 28.003446 \n",
       "L 216.240226 28.786799 \n",
       "L 216.849562 28.003446 \n",
       "L 217.154231 28.35159 \n",
       "L 217.458899 28.96086 \n",
       "L 217.763567 27.56826 \n",
       "L 218.372904 28.612716 \n",
       "L 218.677572 27.742344 \n",
       "L 218.98224 28.612716 \n",
       "L 219.286909 28.264572 \n",
       "L 219.591577 28.699758 \n",
       "L 219.896245 28.17753 \n",
       "L 220.200914 28.090488 \n",
       "L 220.505582 29.047902 \n",
       "L 220.81025 28.525674 \n",
       "L 221.114919 29.309027 \n",
       "L 221.419587 28.17753 \n",
       "L 221.724255 28.17753 \n",
       "L 222.028924 28.525674 \n",
       "L 222.333592 28.35159 \n",
       "L 222.63826 29.221985 \n",
       "L 222.942928 28.264572 \n",
       "L 223.247597 28.264572 \n",
       "L 223.552265 28.438632 \n",
       "L 223.856933 28.438632 \n",
       "L 224.161602 28.699758 \n",
       "L 224.46627 27.742344 \n",
       "L 224.770938 29.047902 \n",
       "L 225.380275 30.701626 \n",
       "L 225.684943 30.614585 \n",
       "L 225.989611 29.483087 \n",
       "L 226.29428 27.655302 \n",
       "L 226.903616 28.612716 \n",
       "L 227.208285 29.483087 \n",
       "L 227.512953 28.525674 \n",
       "L 228.12229 27.655302 \n",
       "L 228.426958 28.525674 \n",
       "L 228.731626 27.56826 \n",
       "L 229.036295 28.525674 \n",
       "L 229.645631 27.742344 \n",
       "L 229.950299 29.047902 \n",
       "L 230.254968 29.570129 \n",
       "L 230.559636 29.744213 \n",
       "L 230.864304 28.612716 \n",
       "L 231.168973 28.264572 \n",
       "L 231.473641 28.17753 \n",
       "L 231.778309 27.916404 \n",
       "L 232.387646 29.918273 \n",
       "L 232.996982 28.35159 \n",
       "L 233.301651 28.17753 \n",
       "L 233.606319 28.264572 \n",
       "L 233.910987 27.56826 \n",
       "L 234.824992 29.134943 \n",
       "L 235.129661 28.264572 \n",
       "L 235.434329 28.525674 \n",
       "L 235.738997 27.56826 \n",
       "L 236.043666 27.56826 \n",
       "L 236.348334 28.264572 \n",
       "L 236.653002 28.438632 \n",
       "L 236.95767 29.831255 \n",
       "L 237.262339 29.483087 \n",
       "L 237.567007 30.527543 \n",
       "L 237.871675 29.221985 \n",
       "L 238.176344 29.047902 \n",
       "L 238.481012 28.438632 \n",
       "L 238.78568 28.090488 \n",
       "L 239.395017 28.96086 \n",
       "L 239.699685 28.438632 \n",
       "L 240.004354 27.394177 \n",
       "L 240.309022 27.307135 \n",
       "L 240.61369 27.829363 \n",
       "L 240.918358 27.829363 \n",
       "L 241.223027 28.96086 \n",
       "L 241.527695 28.699758 \n",
       "L 241.832363 28.786799 \n",
       "L 242.137032 27.655302 \n",
       "L 242.4417 28.612716 \n",
       "L 242.746368 28.17753 \n",
       "L 243.051037 28.003446 \n",
       "L 243.355705 27.481219 \n",
       "L 243.660373 27.394177 \n",
       "L 243.965041 28.264572 \n",
       "L 244.574378 30.527543 \n",
       "L 244.879046 30.962729 \n",
       "L 245.183715 32.181268 \n",
       "L 245.488383 27.307135 \n",
       "L 245.793051 27.916404 \n",
       "L 246.09772 31.136812 \n",
       "L 246.402388 30.266441 \n",
       "L 246.707056 28.438632 \n",
       "L 247.011725 30.179399 \n",
       "L 247.316393 29.483087 \n",
       "L 247.621061 28.003446 \n",
       "L 247.925729 30.788668 \n",
       "L 248.839734 28.090488 \n",
       "L 249.144403 30.005315 \n",
       "L 249.449071 28.612716 \n",
       "L 249.753739 30.266441 \n",
       "L 250.058408 28.090488 \n",
       "L 250.363076 29.831255 \n",
       "L 250.667744 27.481219 \n",
       "L 250.972412 28.525674 \n",
       "L 251.277081 29.134943 \n",
       "L 251.581749 28.17753 \n",
       "L 251.886417 30.005315 \n",
       "L 252.191086 30.266441 \n",
       "L 252.495754 28.786799 \n",
       "L 252.800422 27.916404 \n",
       "L 253.105091 26.436763 \n",
       "L 253.409759 29.134943 \n",
       "L 253.714427 28.612716 \n",
       "L 254.019096 30.962729 \n",
       "L 254.323764 28.17753 \n",
       "L 254.628432 29.570129 \n",
       "L 254.9331 29.744213 \n",
       "L 255.237769 33.573867 \n",
       "L 255.542437 28.699758 \n",
       "L 255.847105 29.047902 \n",
       "L 256.151774 29.221985 \n",
       "L 256.456442 26.784931 \n",
       "L 256.76111 29.570129 \n",
       "L 257.065779 29.047902 \n",
       "L 257.370447 27.394177 \n",
       "L 257.675115 27.655302 \n",
       "L 257.979783 29.047902 \n",
       "L 258.284452 28.96086 \n",
       "L 258.58912 27.220116 \n",
       "L 258.893788 29.657171 \n",
       "L 259.198457 27.133075 \n",
       "L 259.503125 27.481219 \n",
       "L 259.807793 25.740475 \n",
       "L 260.112462 28.17753 \n",
       "L 260.41713 27.829363 \n",
       "L 260.721798 27.655302 \n",
       "L 261.026467 25.827494 \n",
       "L 261.331135 28.699758 \n",
       "L 261.635803 28.35159 \n",
       "L 261.940471 27.220116 \n",
       "L 262.24514 28.612716 \n",
       "L 262.854476 26.523805 \n",
       "L 263.159145 25.914536 \n",
       "L 263.463813 26.262703 \n",
       "L 263.768481 27.916404 \n",
       "L 264.07315 26.001577 \n",
       "L 264.377818 29.221985 \n",
       "L 264.682486 28.612716 \n",
       "L 264.987154 29.657171 \n",
       "L 265.291823 28.525674 \n",
       "L 265.596491 30.005315 \n",
       "L 265.901159 28.003446 \n",
       "L 266.205828 27.046033 \n",
       "L 266.510496 27.916404 \n",
       "L 266.815164 28.003446 \n",
       "L 267.119833 26.436763 \n",
       "L 267.424501 26.001577 \n",
       "L 267.729169 26.262703 \n",
       "L 268.033838 27.916404 \n",
       "L 268.338506 26.610847 \n",
       "L 268.643174 27.829363 \n",
       "L 268.947842 27.046033 \n",
       "L 269.252511 28.525674 \n",
       "L 269.557179 26.349721 \n",
       "L 269.861847 28.17753 \n",
       "L 270.166516 28.003446 \n",
       "L 270.775852 25.914536 \n",
       "L 271.080521 27.307135 \n",
       "L 271.385189 26.088619 \n",
       "L 271.689857 25.47935 \n",
       "L 271.994525 26.175661 \n",
       "L 272.299194 26.175661 \n",
       "L 272.603862 27.220116 \n",
       "L 272.90853 26.262703 \n",
       "L 273.517867 30.614585 \n",
       "L 273.822535 39.492443 \n",
       "L 274.127204 29.918273 \n",
       "L 274.431872 27.742344 \n",
       "L 274.73654 28.96086 \n",
       "L 275.041209 29.396046 \n",
       "L 275.345877 28.438632 \n",
       "L 275.650545 28.438632 \n",
       "L 275.955213 34.44425 \n",
       "L 276.56455 29.134943 \n",
       "L 276.869218 35.488706 \n",
       "L 277.173887 29.570129 \n",
       "L 277.478555 31.920154 \n",
       "L 277.783223 28.17753 \n",
       "L 278.087892 33.138681 \n",
       "L 278.39256 32.268298 \n",
       "L 278.697228 35.14055 \n",
       "L 279.001896 28.35159 \n",
       "L 279.306565 35.14055 \n",
       "L 279.611233 26.349721 \n",
       "L 279.915901 29.831255 \n",
       "L 280.22057 29.570129 \n",
       "L 280.525238 25.740475 \n",
       "L 280.829906 30.440501 \n",
       "L 281.134575 25.653433 \n",
       "L 281.439243 29.221985 \n",
       "L 281.743911 26.175661 \n",
       "L 282.04858 28.090488 \n",
       "L 282.353248 26.697889 \n",
       "L 282.657916 26.523805 \n",
       "L 282.962584 24.173792 \n",
       "L 283.267253 25.218248 \n",
       "L 283.571921 27.481219 \n",
       "L 283.876589 26.958991 \n",
       "L 284.181258 27.220116 \n",
       "L 284.485926 26.871949 \n",
       "L 284.790594 26.001577 \n",
       "L 285.095263 22.955253 \n",
       "L 285.399931 26.436763 \n",
       "L 285.704599 25.47935 \n",
       "L 286.009268 25.044164 \n",
       "L 286.313936 25.827494 \n",
       "L 286.618604 23.651565 \n",
       "L 286.923272 23.999709 \n",
       "L 287.227941 25.218248 \n",
       "L 287.532609 24.434894 \n",
       "L 287.837277 25.653433 \n",
       "L 288.141946 25.566392 \n",
       "L 288.446614 22.868211 \n",
       "L 289.055951 25.827494 \n",
       "L 289.360619 27.394177 \n",
       "L 289.665287 24.783038 \n",
       "L 289.969955 26.523805 \n",
       "L 290.274624 25.914536 \n",
       "L 290.579292 29.483087 \n",
       "L 290.88396 28.699758 \n",
       "L 291.188629 27.481219 \n",
       "L 291.493297 22.78117 \n",
       "L 291.797965 24.521936 \n",
       "L 292.102634 25.305266 \n",
       "L 292.407302 28.003446 \n",
       "L 292.71197 29.309027 \n",
       "L 293.321307 24.347853 \n",
       "L 293.625975 23.303397 \n",
       "L 293.930643 25.827494 \n",
       "L 294.235312 26.001577 \n",
       "L 294.53998 26.871949 \n",
       "L 295.149317 26.001577 \n",
       "L 295.453985 24.87008 \n",
       "L 295.758653 22.868211 \n",
       "L 296.063322 24.783038 \n",
       "L 296.36799 25.131206 \n",
       "L 296.977326 27.56826 \n",
       "L 297.891331 23.651565 \n",
       "L 298.196 25.044164 \n",
       "L 298.500668 24.957122 \n",
       "L 298.805336 26.871949 \n",
       "L 299.110005 25.305266 \n",
       "L 299.414673 24.521936 \n",
       "L 299.719341 26.001577 \n",
       "L 300.02401 24.521936 \n",
       "L 300.328678 23.825625 \n",
       "L 300.633346 23.651565 \n",
       "L 300.938014 23.999709 \n",
       "L 301.242683 25.653433 \n",
       "L 301.547351 23.042295 \n",
       "L 301.852019 24.260811 \n",
       "L 302.156688 23.129337 \n",
       "L 302.461356 24.957122 \n",
       "L 302.766024 24.434894 \n",
       "L 303.375361 28.438632 \n",
       "L 303.680029 28.003446 \n",
       "L 303.984697 25.47935 \n",
       "L 304.289366 24.347853 \n",
       "L 304.594034 27.742344 \n",
       "L 304.898702 27.829363 \n",
       "L 305.203371 24.957122 \n",
       "L 305.508039 26.697889 \n",
       "L 305.812707 25.131206 \n",
       "L 306.117376 27.394177 \n",
       "L 306.422044 23.042295 \n",
       "L 306.726712 28.525674 \n",
       "L 307.031381 30.092357 \n",
       "L 307.640717 23.564523 \n",
       "L 307.945385 32.703495 \n",
       "L 308.250054 25.305266 \n",
       "L 308.554722 26.262703 \n",
       "L 308.85939 25.131206 \n",
       "L 309.164059 25.47935 \n",
       "L 309.468727 23.651565 \n",
       "L 309.773395 24.173792 \n",
       "L 310.078064 25.740475 \n",
       "L 310.382732 25.305266 \n",
       "L 310.992068 23.912667 \n",
       "L 311.296737 24.173792 \n",
       "L 311.601405 25.044164 \n",
       "L 312.210742 23.303397 \n",
       "L 312.51541 25.044164 \n",
       "L 312.820078 25.392308 \n",
       "L 313.734083 23.738606 \n",
       "L 314.34342 23.477481 \n",
       "L 314.648088 25.392308 \n",
       "L 314.952756 24.69602 \n",
       "L 315.257425 27.916404 \n",
       "L 315.562093 24.957122 \n",
       "L 315.866761 23.564523 \n",
       "L 316.17143 24.783038 \n",
       "L 316.476098 23.477481 \n",
       "L 316.780766 25.740475 \n",
       "L 317.085435 22.955253 \n",
       "L 317.390103 25.47935 \n",
       "L 317.694771 23.738606 \n",
       "L 317.999439 27.133075 \n",
       "L 318.608776 22.258942 \n",
       "L 318.913444 26.262703 \n",
       "L 319.218113 24.434894 \n",
       "L 319.522781 24.173792 \n",
       "L 319.827449 24.521936 \n",
       "L 320.132118 25.131206 \n",
       "L 320.436786 23.042295 \n",
       "L 320.741454 25.566392 \n",
       "L 321.046123 23.477481 \n",
       "L 321.350791 23.303397 \n",
       "L 321.655459 23.564523 \n",
       "L 321.960127 21.649696 \n",
       "L 322.264796 21.99784 \n",
       "L 322.569464 23.390439 \n",
       "L 322.874132 23.999709 \n",
       "L 323.178801 24.173792 \n",
       "L 323.483469 28.264572 \n",
       "L 323.788137 39.840599 \n",
       "L 324.092806 27.829363 \n",
       "L 324.397474 23.738606 \n",
       "L 324.702142 32.529412 \n",
       "L 325.006811 23.912667 \n",
       "L 325.311479 26.871949 \n",
       "L 325.920815 21.562654 \n",
       "L 326.225484 28.438632 \n",
       "L 326.530152 32.442381 \n",
       "L 326.83482 24.260811 \n",
       "L 327.139489 30.788668 \n",
       "L 327.444157 29.570129 \n",
       "L 327.748825 26.697889 \n",
       "L 328.053494 29.744213 \n",
       "L 328.358162 25.47935 \n",
       "L 328.66283 29.047902 \n",
       "L 328.967498 27.220116 \n",
       "L 329.272167 24.521936 \n",
       "L 329.576835 23.303397 \n",
       "L 329.881503 25.305266 \n",
       "L 330.186172 25.218248 \n",
       "L 330.49084 27.220116 \n",
       "L 330.795508 24.347853 \n",
       "L 331.100177 25.392308 \n",
       "L 331.404845 23.825625 \n",
       "L 331.709513 24.608978 \n",
       "L 332.014182 22.084882 \n",
       "L 332.31885 25.914536 \n",
       "L 332.623518 22.345984 \n",
       "L 332.928186 23.999709 \n",
       "L 333.232855 24.347853 \n",
       "L 333.537523 22.520067 \n",
       "L 333.842191 22.78117 \n",
       "L 334.14686 22.520067 \n",
       "L 334.451528 22.520067 \n",
       "L 334.756196 23.390439 \n",
       "L 335.060865 22.084882 \n",
       "L 335.365533 22.607109 \n",
       "L 335.670201 21.736714 \n",
       "L 335.974869 21.99784 \n",
       "L 336.279538 21.823756 \n",
       "L 336.584206 21.38857 \n",
       "L 336.888874 22.171923 \n",
       "L 337.193543 21.823756 \n",
       "L 337.802879 22.433026 \n",
       "L 338.107548 24.783038 \n",
       "L 338.412216 29.831255 \n",
       "L 338.716884 26.262703 \n",
       "L 339.326221 22.171923 \n",
       "L 339.630889 24.521936 \n",
       "L 339.935557 23.564523 \n",
       "L 340.240226 21.475612 \n",
       "L 340.544894 26.175661 \n",
       "L 340.849562 26.088619 \n",
       "L 341.154231 21.736714 \n",
       "L 341.458899 24.260811 \n",
       "L 341.763567 24.08675 \n",
       "L 342.068236 21.736714 \n",
       "L 342.372904 23.825625 \n",
       "L 342.677572 23.216379 \n",
       "L 342.98224 24.08675 \n",
       "L 343.286909 22.084882 \n",
       "L 343.591577 22.258942 \n",
       "L 343.896245 23.216379 \n",
       "L 344.200914 21.562654 \n",
       "L 344.505582 21.214487 \n",
       "L 344.81025 23.216379 \n",
       "L 345.114919 23.477481 \n",
       "L 345.419587 23.303397 \n",
       "L 345.724255 20.953384 \n",
       "L 346.333592 23.651565 \n",
       "L 346.63826 21.040426 \n",
       "L 346.942928 20.60524 \n",
       "L 347.247597 22.433026 \n",
       "L 347.552265 21.214487 \n",
       "L 347.856933 21.38857 \n",
       "L 348.46627 22.955253 \n",
       "L 348.770938 21.38857 \n",
       "L 349.075607 21.127468 \n",
       "L 349.380275 21.99784 \n",
       "L 349.684943 21.214487 \n",
       "L 349.684943 21.214487 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 30.103125 224.64 \n",
       "L 30.103125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 364.903125 224.64 \n",
       "L 364.903125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 30.103125 224.64 \n",
       "L 364.903125 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 30.103125 7.2 \n",
       "L 364.903125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 200.498438 219.64 \n",
       "L 357.903125 219.64 \n",
       "Q 359.903125 219.64 359.903125 217.64 \n",
       "L 359.903125 188.7275 \n",
       "Q 359.903125 186.7275 357.903125 186.7275 \n",
       "L 200.498438 186.7275 \n",
       "Q 198.498438 186.7275 198.498438 188.7275 \n",
       "L 198.498438 217.64 \n",
       "Q 198.498438 219.64 200.498438 219.64 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_14\">\n",
       "     <path d=\"M 202.498438 194.825938 \n",
       "L 222.498438 194.825938 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\"/>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- categorical_accuracy -->\n",
       "     <defs>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 45.40625 27.984375 \n",
       "Q 45.40625 37.75 41.375 43.109375 \n",
       "Q 37.359375 48.484375 30.078125 48.484375 \n",
       "Q 22.859375 48.484375 18.828125 43.109375 \n",
       "Q 14.796875 37.75 14.796875 27.984375 \n",
       "Q 14.796875 18.265625 18.828125 12.890625 \n",
       "Q 22.859375 7.515625 30.078125 7.515625 \n",
       "Q 37.359375 7.515625 41.375 12.890625 \n",
       "Q 45.40625 18.265625 45.40625 27.984375 \n",
       "z\n",
       "M 54.390625 6.78125 \n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \n",
       "Q 42 -20.796875 29.203125 -20.796875 \n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \n",
       "L 12.109375 -9.1875 \n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \n",
       "Q 23.78125 -13.375 27.78125 -13.375 \n",
       "Q 36.625 -13.375 41.015625 -8.765625 \n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \n",
       "L 45.40625 9.625 \n",
       "Q 42.625 4.78125 38.28125 2.390625 \n",
       "Q 33.9375 0 27.875 0 \n",
       "Q 17.828125 0 11.671875 7.65625 \n",
       "Q 5.515625 15.328125 5.515625 27.984375 \n",
       "Q 5.515625 40.671875 11.671875 48.328125 \n",
       "Q 17.828125 56 27.875 56 \n",
       "Q 33.9375 56 38.28125 53.609375 \n",
       "Q 42.625 51.21875 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-103\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 50.984375 -16.609375 \n",
       "L 50.984375 -23.578125 \n",
       "L -0.984375 -23.578125 \n",
       "L -0.984375 -16.609375 \n",
       "z\n",
       "\" id=\"DejaVuSans-95\"/>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "      <path d=\"M 32.171875 -5.078125 \n",
       "Q 28.375 -14.84375 24.75 -17.8125 \n",
       "Q 21.140625 -20.796875 15.09375 -20.796875 \n",
       "L 7.90625 -20.796875 \n",
       "L 7.90625 -13.28125 \n",
       "L 13.1875 -13.28125 \n",
       "Q 16.890625 -13.28125 18.9375 -11.515625 \n",
       "Q 21 -9.765625 23.484375 -3.21875 \n",
       "L 25.09375 0.875 \n",
       "L 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 11.921875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-121\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(230.498438 198.325938)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"155.46875\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"216.992188\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "      <use x=\"280.46875\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"341.650391\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"382.763672\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"410.546875\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"465.527344\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"526.806641\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"554.589844\" xlink:href=\"#DejaVuSans-95\"/>\n",
       "      <use x=\"604.589844\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"665.869141\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"720.849609\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"775.830078\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "      <use x=\"839.208984\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"880.322266\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"941.601562\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"996.582031\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <path d=\"M 202.498438 209.782187 \n",
       "L 222.498438 209.782187 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\"/>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- val_categorical_accuracy -->\n",
       "     <defs>\n",
       "      <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(230.498438 213.282187)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n",
       "      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"253.222656\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"353.710938\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"415.234375\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "      <use x=\"478.710938\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"539.892578\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"581.005859\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"608.789062\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"663.769531\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"725.048828\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"752.832031\" xlink:href=\"#DejaVuSans-95\"/>\n",
       "      <use x=\"802.832031\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"864.111328\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"919.091797\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"974.072266\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "      <use x=\"1037.451172\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"1078.564453\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"1139.84375\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"1194.824219\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p39242617b3\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "ax.plot(history.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show learning rate for GIN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_history.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "74491/74491 [==============================] - 0s 5us/sample - loss: 0.0944 - categorical_accuracy: 0.5427\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate([X, fltr],\n",
    "                              Y,\n",
    "                              sample_weight=test_mask,\n",
    "                              batch_size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(dataLoc+'fires_merged_weather.csv', index_col=0,\n",
    "                  #dtype for smaller representation\n",
    "                  dtype={#'STAT_CAUSE_DESCR': 'category', 'STATE': 'category', 'DISCOVERY_MONTH': 'category',\n",
    "                        'Fog': 'bool', 'FunnelCloud': 'bool', 'Hail': 'bool', 'Rain': 'bool',\n",
    "                        'Snow': 'bool', 'Thunder': 'bool'}\n",
    "                  )\n",
    "#data['DAY'] = (data['FIRE_YEAR']-1992)*365+data['DISCOVERY_DOY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_sorted:\n",
    "    data['DAY'] = (data['FIRE_YEAR']-1992)*365+data['DISCOVERY_DOY']\n",
    "    data = data.sort_values('DAY', ascending=True, kind='mergesort')\n",
    "    data.drop('DAY', axis='columns', inplace=True)\n",
    "data.drop(['Unnamed: 0.1', 'index_x', 'FOD_ID', 'FIRE_NAME', 'DISCOVERY_DOY_SCALED',\n",
    "           'x_fire', 'y_fire', 'z_fire',\n",
    "           'index_y', 'Begin', 'End', 'Country', 'Day', 'ICAO', 'Latitude', 'Longitude', 'Month', 'STATION NAME',\n",
    "           'State', 'Station', 'USAF', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1', \n",
    "           'WBAN', 'Year', 'doy', 'x', 'y', 'z'\n",
    "          ],\n",
    "         axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Sorting so that adjacency matrix is aligned with rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Cleanup memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subset_2015:\n",
    "    data_full = data\n",
    "    data = data[data['FIRE_YEAR']==2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data[-data.shape[0]//2:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Preprocess the data\n",
    "\n",
    "Perform imputation as appropriate; split into train/validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Split into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(data['STAT_CAUSE_DESCR'])\n",
    "X = pd.get_dummies(data.drop('STAT_CAUSE_DESCR', axis='columns'))\n",
    "\n",
    "# Number of examples\n",
    "N = X.shape[0]\n",
    "# Number of features\n",
    "F = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Find the missing values, and set to NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for na in [9999.9, 999.9, 99.99]:\n",
    "    X[X == na] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Zero-imputing for some features -- see EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zcol in ['Gust', 'MaxWindspeed', 'Precip', 'SnowDepth', 'Visibility', 'Windspeed']:\n",
    "    X.loc[X[zcol].isna(), zcol] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Split into training/validation/testing\n",
    "\n",
    "We'll have three sets of nodes: training nodes, validation nodes, and censored nodes (nodes without valid labels).  Note that this is a semi-supervised problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine which nodes lack valid labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determing which fires fit in these categories\n",
    "censor = (data['STAT_CAUSE_DESCR'] == 'Missing/Undefined') | (data['STAT_CAUSE_DESCR'] == 'Miscellaneous')\n",
    "\n",
    "# Drop these levels from the label\n",
    "Y = Y.drop(['Missing/Undefined', 'Miscellaneous'], axis='columns').values\n",
    "\n",
    "# Number of classes left\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ind  = np.arange(data.shape[0])[~censor]\n",
    "censor_ind = np.arange(data.shape[0])[censor ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine which nodes lack valid labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " train_ind, test_ind = train_test_split(labeled_ind, test_size=0.1, random_state=42)\n",
    "\n",
    " train_ind, val_ind = train_test_split(train_ind, test_size=0.1, random_state=42)\n",
    "\n",
    "# \"Masks\" for training and validation: 1 if in the given set, 0 if not (or no label)\n",
    "train_mask = np.zeros(N)\n",
    "train_mask[train_ind] = 1\n",
    "\n",
    "test_mask = np.zeros(N)\n",
    "test_mask[test_ind] = 1\n",
    "\n",
    "val_mask = np.zeros(N)\n",
    "val_mask[val_ind] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Mean imputation for the other features -- see EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "              missing_values=nan, strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to fit only on the training data!  Otherwise the imputation introduces bias\n",
    "imp.fit(X.values[train_ind, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the training and testing values\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).to_csv(data_path+'Wildfire_Data_X_{}_{}.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(Y).to_csv(data_path+'Wildfire_Data_Y_{}_{}.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(train_mask).to_csv(data_path+'train_mask_{}_{}.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(val_mask).to_csv(data_path+'val_mask_{}_{}.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(test_mask).to_csv(data_path+'test_mask_{}_{}.csv'.format(postfix_time,postfix_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).to_csv(data_path+'Wildfire_Data_X_{}_{}_HALF.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(Y).to_csv(data_path+'Wildfire_Data_Y_{}_{}_HALF.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(train_mask).to_csv(data_path+'train_mask_{}_{}_HALF.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(val_mask).to_csv(data_path+'val_mask_{}_{}_HALF.csv'.format(postfix_time,postfix_sort))\n",
    "pd.DataFrame(test_mask).to_csv(data_path+'test_mask_{}_{}_HALF.csv'.format(postfix_time,postfix_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.1/Keras Py3.7",
   "language": "python",
   "name": "tensorflow210_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}